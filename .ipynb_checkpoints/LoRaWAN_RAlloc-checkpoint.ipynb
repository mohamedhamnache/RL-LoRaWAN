{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import simpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling2D, Activation, Flatten\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "# log-distance loss model parameters\n",
    "\n",
    "PTX = 14.0\n",
    "GAMMA = 2.08\n",
    "D0 = 40.0\n",
    "VAR = 0  # variance ignored for now\n",
    "LPLD0 = 127.41\n",
    "\n",
    "# Gate way localisation\n",
    "BSX = 0.0\n",
    "BSY = 0.0\n",
    "HM = 1.0  # m\n",
    "HB = 15.0  # m\n",
    "\n",
    "RAY = 700.0\n",
    "\n",
    "# Created Nodes\n",
    "NODES = []\n",
    "\n",
    "packetlen = 20\n",
    "\n",
    "\n",
    "BW = 125\n",
    "PL = 20\n",
    "CR = 1\n",
    "\n",
    "p = 600\n",
    "AVGSENDTIME = p\n",
    "# Inter-SF Thresshold Mateix\n",
    "\n",
    "IS7 = np.array([6, -8, -9, -9, -9, -9])\n",
    "IS8 = np.array([-11, 6, -11, -12, -13, -13])\n",
    "IS9 = np.array([-15, -13, 6, -13, -14, -15])\n",
    "IS10 = np.array([-19, -18, -17, 6, -17, -18])\n",
    "IS11 = np.array([-22, -22, -21, -20, 6, -20])\n",
    "IS12 = np.array([-25, -25, -25, -24, -23, 6])\n",
    "SIR = np.array([IS7, IS8, IS9, IS10, IS11, IS12])\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    0: {\"sf\": 7, \"sens\": -126.5, \"snr\": -7.5},\n",
    "    1: {\"sf\": 8, \"sens\": -127.25, \"snr\": -10},\n",
    "    2: {\"sf\": 9, \"sens\": -131.25, \"snr\": -12.5},\n",
    "    3: {\"sf\": 10, \"sens\": -132.75, \"snr\": -15},\n",
    "    4: {\"sf\": 11, \"sens\": -133.25, \"snr\": -17.5},\n",
    "    5: {\"sf\": 12, \"sens\": -134.5, \"snr\": -20},\n",
    "}\n",
    "\n",
    "ch = [868100000, 868300000, 868500000]\n",
    "tpx = [2, 5, 8, 11, 14]\n",
    "MODEL = 6\n",
    "SEED = 913\n",
    "\n",
    "rho_max = 0.5\n",
    "tp_max = 14\n",
    "tp_min = 2\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "TX = [\n",
    "    22,\n",
    "    22,\n",
    "    22,\n",
    "    23,  # RFO/PA0: -2..1\n",
    "    24,\n",
    "    24,\n",
    "    24,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    26,\n",
    "    31,\n",
    "    32,\n",
    "    34,\n",
    "    35,\n",
    "    44,  # PA_BOOST/PA1: 2..14\n",
    "    82,\n",
    "    85,\n",
    "    90,  # PA_BOOST/PA1: 15..17\n",
    "    105,\n",
    "    115,\n",
    "    125,\n",
    "]  # PA_BOOST/PA1+PA2: 18..20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SF_alloc_ch_utilization(NODES):\n",
    "    sorted_nodes = []\n",
    "    sorted_nodes = sorted(NODES, key=lambda x: x.dist, reverse=False)\n",
    "    c_class = np.zeros(6, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_u = np.zeros(ch_len, dtype=int)\n",
    "    # print(c_class)\n",
    "    for n in sorted_nodes:\n",
    "        sf = n.sf\n",
    "        # print(sf)\n",
    "        config = config_dict[sf - 7]\n",
    "        # print(config)\n",
    "        for i in range(sf, 12):\n",
    "            if n.snr < config[\"snr\"] or n.rssi < config[\"sens\"]:\n",
    "                if n.sf < 12:\n",
    "                    n.sf = n.sf + 1\n",
    "                    n.packet.sf = n.sf\n",
    "                    n.update_rectime(n.sf)\n",
    "            config = config_dict[n.sf - 7]\n",
    "        c_class[n.sf - 7] = c_class[n.sf - 7] + 1\n",
    "        ch_index = ch.index(n.freq)\n",
    "        ch_u[ch_index] = ch_u[ch_index] + 1\n",
    "    return sorted_nodes, c_class, ch_u\n",
    "\n",
    "\n",
    "def compute_sf_ch_utilization(nodes):\n",
    "    sf_dist = np.zeros(6, dtype=int)\n",
    "    tp_dist = np.zeros(5, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_dist = np.zeros(ch_len, dtype=int)\n",
    "    # print(ch_dist)\n",
    "    # print(ch_len)\n",
    "    # print_nodes(nodes)\n",
    "    for n in nodes:\n",
    "        sf = n.sf\n",
    "        freq = n.freq\n",
    "        tx = n.tx\n",
    "        sf_dist[sf - 7] = sf_dist[sf - 7] + 1\n",
    "        tp_index = tpx.index(tx)\n",
    "        # print('index',tp_index)\n",
    "        tp_dist[tp_index] = tp_dist[tp_index] + 1\n",
    "\n",
    "        # print('Channel : ',freq)\n",
    "        ch_index = ch.index(freq)\n",
    "        # print('index',ch_index)\n",
    "        ch_dist[ch_index] = ch_dist[ch_index] + 1\n",
    "    return sf_dist, tp_dist, ch_dist\n",
    "\n",
    "\n",
    "def energy_consumption(nodes):\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "    return energy\n",
    "\n",
    "\n",
    "def print_nodes(nodes):\n",
    "    for n in nodes:\n",
    "        print(\"node_id: {} SF: {} TP: {} CH: {} RSSI: {} SNR: {}\".format(n.id, n.sf, n.tx, n.freq,n.rssi,n.snr))\n",
    "\n",
    "\n",
    "def reward_plot(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Packet:\n",
    "    def __init__(self, nodeid, sf, bw, freq, rssi, rectime):\n",
    "        self.nodeid = nodeid\n",
    "        self.pl = packetlen\n",
    "        self.arriveTime = 0\n",
    "        # Reception Parameters\n",
    "        self.sf = sf\n",
    "        self.bw = bw\n",
    "        self.freq = freq\n",
    "        self.rssi = rssi\n",
    "        self.rectime = rectime\n",
    "        self.collided = 0\n",
    "        self.processed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class Device:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.dist = 0\n",
    "        found = False\n",
    "        while not found:\n",
    "            a = random.random()\n",
    "            b = random.random()\n",
    "            if b < a:\n",
    "                a, b = b, a\n",
    "            posx = b * RAY * math.cos(2 * math.pi * a / b) + BSX\n",
    "            posy = b * RAY * math.sin(2 * math.pi * a / b) + BSY\n",
    "            if len(NODES) > 0:\n",
    "                for index, n in enumerate(NODES):\n",
    "                    dist = np.sqrt(((abs(n.x - posx)) ** 2) + ((abs(n.y - posy)) ** 2))\n",
    "                    if dist >= 20:\n",
    "                        found = 1\n",
    "                        self.x = posx\n",
    "                        self.y = posy\n",
    "                        break\n",
    "            else:\n",
    "                self.x = posx\n",
    "                self.y = posy\n",
    "                found = True\n",
    "\n",
    "        dist_2d = np.sqrt(\n",
    "            (self.x - BSX) * (self.x - BSX) + (self.y - BSY) * (self.y - BSY)\n",
    "        )\n",
    "        # self.dist = np.sqrt((dist_2d)**2+(HB-HM)**2)\n",
    "        self.dist = dist_2d\n",
    "        # Radio Parameters\n",
    "        self.bw = BW\n",
    "        self.sf = 7\n",
    "        self.tx = PTX\n",
    "        self.cr = 1\n",
    "        self.freq = random.choice(ch)\n",
    "        # self.freq = 868100000\n",
    "        self.rssi = self.tx - self.estimatePathLoss(MODEL)\n",
    "        self.snr = random.randrange(-10, 20)\n",
    "        self.rectime = self.airtime(self.sf, self.cr, packetlen, BW)\n",
    "        self.packet = Packet(\n",
    "            self.id, self.sf, self.bw, self.freq, self.rssi, self.rectime\n",
    "        )\n",
    "        self.sent = 0\n",
    "        self.received = 0\n",
    "\n",
    "    def airtime(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def log_distance_loss(self, dist):\n",
    "        Lpl = LPLD0 + 10 * GAMMA * math.log10(dist / D0)\n",
    "        rssi = PTX - Lpl\n",
    "        return rssi\n",
    "\n",
    "    def update_rectime(self, sf):\n",
    "        self.rectime = self.airtime(sf, self.cr, packetlen, BW)\n",
    "        self.packet.rectime = self.rectime\n",
    "\n",
    "    def estimatePathLoss(self, model):\n",
    "        # Log-Distance model\n",
    "        if model == 0:\n",
    "            Lpl = LPLD0 + 10 * GAMMA * math.log10(self.dist / D0)\n",
    "\n",
    "        # Okumura-Hata model\n",
    "        elif model >= 1 and model <= 4:\n",
    "            # small and medium-size cities\n",
    "            if model == 1:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = 0\n",
    "            # metropolitan areas\n",
    "            elif model == 2:\n",
    "                if self.freq <= 200000000:\n",
    "                    ahm = 8.29 * ((math.log10(1.54 * HM)) ** 2) - 1.1\n",
    "                elif self.freq >= 400000000:\n",
    "                    ahm = 3.2 * ((math.log10(11.75 * HM)) ** 2) - 4.97\n",
    "                C = 0\n",
    "            # suburban enviroments\n",
    "            elif model == 3:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = -2 * ((math.log10(self.freq) - math.log10(28000000)) ** 2) - 5.4\n",
    "            # rural area\n",
    "            elif model == 4:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = (\n",
    "                    -4.78 * ((math.log10(self.freq) - math.log10(1000000)) ** 2)\n",
    "                    + 18.33 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                    - 40.98\n",
    "                )\n",
    "\n",
    "            A = (\n",
    "                69.55\n",
    "                + 26.16 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log(HB)\n",
    "                - ahm\n",
    "            )\n",
    "\n",
    "            B = 44.9 - 6.55 * math.log10(HB)\n",
    "\n",
    "            Lpl = A + B * (math.log10(self.dist) - math.log10(1000)) + C\n",
    "\n",
    "        # 3GPP model\n",
    "        elif model >= 5 and model < 7:\n",
    "            # Suburban Macro\n",
    "            if model == 5:\n",
    "                C = 0  # dB\n",
    "            # Urban Macro\n",
    "            elif model == 6:\n",
    "                C = 3  # dB\n",
    "\n",
    "            Lpl = (\n",
    "                (44.9 - 6.55 * math.log10(HB))\n",
    "                * (math.log10(self.dist) - math.log10(1000))\n",
    "                + 45.5\n",
    "                + (35.46 - 1.1 * HM) * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log10(HM)\n",
    "                + 0.7 * HM\n",
    "                + C\n",
    "            )\n",
    "\n",
    "        # Polynomial 3rd degree\n",
    "        elif model == 7:\n",
    "            p1 = -5.491e-06\n",
    "            p2 = 0.002936\n",
    "            p3 = -0.5004\n",
    "            p4 = -70.57\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 3)\n",
    "                + p2 * math.pow(self.dist, 2)\n",
    "                + p3 * self.dist\n",
    "                + p4\n",
    "            )\n",
    "\n",
    "        # Polynomial 6th degree\n",
    "        elif model == 8:\n",
    "            p1 = 3.69e-12\n",
    "            p2 = 5.997e-11\n",
    "            p3 = -1.381e-06\n",
    "            p4 = 0.0005134\n",
    "            p5 = -0.07318\n",
    "            p6 = 4.254\n",
    "            p7 = -171\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 6)\n",
    "                + p2 * math.pow(self.dist, 5)\n",
    "                + p3 * math.pow(self.dist, 4)\n",
    "                + p4 * math.pow(self.dist, 3)\n",
    "                + p5 * math.pow(self.dist, 2)\n",
    "                + p6 * self.dist\n",
    "                + p7\n",
    "            )\n",
    "\n",
    "        return Lpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Collision Detection ################\n",
    "\"\"\"\n",
    "\n",
    "This Part Allows Collision Checking Between Two Packets :\n",
    " 1- Timing Collision\n",
    " 2- Frequency Collision\n",
    " 3- SF Collision\n",
    " 4- Capture Effect\n",
    " 5- Imperfect SF Orthogonality\n",
    " \n",
    "\"\"\"\n",
    "########################### 1-Timining Collision #################\n",
    "\n",
    "\n",
    "def timingCollision(env, p1, p2):\n",
    "    Npream = 8\n",
    "    Tpreamb = 2 ** p1.sf / (1.0 * p1.bw) * (Npream - 5)\n",
    "    p2_end = p2.addTime + p2.rectime\n",
    "\n",
    "    p1_cs = env.now + (Tpreamb / 1000.0)  # to sec\n",
    "\n",
    "    \"\"\" print (\"collision timing node {} ({},{},{}) node {} ({},{})\".format(\n",
    "        p1.nodeid, env.now - env.now, p1_cs - env.now, p1.rectime,\n",
    "        p2.nodeid, p2.addTime - env.now, p2_end - env.now\n",
    "    ))\"\"\"\n",
    "    if p1_cs < p2_end:\n",
    "        # p1 collided with p2 and lost\n",
    "        # print (\"not late enough\")\n",
    "        return True\n",
    "    # print (\"saved by the preamble\")\n",
    "    return False\n",
    "\n",
    "\n",
    "######################### 2-Frequency Collision #######################\n",
    "def frequencyCollision(p1, p2):\n",
    "    if abs(p1.freq - p2.freq) <= 120 and (p1.bw == 500 or p2.freq == 500):\n",
    "        # print (\"frequency coll 500\")\n",
    "        return True\n",
    "    elif abs(p1.freq - p2.freq) <= 60 and (p1.bw == 250 or p2.freq == 250):\n",
    "        # print( \"frequency coll 250\")\n",
    "        return True\n",
    "    else:\n",
    "        if abs(p1.freq - p2.freq) <= 30:\n",
    "            # print (\"frequency coll 125\")\n",
    "            return True\n",
    "        # else:\n",
    "    # print (\"no frequency coll\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 3- SF Collision ###############################\n",
    "def sfCollision(p1, p2):\n",
    "    if p1.sf == p2.sf:\n",
    "        # print (\"collision sf node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "        return True\n",
    "    # print (\"no sf collision\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 4-5 ############################################\n",
    "def powerCollision_2(p1, p2):\n",
    "    # powerThreshold = 6\n",
    "    # print (\"SF: node {0.nodeid} {0.sf} node {1.nodeid} {1.sf}\".format(p1, p2))\n",
    "    # print (\"pwr: node {0.nodeid} {0.rssi:3.2f} dBm node {1.nodeid} {1.rssi:3.2f} dBm; diff {2:3.2f} dBm\".format(p1, p2, round(p1.rssi - p2.rssi,2)))\n",
    "\n",
    "    if p1.sf == p2.sf:\n",
    "\n",
    "        if abs(p1.rssi - p2.rssi) < 6:\n",
    "            # print (\"collision pwr both node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "            # packets are too close to each other, both collide\n",
    "            # return both packets as casualties\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{} ==> |{}| < {}\".format(\n",
    "                    p1.freq, p2.freq, abs(p1.rssi - p2.rssi), SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1, p2)\n",
    "        elif p1.rssi - p2.rssi < 6:\n",
    "            # p2 overpowered p1, return p1 as casualty\n",
    "            # print (\"collision pwr node {} overpowered node {}\".format(p2.nodeid, p1.nodeid))\n",
    "            # print (\"capture - p2 wins, p1 lost\")\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{}  => {} < {}\".format(\n",
    "                    p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1,)\n",
    "        # print (\"capture - p1 wins, p2 lost\")\n",
    "        # p2 was the weaker packet, return it as a casualty\n",
    "        \"\"\"print(\n",
    "            \"power coll  cap : freq 1 : {} and freq 2 :{} => {} > {}\".format(\n",
    "                p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "            )\n",
    "        )\"\"\"\n",
    "        return (p2,)\n",
    "    else:\n",
    "\n",
    "        if p1.rssi - p2.rssi > SIR[p1.sf - 7][p2.sf - 7]:\n",
    "\n",
    "            # print (\"P1 is OK\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "                # print (\"p2 is OK\")\n",
    "                return ()\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"power coll  Imp : 2\")\n",
    "                return (p2,)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # print (\"p1 is lost\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "\n",
    "                # print (\"p2 is OK\")\n",
    "                # print(\"power coll  Imp : 1\")\n",
    "                return (p1,)\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"opwer coll  cap : 1\")\n",
    "                return (p1, p2)\n",
    "\n",
    "\n",
    "######################## Check Collision ############################\n",
    "def checkcollision(env, packet, packetsAtBS, maxBSReceives, full_collision):\n",
    "    col = 0  # flag needed since there might be several collisions for packet\n",
    "    processing = 0\n",
    "    for i in range(0, len(packetsAtBS)):\n",
    "        if packetsAtBS[i].packet.processed == 1:\n",
    "            processing = processing + 1\n",
    "    if processing > maxBSReceives:\n",
    "        # print (\"too long:\", len(packetsAtBS))\n",
    "        packet.processed = 0\n",
    "    else:\n",
    "        packet.processed = 1\n",
    "\n",
    "    if packetsAtBS:\n",
    "        # print (\"CHECK node {} (sf:{} bw:{} freq:{:.6e}) others: {}\".format(\n",
    "        # packet.nodeid, packet.sf, packet.bw, packet.freq,\n",
    "        # len(packetsAtBS)))\n",
    "        # print(len(packetsAtBS))\n",
    "        for other in packetsAtBS:\n",
    "\n",
    "            if other.id != packet.nodeid:\n",
    "                # print (\">> node {} (sf:{} bw:{} freq:{:.6e})\".format(\n",
    "                # other.id, other.packet.sf, other.packet.bw, other.packet.freq))\n",
    "                if full_collision == 1 or full_collision == 2:\n",
    "\n",
    "                    if frequencyCollision(packet, other.packet) and timingCollision(\n",
    "                        env, packet, other.packet\n",
    "                    ):\n",
    "                        # check who collides in the power domain\n",
    "                        if full_collision == 1:\n",
    "                            # Capture effect\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        else:\n",
    "                            # Capture + Non-orthognalitiy SFs effects\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        # mark all the collided packets\n",
    "                        # either this one, the other one, or both\n",
    "                        for p in c:\n",
    "                            p.collided = 1\n",
    "                            if p == packet:\n",
    "                                col = 1\n",
    "\n",
    "                    else:\n",
    "                        # no freq or timing collision, all fimone\n",
    "                        pass\n",
    "                else:\n",
    "                    # simple collision\n",
    "                    if frequencyCollision(packet, other.packet) and sfCollision(\n",
    "                        packet, other.packet\n",
    "                    ):\n",
    "                        packet.collided = 1\n",
    "                        other.packet.collided = (\n",
    "                            1  # other also got lost, if it wasn't lost already\n",
    "                        )\n",
    "                        col = 1\n",
    "        return col\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Node generation and Plot Function\n",
    "\n",
    "\n",
    "def node_Gen(nrNodes, display):\n",
    "    NODES = []\n",
    "    x = []\n",
    "    y = []\n",
    "    rssi = []\n",
    "    dist = []\n",
    "\n",
    "    for i in range(nrNodes):\n",
    "        device = Device(i)\n",
    "        NODES.append(device)\n",
    "\n",
    "        x.append(NODES[i].x)\n",
    "        y.append(NODES[i].y)\n",
    "        rssi.append(NODES[i].rssi)\n",
    "        dist.append(NODES[i].dist)\n",
    "    pickle.dump(NODES, open(\"data/nodes.p\", \"wb\"))\n",
    "    if display:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x, y, \"ro\")\n",
    "        ax.plot(0, 0, \"bo\")\n",
    "        for i in range(6):\n",
    "            v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            X, Y = np.meshgrid(v1, v2)\n",
    "            F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "            ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "        ax.set_aspect(1)\n",
    "        plt.show()\n",
    "\n",
    "    return NODES\n",
    "\n",
    "\n",
    "# SF Allocation Display\n",
    "\n",
    "# RA-Lora Plot\n",
    "def SF_alloc_plot(sorted_nodes, exp, k, display=False, save=False):\n",
    "    groups = []\n",
    "    for s in range(7, 13):\n",
    "        group = []\n",
    "        posx = []\n",
    "        posy = []\n",
    "        for n in sorted_nodes:\n",
    "            if n.sf == s:\n",
    "                posx.append(n.x)\n",
    "                posy.append(n.y)\n",
    "\n",
    "        group.append(posx)\n",
    "        group.append(posy)\n",
    "        groups.append(group)\n",
    "        group = []\n",
    "    # print(groups)\n",
    "    colors = [\"ro\", \"go\", \"bo\", \"yo\", \"co\", \"mo\"]\n",
    "    # print(len(groups[3][0]))\n",
    "    # print(len(groups[3][1]))\n",
    "    fig, ax = plt.subplots()\n",
    "    for g, c in zip(groups, colors):\n",
    "        ax.plot(g[0], g[1], c)\n",
    "\n",
    "    ax.plot(0, 0, \"ko\")\n",
    "\n",
    "    for i in range(6):\n",
    "        v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        X, Y = np.meshgrid(v1, v2)\n",
    "        F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "        ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "    ax.set_aspect(1)\n",
    "    if save:\n",
    "        if not os.path.exists(\"reports/graphics/{}\".format(exp)):\n",
    "            os.mkdir(\"reports/graphics/{}\".format(exp))\n",
    "        plt.savefig(\"reports/graphics/{}/sf_alloc_{}\".format(exp, k))\n",
    "    if display:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def finalReport(data):\n",
    "    fname = \"sim_results_P600.csv\"\n",
    "    if not os.path.isfile(\"reports/results/{}\".format(fname)):\n",
    "        df_new = pd.DataFrame(data, index=[0])\n",
    "        df_new.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "    else:\n",
    "        df = pd.read_csv(\"reports/results/{}\".format(fname))\n",
    "        df_new = pd.DataFrame(data, index=[df.ndim - 1])\n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "        df.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "\n",
    "\n",
    "def airtime(sf, cr, pl, bw):\n",
    "    H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "    DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "    Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "    if bw == 125 and sf in [11, 12]:\n",
    "        # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "        DE = 1\n",
    "    if sf == 6:\n",
    "        # can only have implicit header with SF6\n",
    "        H = 1\n",
    "    Tsym = (2.0 ** sf) / bw  # msec\n",
    "    Tpream = (Npream + 4.25) * Tsym\n",
    "    # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "    payloadSymbNB = 8 + max(\n",
    "        math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "        * (cr + 4),\n",
    "        0,\n",
    "    )\n",
    "    Tpayload = payloadSymbNB * Tsym\n",
    "    return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "        \n",
    "        \n",
    "def mathematical_model(N, sf_dist, avgtime):\n",
    "    charges = []\n",
    "    DERs = []\n",
    "    s = 0\n",
    "    for i in range(6):\n",
    "        Nsf = sf_dist[i]\n",
    "        toa = airtime(i + 7, 1, packetlen, BW)\n",
    "        c = (Nsf * toa) / avgtime\n",
    "        charges.append(c)\n",
    "        DERsf = math.exp(-2*c)\n",
    "        DERs.append(DERsf)\n",
    "        s = s + (DERsf * Nsf)\n",
    "    DER = s / N\n",
    "    return DER, DERs, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Environment(gym.Env):\n",
    "    def __init__(self, nodes, SF_dist_init, Ch_dist_init):\n",
    "\n",
    "        \"\"\" Environment Parameter\"\"\"\n",
    "\n",
    "        super(Environment, self).__init__()\n",
    "\n",
    "        self.NODES = nodes\n",
    "        self.N = len(self.NODES)\n",
    "        self.TP_SET = [2, 5, 8, 11, 14]\n",
    "        self.SF_SET = [7, 8, 9, 10, 11, 12]\n",
    "        self.CH_SET = [\"868.1\", \"868.3\", \"868.5\"]\n",
    "        self.ACTION_SPACE_GEN = self.action_space_generator()\n",
    "        self.ACTION_SPACE_SIZE = len(self.ACTION_SPACE_GEN)\n",
    "        self.STATE_SPACE = []\n",
    "        self.SF_DIST = SF_dist_init\n",
    "        self.ENERGY = 1\n",
    "        self.CH_DIST = Ch_dist_init\n",
    "\n",
    "        \"\"\" Simulation Parameters\"\"\"\n",
    "        self.action_space = spaces.Box(\n",
    "            np.array([7, 0, 0]), np.array([12, 4, 2]), dtype=np.int32\n",
    "        )\n",
    "        # self.observation_space = spaces.Box(np.array([0,7,0,0,-10,-135]), np.array([N,12,4,2,10,-70]),dtype =np.int)\n",
    "        self.observation_space = spaces.Box(\n",
    "            np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "            np.array(\n",
    "                [\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                ]\n",
    "            ),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        self.condition = 0\n",
    "\n",
    "    def action_space_generator(self):\n",
    "        action_space = []\n",
    "        for sf in self.SF_SET:\n",
    "            for tp in range(len(self.TP_SET)):\n",
    "                for ch in range(len(self.CH_SET)):\n",
    "                    action_space.append([sf, tp, ch])\n",
    "                    \n",
    "        return action_space\n",
    "\n",
    "    def toa(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def rho_compute(self, sf, n):\n",
    "        return (self.toa(sf, CR, PL, BW) * n) / p\n",
    "\n",
    "    def reward_function(self, rho, u_ch, tp):\n",
    "        return (rho_max) / (rho * u_ch) + ((tp_max - tp) / (tp_max - tp_min))\n",
    "\n",
    "    def reward_function_zero(self, tp):\n",
    "        return (tp_max - tp) / (tp_max - tp_min)\n",
    "\n",
    "    def compute_der(self, sf_dist):\n",
    "        der = 0\n",
    "        for index in range(len(sf_dist)):\n",
    "            rho = self.rho_compute(self.SF_SET[index], sf_dist[index])\n",
    "            der_sf = math.exp(-2 * rho)\n",
    "            der = der + (der_sf * sf_dist[index])\n",
    "        return (der / self.N) * 100\n",
    "\n",
    "    def reset(self):\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        self.condition = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print('Step : ',self.condition)\n",
    "        # print('Action : ',action)\n",
    "        # assert self.action_space.contains(action)\n",
    "        reward = 0\n",
    "        node_index = random.randint(0,self.N-1)\n",
    "        node = self.NODES[node_index]\n",
    "        old_sf = node.sf\n",
    "        old_tp = node.tx\n",
    "        old_ch = node.freq\n",
    "        new_sf = action[0]\n",
    "        new_tp = self.TP_SET[action[1]]\n",
    "        new_ch = action[2]\n",
    "\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        der = self.compute_der(sf_dist)\n",
    "        #print(\"DER : \", der)\n",
    "        config = config_dict[new_sf - 7]\n",
    "        # print(\"config : \", config)\n",
    "        \n",
    "        # print('rho',rho)\n",
    "        # print(\"node snr : {} rssi : {}\".format(node.snr, node.rssi))\n",
    "        # print(\"node old sf : {} new sf: {}\".format(old_sf, new_sf))\n",
    "        if node.snr < config[\"snr\"] or node.rssi < config[\"sens\"]:\n",
    "             \n",
    "            #rho = self.rho_compute(new_sf, sf_dist[new_sf - 7])\n",
    "            #if rho > 0 and u_ch[new_ch] > 0:\n",
    "                #reward = - self.reward_function(rho, u_ch[new_ch], new_tp)\n",
    "            #else:\n",
    "                #reward = - self.reward_function_zero(new_tp)\n",
    "            reward = -1\n",
    "\n",
    "        else: \n",
    "            rho = self.rho_compute(new_sf, sf_dist[new_sf - 7])\n",
    "            old_rho = self.rho_compute(old_sf, sf_dist[old_sf - 7])\n",
    "            if new_sf > old_sf:\n",
    "                if old_rho > rho_max and rho < rho_max :\n",
    "                    self.NODES[node_index].sf = new_sf\n",
    "                    self.NODES[node_index].freq = ch[new_ch]\n",
    "                    self.NODES[node_index].tx = new_tp\n",
    "                    \n",
    "                    self.NODES[node_index].packet.sf = new_sf\n",
    "                    self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "            \n",
    "                    \n",
    "                    self.NODES[node_index].update_rectime(new_sf)\n",
    "\n",
    "                    sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                    sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                    old_tp_index = self.TP_SET.index(old_tp)\n",
    "                    new_tp_index = self.TP_SET.index(new_tp)\n",
    "                    tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                    tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                    old_ch_index = ch.index(old_ch)\n",
    "                    new_ch_index = new_ch\n",
    "                    u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                    u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "            else:\n",
    "                \n",
    "                self.NODES[node_index].sf = new_sf\n",
    "                self.NODES[node_index].freq = ch[new_ch]\n",
    "                self.NODES[node_index].tx = new_tp\n",
    "                self.NODES[node_index].update_rectime(new_sf)\n",
    "                \n",
    "                self.NODES[node_index].packet.sf = new_sf\n",
    "                self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "\n",
    "                sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                old_tp_index = self.TP_SET.index(old_tp)\n",
    "                new_tp_index = self.TP_SET.index(new_tp)\n",
    "                tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                old_ch_index = ch.index(old_ch)\n",
    "                new_ch_index = new_ch\n",
    "                u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "        \n",
    "\n",
    "            if rho > 0 and u_ch[new_ch] > 0:\n",
    "                reward = self.reward_function(rho, u_ch[new_ch], new_tp)\n",
    "            else:\n",
    "                reward = self.reward_function_zero(new_tp)\n",
    "\n",
    "           \n",
    "        # print(sf_dist)\n",
    "        # print(tp_dist)\n",
    "        # print(u_ch)\n",
    "\n",
    "        # print('Reward = ',reward)\n",
    "        new_state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        # print('')\n",
    "        done = False\n",
    "        self.condition = self.condition + 1\n",
    "        if self.condition == 100:\n",
    "            done = True\n",
    "        #if der > 95:\n",
    "            #done = True\n",
    "\n",
    "        info = \"\"\n",
    "\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nodes (nodes,file_name):\n",
    "    pickle.dump(nodes, open(\"data/\"+file_name, \"wb\"))\n",
    "    print('All Nodes Were Saved !')\n",
    "    \n",
    "def load_nodes (file_name):\n",
    "    print('Loading Nodes From File')\n",
    "    nodes = pickle.load(open(\"data/\"+file_name,\"rb\"))\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Nodes Were Saved !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nodes = node_Gen(2000, display=False)\n",
    "nodes, sf_distribution, ch_u = SF_alloc_ch_utilization(nodes)\n",
    "save_nodes(nodes,'nodes_{}.alloc'.format(len(nodes)))\n",
    "#train_episodes = 300\n",
    "#test_episodes = 100\n",
    "\n",
    "train_episodes = 300\n",
    "test_episodes = 100\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def create_model(self, action_shape, state_shape):\n",
    "        model = Sequential()\n",
    "        learning_rate = 0.001\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "\n",
    "        model.add(\n",
    "            Dense(\n",
    "                256, input_shape=state_shape, activation=\"relu\", kernel_initializer=init\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(256, activation=\"relu\", kernel_initializer=init))\n",
    "\n",
    "        model.add(Dense(90, activation=\"relu\", kernel_initializer=init))\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        model.save('lora_rl_model.h5')\n",
    "        return model\n",
    "\n",
    "    def get_qs(self, model, state, step):\n",
    "        return model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "\n",
    "    def train(self, env, replay_memory, model, target_model, done):\n",
    "        learning_rate = 0.7  # Learning rate\n",
    "        discount_factor = 0.618\n",
    "\n",
    "        MIN_REPLAY_SIZE = 1000\n",
    "        if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "            return\n",
    "\n",
    "        batch_size = 64 * 2\n",
    "        mini_batch = random.sample(replay_memory, batch_size)\n",
    "        # print(len(mini_batch))\n",
    "        # print(mini_batch)\n",
    "\n",
    "        # current_states = np.array([transition[0] for transition in mini_batch],dtype=object)\n",
    "        # print()\n",
    "        current_states = np.array(\n",
    "            np.array(\n",
    "                [\n",
    "                    transition[0]\n",
    "                    for transition in mini_batch\n",
    "                    if transition[0] is not None\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        # print('states len : ',len(current_states))\n",
    "        # current_states= np.array(current_states).astype(\"float32\")\n",
    "        # print(current_states)\n",
    "        # current_states = np.asarray(current_states).astype(np.float32)\n",
    "        current_qs_list = model.predict(current_states)\n",
    "        # print(current_qs_list)\n",
    "        # print('Current: ',len(current_qs_list))\n",
    "        # print('Mini batch : ',len(mini_batch))\n",
    "        new_current_states = np.array(\n",
    "            [transition[3] for transition in mini_batch if transition[3] is not None]\n",
    "        )\n",
    "        future_qs_list = target_model.predict(new_current_states)\n",
    "        # print()\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        #print(mini_batch)\n",
    "        #print(len(mini_batch))\n",
    "        for index, (observation, action, reward, new_observation, done) in enumerate(\n",
    "            mini_batch\n",
    "        ):\n",
    "            if not done:\n",
    "                max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "            else:\n",
    "                max_future_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            #print('The current_qs : ',current_qs)\n",
    "            #print('Action : ',action)\n",
    "            #print(type(action))\n",
    "            #print(env.ACTION_SPACE_GEN)\n",
    "            action_index = env.ACTION_SPACE_GEN.index(action)\n",
    "            current_qs[action_index] = (1 - learning_rate) * current_qs[\n",
    "                action_index\n",
    "            ] + learning_rate * max_future_q\n",
    "\n",
    "            X.append(observation)\n",
    "            Y.append(current_qs)\n",
    "        model.fit(\n",
    "            np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True\n",
    "        )\n",
    "\n",
    "\n",
    "env = Environment(nodes, sf_distribution, ch_u)\n",
    "\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "# agent.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    epsilon = 1  # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "    max_epsilon = 1  # You can't explore more than 100% of the time\n",
    "    min_epsilon = 0.01  # At a minimum, we'll always explore 1% of the time\n",
    "    decay = 0.01\n",
    "    final_rewards = []\n",
    "    # 1. Initialize the Target and Main models\n",
    "    # Main Model (updated every 4 steps)\n",
    "    print(env.observation_space.shape)\n",
    "    print(env.action_space.shape)\n",
    "    #print_nodes(nodes)\n",
    "    #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    model = agent.create_model(env.action_space.shape, env.observation_space.shape)\n",
    "    print(\"Model Created !\")\n",
    "    # Target Model (updated every 100 steps)\n",
    "    target_model = agent.create_model(\n",
    "        env.action_space.shape, env.observation_space.shape\n",
    "    )\n",
    "    print(\"Target Model Created !\")\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "    target_update_counter = 0\n",
    "\n",
    "    # X = states, y = actions\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    steps_to_update_target_model = 0\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        print('Episode : ',episode)\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps_to_update_target_model += 1\n",
    "            # if True:\n",
    "            # env.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                # print('Explore')\n",
    "                action = env.action_space.sample()\n",
    "                a =[]\n",
    "                a.append(action[0])\n",
    "                a.append(action[1])\n",
    "                a.append(action[2])\n",
    "                action = a\n",
    "            else:\n",
    "\n",
    "                # Exploit best known action\n",
    "                # model dims are (batch, env.observation_space.n)\n",
    "                # print('Exploit')\n",
    "                encoded = observation\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                # print('model : ',model)\n",
    "                # print('Encoded state : ',encoded_reshaped)\n",
    "                predicted = model.predict(encoded_reshaped)\n",
    "                action = np.argmax(predicted)\n",
    "                action = env.ACTION_SPACE_GEN[action]\n",
    "            #print(action)\n",
    "            #print(type(action))\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "            # 3. Update the Main Network using the Bellman Equation\n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                agent.train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\n",
    "                    \"Total training rewards: {} after n steps = {} with final reward = {}\".format(\n",
    "                        total_training_rewards, episode, reward\n",
    "                    )\n",
    "                )\n",
    "                final_rewards.append(total_training_rewards)\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    print(\"Copying main network weights to the target network weights\")\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    env.close()\n",
    "    save_nodes(env.NODES,'nodes_{}.rl'.format(len(env.NODES)))\n",
    "    #print_nodes(env.NODES)\n",
    "    print(final_rewards)\n",
    "    #print_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "(3,)\n",
      "Model Created !\n",
      "Target Model Created !\n",
      "Episode :  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamnache/anaconda3/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 25.49442104953507 after n steps = 0 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  1\n",
      "Total training rewards: 28.503396276099668 after n steps = 1 with final reward = 1.000554668826963\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbe304477a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbe304477a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: 25.658674563054642 after n steps = 2 with final reward = 1.0035591409363138\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  3\n",
      "Total training rewards: 33.61233597965538 after n steps = 3 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  4\n",
      "Total training rewards: 36.34342311003635 after n steps = 4 with final reward = 0.7505757378570891\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  5\n",
      "Total training rewards: 32.07253883114332 after n steps = 5 with final reward = 1.0002342559317392\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  6\n",
      "Total training rewards: 34.81140759180278 after n steps = 6 with final reward = 0.0002413329043942613\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  7\n",
      "Total training rewards: 28.304570746145604 after n steps = 7 with final reward = 0.00012107325487438864\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  8\n",
      "Total training rewards: 29.049571584141386 after n steps = 8 with final reward = 0.2510650135665211\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  9\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbe3035b290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbe3035b290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbe303dc4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbe303dc4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: 26.54684280357214 after n steps = 9 with final reward = 1.0003792242292937\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  10\n",
      "Total training rewards: 19.538406622607003 after n steps = 10 with final reward = 0.7501226590769895\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  11\n",
      "Total training rewards: 29.789151212575522 after n steps = 11 with final reward = 0.5003770342019522\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  12\n",
      "Total training rewards: 27.790367927874456 after n steps = 12 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  13\n",
      "Total training rewards: 36.534572350116726 after n steps = 13 with final reward = 0.2504118739435923\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  14\n",
      "Total training rewards: 37.53719961380015 after n steps = 14 with final reward = 0.5003761811833958\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  15\n",
      "Total training rewards: 28.78302505550288 after n steps = 15 with final reward = 0.5004936200893966\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  16\n",
      "Total training rewards: 33.28158025362068 after n steps = 16 with final reward = 0.2503676437444359\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  17\n",
      "Total training rewards: 28.780037951923415 after n steps = 17 with final reward = 1.0003681869934118\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  18\n",
      "Total training rewards: 34.28200028414995 after n steps = 18 with final reward = 0.25012652320584383\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  19\n",
      "Total training rewards: 25.779749912129958 after n steps = 19 with final reward = 0.7501238364598344\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  20\n",
      "Total training rewards: 23.027832667947536 after n steps = 20 with final reward = 0.750500976776357\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  21\n",
      "Total training rewards: 31.52857843961642 after n steps = 21 with final reward = 1.0002571616787208\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  22\n",
      "Total training rewards: 37.03109140550478 after n steps = 22 with final reward = 0.0001244898750967192\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  23\n",
      "Total training rewards: 43.53257117819618 after n steps = 23 with final reward = 1.0004668831501693\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  24\n",
      "Total training rewards: 28.530682515679533 after n steps = 24 with final reward = 0.00037356903716853976\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  25\n",
      "Total training rewards: 44.78117063544018 after n steps = 25 with final reward = 0.5004702564065818\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  26\n",
      "Total training rewards: 33.78175449611723 after n steps = 26 with final reward = 1.0003674521172616\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  27\n",
      "Total training rewards: 14.275497019527288 after n steps = 27 with final reward = 1.0003692907913098\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  28\n",
      "Total training rewards: 24.77691301334604 after n steps = 28 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  29\n",
      "Total training rewards: 31.77991561472446 after n steps = 29 with final reward = 0.5003669669334189\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  30\n",
      "Total training rewards: 27.028452393133065 after n steps = 30 with final reward = 1.0004356299550516\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  31\n",
      "Total training rewards: 26.777671238417728 after n steps = 31 with final reward = 0.00036787302461250177\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  32\n",
      "Total training rewards: 30.278500932929383 after n steps = 32 with final reward = 0.750265173154618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  33\n",
      "Total training rewards: 28.279181708306687 after n steps = 33 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  34\n",
      "Total training rewards: 31.778044939099185 after n steps = 34 with final reward = 0.00037262509725407726\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  35\n",
      "Total training rewards: 29.030284532830137 after n steps = 35 with final reward = 0.00036689905668080486\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  36\n",
      "Total training rewards: 18.02683813008886 after n steps = 36 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  37\n",
      "Total training rewards: 22.275871928762545 after n steps = 37 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  38\n",
      "Total training rewards: 33.02975944157259 after n steps = 38 with final reward = 0.7503851167662927\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  39\n",
      "Total training rewards: 37.529470470612246 after n steps = 39 with final reward = 0.500387172888063\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  40\n",
      "Total training rewards: 31.279516851395947 after n steps = 40 with final reward = 0.7502703593816578\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  41\n",
      "Total training rewards: 33.027295276089 after n steps = 41 with final reward = 0.7503687318502278\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  42\n",
      "Total training rewards: 28.526841264095363 after n steps = 42 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  43\n",
      "Total training rewards: 25.02673479728009 after n steps = 43 with final reward = 0.00037092783951657335\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  44\n",
      "Total training rewards: 29.77700748782052 after n steps = 44 with final reward = 0.5001332761246339\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  45\n",
      "Total training rewards: 32.029634865517735 after n steps = 45 with final reward = 0.2503698730655166\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  46\n",
      "Total training rewards: 24.277335466215877 after n steps = 46 with final reward = 0.7503852656324568\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  47\n",
      "Total training rewards: 15.529066002853945 after n steps = 47 with final reward = 0.0003930183133162579\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  48\n",
      "Total training rewards: 29.529101145743024 after n steps = 48 with final reward = 1.0003874149417789\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  49\n",
      "Total training rewards: 28.777645055378585 after n steps = 49 with final reward = 0.7502762891322606\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  50\n",
      "Total training rewards: 31.779772557792874 after n steps = 50 with final reward = 0.00036629260976319886\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  51\n",
      "Total training rewards: 18.27444975977476 after n steps = 51 with final reward = 1.0003712508685156\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  52\n",
      "Total training rewards: 39.279787436591455 after n steps = 52 with final reward = 0.7503872114184793\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  53\n",
      "Total training rewards: 25.77880699542565 after n steps = 53 with final reward = 0.00027887465076017326\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  54\n",
      "Total training rewards: 43.28231931384887 after n steps = 54 with final reward = 0.7503665620416203\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  55\n",
      "Total training rewards: 21.277525243699696 after n steps = 55 with final reward = 0.7503673168256132\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  56\n",
      "Total training rewards: 46.78177771730998 after n steps = 56 with final reward = 0.7502807216220351\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  57\n",
      "Total training rewards: 30.52804648210244 after n steps = 57 with final reward = 1.0002991520554612\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  58\n",
      "Total training rewards: 22.02427037414702 after n steps = 58 with final reward = 1.0003901574103893\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  59\n",
      "Total training rewards: 22.777723273153107 after n steps = 59 with final reward = 1.0003661718834378\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  60\n",
      "Total training rewards: 36.029627762433655 after n steps = 60 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  61\n",
      "Total training rewards: 40.52763571495817 after n steps = 61 with final reward = 0.2503649171420188\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  62\n",
      "Total training rewards: 26.275540541847565 after n steps = 62 with final reward = 0.2503677229997661\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  63\n",
      "Total training rewards: 11.527771966139847 after n steps = 63 with final reward = 0.2503671816335537\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  64\n",
      "Total training rewards: 25.529012265270694 after n steps = 64 with final reward = 1.0003903964712753\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  65\n",
      "Total training rewards: 25.02825310987581 after n steps = 65 with final reward = 0.25036451930819\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  66\n",
      "Total training rewards: 34.530620986141585 after n steps = 66 with final reward = 0.0002820510073511507\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  67\n",
      "Total training rewards: 21.779282828831796 after n steps = 67 with final reward = 0.7503653207430926\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  68\n",
      "Total training rewards: 10.523708513611426 after n steps = 68 with final reward = 1.0003922823458373\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  69\n",
      "Total training rewards: 34.52768377933498 after n steps = 69 with final reward = 0.2503680085211225\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  70\n",
      "Total training rewards: 28.775333104071343 after n steps = 70 with final reward = 0.5003909010699716\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  71\n",
      "Total training rewards: 21.02492137969099 after n steps = 71 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  72\n",
      "Total training rewards: 31.779408202367073 after n steps = 72 with final reward = 0.5003677794063341\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  73\n",
      "Total training rewards: 34.777704535233916 after n steps = 73 with final reward = 1.0003645509111798\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  74\n",
      "Total training rewards: 25.528148802873535 after n steps = 74 with final reward = 0.2503640183313242\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  75\n",
      "Total training rewards: 39.029528045486835 after n steps = 75 with final reward = 0.00039013788943676484\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  76\n",
      "Total training rewards: 13.525897134114373 after n steps = 76 with final reward = 0.7502792364455599\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  77\n",
      "Total training rewards: 27.775856725637087 after n steps = 77 with final reward = 0.00029929949796286934\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  78\n",
      "Total training rewards: 28.776614280769586 after n steps = 78 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  79\n",
      "Total training rewards: 48.02958479142703 after n steps = 79 with final reward = 0.0002795804875787318\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  80\n",
      "Total training rewards: 34.527822352812294 after n steps = 80 with final reward = 0.00012680971462563276\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  81\n",
      "Total training rewards: 43.2776641981125 after n steps = 81 with final reward = 0.0001344455824536794\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  82\n",
      "Total training rewards: 40.77658788233254 after n steps = 82 with final reward = 0.25012528411470486\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  83\n",
      "Total training rewards: 46.7782025813694 after n steps = 83 with final reward = 0.0003642196304091732\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  84\n",
      "Total training rewards: 26.026209370086246 after n steps = 84 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  85\n",
      "Total training rewards: 36.77710471432649 after n steps = 85 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  86\n",
      "Total training rewards: 35.2799945538689 after n steps = 86 with final reward = 1.0003648216710392\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  87\n",
      "Total training rewards: 5.027947966054864 after n steps = 87 with final reward = 0.00012626520760302236\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  88\n",
      "Total training rewards: 18.029875477038637 after n steps = 88 with final reward = 1.0003629858265801\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  89\n",
      "Total training rewards: 41.777168830078665 after n steps = 89 with final reward = 0.5001342015059422\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  90\n",
      "Total training rewards: 38.02964460886571 after n steps = 90 with final reward = 0.00012612630198409716\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  91\n",
      "Total training rewards: 50.27889222239229 after n steps = 91 with final reward = 0.5001337696872851\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  92\n",
      "Total training rewards: 39.77792668461448 after n steps = 92 with final reward = 0.0003866001369027168\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  93\n",
      "Total training rewards: 36.02938534010744 after n steps = 93 with final reward = 1.0003875806131957\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  94\n",
      "Total training rewards: 40.77724159290891 after n steps = 94 with final reward = 0.5003695521658011\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  95\n",
      "Total training rewards: 13.277012664842472 after n steps = 95 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  96\n",
      "Total training rewards: 28.77868548600992 after n steps = 96 with final reward = 1.0003634467568203\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  97\n",
      "Total training rewards: 57.03096885555745 after n steps = 97 with final reward = 0.7503707084596489\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  98\n",
      "Total training rewards: 55.28048564993151 after n steps = 98 with final reward = 0.5001332609165093\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  99\n",
      "Total training rewards: 56.781710604106266 after n steps = 99 with final reward = 1.0003798017795722\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  100\n",
      "Total training rewards: 47.52995759829215 after n steps = 100 with final reward = 0.5003777136748926\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  101\n",
      "Total training rewards: 31.278205900135525 after n steps = 101 with final reward = 0.7502804586416973\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  102\n",
      "Total training rewards: 14.028080616320468 after n steps = 102 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  103\n",
      "Total training rewards: 9.525336455066137 after n steps = 103 with final reward = 0.7503621865917326\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  104\n",
      "Total training rewards: -13.97948696400775 after n steps = 104 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  105\n",
      "Total training rewards: 11.526549098430635 after n steps = 105 with final reward = 0.5003788225736091\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  106\n",
      "Total training rewards: 9.7729533559935 after n steps = 106 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  107\n",
      "Total training rewards: 26.277315020338808 after n steps = 107 with final reward = 0.5003812862246234\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  108\n",
      "Total training rewards: 21.279613365259358 after n steps = 108 with final reward = 0.0003714231906331188\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  109\n",
      "Total training rewards: 31.78076517461221 after n steps = 109 with final reward = 0.5003814321435766\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  110\n",
      "Total training rewards: 38.53312804002742 after n steps = 110 with final reward = 0.5003809947213936\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  111\n",
      "Total training rewards: 11.026891528429445 after n steps = 111 with final reward = 0.0003820882797263419\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  112\n",
      "Total training rewards: 15.782909834705142 after n steps = 112 with final reward = 0.0002836516345485257\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  113\n",
      "Total training rewards: 8.525546120464389 after n steps = 113 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  114\n",
      "Total training rewards: -2.9755335776166674 after n steps = 114 with final reward = 0.00037143890291463323\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  115\n",
      "Total training rewards: 12.278327884175736 after n steps = 115 with final reward = 0.25036723735857247\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  116\n",
      "Total training rewards: 19.02603170942033 after n steps = 116 with final reward = 0.2503859313339905\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  117\n",
      "Total training rewards: 13.026227218660551 after n steps = 117 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  118\n",
      "Total training rewards: 8.278676969856432 after n steps = 118 with final reward = 0.2503661888105411\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  119\n",
      "Total training rewards: 9.528153028762665 after n steps = 119 with final reward = 0.00038988566034214075\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  120\n",
      "Total training rewards: 30.0276555358707 after n steps = 120 with final reward = 1.0003679433250574\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  121\n",
      "Total training rewards: 37.77768931933046 after n steps = 121 with final reward = 1.0003660677793735\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  122\n",
      "Total training rewards: 20.528155091112726 after n steps = 122 with final reward = 0.00038898018019292626\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  123\n",
      "Total training rewards: 48.02220973799132 after n steps = 123 with final reward = 1.000134498223637\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  124\n",
      "Total training rewards: 40.527615046190384 after n steps = 124 with final reward = 0.000366158042309208\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  125\n",
      "Total training rewards: 48.03368967416732 after n steps = 125 with final reward = 0.7503661206063934\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  126\n",
      "Total training rewards: 59.533344835817374 after n steps = 126 with final reward = 0.7503624987186222\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  127\n",
      "Total training rewards: 60.78314896477734 after n steps = 127 with final reward = 0.7503595898844712\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  128\n",
      "Total training rewards: 61.28296968323123 after n steps = 128 with final reward = 0.7503553452611729\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  129\n",
      "Total training rewards: 58.783347767296576 after n steps = 129 with final reward = 0.7503526129412864\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  130\n",
      "Total training rewards: 68.03344412219055 after n steps = 130 with final reward = 0.7503511314128745\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  131\n",
      "Total training rewards: 62.28216170454771 after n steps = 131 with final reward = 0.7503502933904094\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  132\n",
      "Total training rewards: 63.78318340303851 after n steps = 132 with final reward = 0.7503488267383998\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  133\n",
      "Total training rewards: 64.78316827879449 after n steps = 133 with final reward = 0.7503454108618719\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  134\n",
      "Total training rewards: 69.03364900151527 after n steps = 134 with final reward = 0.7503447869260664\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  135\n",
      "Total training rewards: 60.03136993926464 after n steps = 135 with final reward = 1.0003537324727028\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  136\n",
      "Total training rewards: 62.28347501180779 after n steps = 136 with final reward = 0.7503427966799102\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  137\n",
      "Total training rewards: 67.03432017551908 after n steps = 137 with final reward = 0.5003520301018659\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  138\n",
      "Total training rewards: 59.78113868927285 after n steps = 138 with final reward = 0.7503396016032983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  139\n",
      "Total training rewards: 70.53298567453415 after n steps = 139 with final reward = 0.7503368747754225\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  140\n",
      "Total training rewards: 67.03253489382261 after n steps = 140 with final reward = 0.750333179338021\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  141\n",
      "Total training rewards: 56.779829993335255 after n steps = 141 with final reward = 0.500398440658965\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  142\n",
      "Total training rewards: 65.53178100836901 after n steps = 142 with final reward = 1.000364079905\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  143\n",
      "Total training rewards: 68.53134144632213 after n steps = 143 with final reward = 0.7503270065680548\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  144\n",
      "Total training rewards: 62.28081223892996 after n steps = 144 with final reward = 0.25037499672834856\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  145\n",
      "Total training rewards: 64.53175788965318 after n steps = 145 with final reward = 0.7503219288457054\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  146\n",
      "Total training rewards: 71.03208913292875 after n steps = 146 with final reward = 0.5003763878476237\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  147\n",
      "Total training rewards: 64.28101609455487 after n steps = 147 with final reward = 0.7503218144024613\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  148\n",
      "Total training rewards: 61.78100535736309 after n steps = 148 with final reward = 0.7503188468251184\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  149\n",
      "Total training rewards: 68.78091856949663 after n steps = 149 with final reward = 0.7503142140084245\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  150\n",
      "Total training rewards: 59.7789621528304 after n steps = 150 with final reward = 0.2502884467090701\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  151\n",
      "Total training rewards: 59.27945814335464 after n steps = 151 with final reward = 0.7503115015469988\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  152\n",
      "Total training rewards: 69.53127790867993 after n steps = 152 with final reward = 0.7503065067497746\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  153\n",
      "Total training rewards: 67.03014096101803 after n steps = 153 with final reward = 0.7503055275962771\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  154\n",
      "Total training rewards: 63.03054314061419 after n steps = 154 with final reward = 0.7503030461434175\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  155\n",
      "Total training rewards: 65.5293835317177 after n steps = 155 with final reward = 0.2503428507020599\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  156\n",
      "Total training rewards: 72.03013089793684 after n steps = 156 with final reward = 0.7502990232910671\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  157\n",
      "Total training rewards: 65.0290016545406 after n steps = 157 with final reward = 0.7502964230028226\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  158\n",
      "Total training rewards: 65.78003070901701 after n steps = 158 with final reward = 1.0004018068540101\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  159\n",
      "Total training rewards: 67.27995206950587 after n steps = 159 with final reward = 0.750294881974921\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  160\n",
      "Total training rewards: 70.27899023615433 after n steps = 160 with final reward = 0.7502907311366439\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  161\n",
      "Total training rewards: 67.52831362528028 after n steps = 161 with final reward = 0.7502880602822943\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  162\n",
      "Total training rewards: 63.52875539114195 after n steps = 162 with final reward = 0.7502858114326653\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  163\n",
      "Total training rewards: 65.27845123728075 after n steps = 163 with final reward = 0.750283789155016\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  164\n",
      "Total training rewards: 67.27996338703825 after n steps = 164 with final reward = 0.7502818180593546\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  165\n",
      "Total training rewards: 68.5278219960015 after n steps = 165 with final reward = 0.7502782565852865\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  166\n",
      "Total training rewards: 67.77828767560709 after n steps = 166 with final reward = 0.7502785168610132\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  167\n",
      "Total training rewards: 71.0281805021573 after n steps = 167 with final reward = 0.7502771291979052\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  168\n",
      "Total training rewards: 64.02752092477726 after n steps = 168 with final reward = 0.5004032271583813\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  169\n",
      "Total training rewards: 68.77797443026994 after n steps = 169 with final reward = 0.7502730346873387\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  170\n",
      "Total training rewards: 63.52729879315025 after n steps = 170 with final reward = 0.7502735679582125\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  171\n",
      "Total training rewards: 67.77738218927564 after n steps = 171 with final reward = 0.7502710719603944\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  172\n",
      "Total training rewards: 65.27710955808494 after n steps = 172 with final reward = 0.7502704541575468\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  173\n",
      "Total training rewards: 62.276775331296136 after n steps = 173 with final reward = 0.5002695558744898\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  174\n",
      "Total training rewards: 70.77704615539146 after n steps = 174 with final reward = 0.7502671251756704\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  175\n",
      "Total training rewards: 64.77638551735298 after n steps = 175 with final reward = 0.7502665225790774\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  176\n",
      "Total training rewards: 65.5261790081471 after n steps = 176 with final reward = 0.00034919097813195313\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  177\n",
      "Total training rewards: 68.02645233143954 after n steps = 177 with final reward = 0.7502644559036092\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  178\n",
      "Total training rewards: 69.27644201924937 after n steps = 178 with final reward = 0.7502621896185033\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  179\n",
      "Total training rewards: 68.77614102754247 after n steps = 179 with final reward = 0.7502598686888389\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  180\n",
      "Total training rewards: 64.52572364650933 after n steps = 180 with final reward = 0.7502561029908188\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  181\n",
      "Total training rewards: 65.27608541323842 after n steps = 181 with final reward = 0.7502541330271993\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  182\n",
      "Total training rewards: 65.27481589972108 after n steps = 182 with final reward = 0.7502528565864788\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  183\n",
      "Total training rewards: 68.02544287324415 after n steps = 183 with final reward = 0.7502503912049153\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  184\n",
      "Total training rewards: 67.52505672814374 after n steps = 184 with final reward = 0.7502493156236931\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  185\n",
      "Total training rewards: 67.77480174968795 after n steps = 185 with final reward = 0.7502496822118314\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  186\n",
      "Total training rewards: 66.52461715837642 after n steps = 186 with final reward = 0.5002468769032616\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  187\n",
      "Total training rewards: 64.5257329187389 after n steps = 187 with final reward = 0.750246266603544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  188\n",
      "Total training rewards: 67.77491143683346 after n steps = 188 with final reward = 0.7503409421085859\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  189\n",
      "Total training rewards: 69.77432213814298 after n steps = 189 with final reward = 0.750241776995363\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  190\n",
      "Total training rewards: 67.02475746801014 after n steps = 190 with final reward = 0.7503868028841452\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  191\n",
      "Total training rewards: 66.77494963362501 after n steps = 191 with final reward = 0.7502381656521407\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  192\n",
      "Total training rewards: 67.77343036255309 after n steps = 192 with final reward = 0.750238004838061\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  193\n",
      "Total training rewards: 64.02416846204078 after n steps = 193 with final reward = 0.7502355624639736\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  194\n",
      "Total training rewards: 69.52382392953176 after n steps = 194 with final reward = 0.7502344346551801\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  195\n",
      "Total training rewards: 61.27381552345624 after n steps = 195 with final reward = 0.7502333154417844\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  196\n",
      "Total training rewards: 70.27422931559691 after n steps = 196 with final reward = 0.7502328371105125\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  197\n",
      "Total training rewards: 70.27474777578216 after n steps = 197 with final reward = 0.7502310179958148\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  198\n",
      "Total training rewards: 70.02278256110615 after n steps = 198 with final reward = 0.7502293007354192\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  199\n",
      "Total training rewards: 69.52329962372518 after n steps = 199 with final reward = 0.7502293007354192\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  200\n",
      "Total training rewards: 69.77252712990504 after n steps = 200 with final reward = 0.7502278374433371\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  201\n",
      "Total training rewards: 73.27340006365944 after n steps = 201 with final reward = 0.7502256342803305\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  202\n",
      "Total training rewards: 66.02270716304689 after n steps = 202 with final reward = 0.7502240597172285\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  203\n",
      "Total training rewards: 70.02735641842983 after n steps = 203 with final reward = 0.7502233165672543\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  204\n",
      "Total training rewards: 68.0218986683636 after n steps = 204 with final reward = 0.7502200996408556\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  205\n",
      "Total training rewards: 67.2721739149792 after n steps = 205 with final reward = 0.7502192358143107\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  206\n",
      "Total training rewards: 69.52276996619989 after n steps = 206 with final reward = 0.0003277338012036608\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  207\n",
      "Total training rewards: 69.52191475481187 after n steps = 207 with final reward = 0.7502178064228946\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  208\n",
      "Total training rewards: 66.27223394263513 after n steps = 208 with final reward = 0.7502166789750987\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  209\n",
      "Total training rewards: 68.5226534826173 after n steps = 209 with final reward = 0.7502167508182602\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  210\n",
      "Total training rewards: 66.0214300456746 after n steps = 210 with final reward = 0.7502152164432417\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  211\n",
      "Total training rewards: 72.02259131244448 after n steps = 211 with final reward = 0.7502142560472621\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  212\n",
      "Total training rewards: 72.52186135051791 after n steps = 212 with final reward = 0.75021480037201\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  213\n",
      "Total training rewards: 66.27172549611048 after n steps = 213 with final reward = 0.2501386150549997\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  214\n",
      "Total training rewards: 70.52166976533226 after n steps = 214 with final reward = 0.7502134191590156\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  215\n",
      "Total training rewards: 69.2730949898388 after n steps = 215 with final reward = 0.5004201271677318\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  216\n",
      "Total training rewards: 64.02523125364448 after n steps = 216 with final reward = 0.7502111169733426\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  217\n",
      "Total training rewards: 69.27212464374107 after n steps = 217 with final reward = 0.7502092551314645\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  218\n",
      "Total training rewards: 70.52148562874864 after n steps = 218 with final reward = 0.7502079327681002\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  219\n",
      "Total training rewards: 68.52157091849574 after n steps = 219 with final reward = 0.7502075405254269\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  220\n",
      "Total training rewards: 71.77082534159983 after n steps = 220 with final reward = 0.7502058487629901\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  221\n",
      "Total training rewards: 66.77251775473297 after n steps = 221 with final reward = 0.5003134071750368\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  222\n",
      "Total training rewards: 67.27178058658033 after n steps = 222 with final reward = 0.7502053483917395\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  223\n",
      "Total training rewards: 73.02057663505178 after n steps = 223 with final reward = 0.7502042006382647\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  224\n",
      "Total training rewards: 70.77079833850496 after n steps = 224 with final reward = 0.7502032639380891\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  225\n",
      "Total training rewards: 66.77115671128603 after n steps = 225 with final reward = 0.500263458080138\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  226\n",
      "Total training rewards: 71.77170425325177 after n steps = 226 with final reward = 0.5002566669941374\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  227\n",
      "Total training rewards: 74.02036632598615 after n steps = 227 with final reward = 0.7501989479507077\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  228\n",
      "Total training rewards: 72.27089522772647 after n steps = 228 with final reward = 0.7501982224864472\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  229\n",
      "Total training rewards: 69.52056974324536 after n steps = 229 with final reward = 0.7503496115289426\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  230\n",
      "Total training rewards: 72.5199651441554 after n steps = 230 with final reward = 0.7501958171317142\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  231\n",
      "Total training rewards: 71.27054624344997 after n steps = 231 with final reward = 0.0003469782456254049\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  232\n",
      "Total training rewards: 68.7702118266749 after n steps = 232 with final reward = 0.7501939324112828\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  233\n",
      "Total training rewards: 68.76922463436362 after n steps = 233 with final reward = 0.7501924521498996\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  234\n",
      "Total training rewards: 70.51999821686395 after n steps = 234 with final reward = 0.750191921462706\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  235\n",
      "Total training rewards: 72.26979107714068 after n steps = 235 with final reward = 0.7501904978090974\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  236\n",
      "Total training rewards: 73.01969234060284 after n steps = 236 with final reward = 0.7501885818828565\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  237\n",
      "Total training rewards: 68.52023129697116 after n steps = 237 with final reward = 0.75032348654292\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  238\n",
      "Total training rewards: 69.51955228385275 after n steps = 238 with final reward = 0.7501867278042251\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  239\n",
      "Total training rewards: 70.52136988614917 after n steps = 239 with final reward = 0.7501851301183714\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  240\n",
      "Total training rewards: 72.51985347442557 after n steps = 240 with final reward = 0.7501846667988341\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  241\n",
      "Total training rewards: 71.02015809652927 after n steps = 241 with final reward = 0.7503252944746431\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  242\n",
      "Total training rewards: 72.0193469817419 after n steps = 242 with final reward = 0.7501842061279957\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  243\n",
      "Total training rewards: 69.26888771381826 after n steps = 243 with final reward = 0.7504010455625623\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  244\n",
      "Total training rewards: 71.51865833882759 after n steps = 244 with final reward = 0.7501833125716272\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  245\n",
      "Total training rewards: 70.26912802420335 after n steps = 245 with final reward = 0.7501815083810429\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  246\n",
      "Total training rewards: 71.51908982807468 after n steps = 246 with final reward = 0.7501806949502298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  247\n",
      "Total training rewards: 79.77415039646311 after n steps = 247 with final reward = 1.0003208853323788\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  248\n",
      "Total training rewards: 76.26902285021727 after n steps = 248 with final reward = 0.7501796744746068\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  249\n",
      "Total training rewards: 69.5190829253616 after n steps = 249 with final reward = 0.750179426305443\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  250\n",
      "Total training rewards: 68.76843092960989 after n steps = 250 with final reward = 0.7501779101023589\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  251\n",
      "Total training rewards: 72.51854821992413 after n steps = 251 with final reward = 0.7501774957209275\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  252\n",
      "Total training rewards: 74.52871291508825 after n steps = 252 with final reward = 1.0003193445897456\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  253\n",
      "Total training rewards: 91.03109824391684 after n steps = 253 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  254\n",
      "Total training rewards: 94.78119499455784 after n steps = 254 with final reward = 1.0003197540058864\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  255\n",
      "Total training rewards: 89.53124324368466 after n steps = 255 with final reward = 1.000319856524003\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  256\n",
      "Total training rewards: 97.28228571251437 after n steps = 256 with final reward = 1.0003196515534654\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  257\n",
      "Total training rewards: 94.28196481209814 after n steps = 257 with final reward = 0.5004613133163849\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  258\n",
      "Total training rewards: 97.03217332624172 after n steps = 258 with final reward = 1.0003196515534654\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  259\n",
      "Total training rewards: 97.78202448185168 after n steps = 259 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  260\n",
      "Total training rewards: 95.2818077870864 after n steps = 260 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  261\n",
      "Total training rewards: 94.28163554910793 after n steps = 261 with final reward = 0.750409796738272\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  262\n",
      "Total training rewards: 90.28066573190893 after n steps = 262 with final reward = 1.0003193445897456\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  263\n",
      "Total training rewards: 92.03136703820375 after n steps = 263 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  264\n",
      "Total training rewards: 96.03186093411698 after n steps = 264 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  265\n",
      "Total training rewards: 92.78190094971686 after n steps = 265 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  266\n",
      "Total training rewards: 93.03124671534606 after n steps = 266 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  267\n",
      "Total training rewards: 95.53224376737222 after n steps = 267 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  268\n",
      "Total training rewards: 91.03035995821891 after n steps = 268 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  269\n",
      "Total training rewards: 92.53083209033203 after n steps = 269 with final reward = 1.0003197540058864\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  270\n",
      "Total training rewards: 90.78132165761137 after n steps = 270 with final reward = 1.0003196515534654\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  271\n",
      "Total training rewards: 92.53185695002092 after n steps = 271 with final reward = 1.0003196515534654\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  272\n",
      "Total training rewards: 94.53165809931821 after n steps = 272 with final reward = 1.0003196515534654\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  273\n",
      "Total training rewards: 93.78098938681687 after n steps = 273 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  274\n",
      "Total training rewards: 94.78133971375881 after n steps = 274 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  275\n",
      "Total training rewards: 95.78112681452505 after n steps = 275 with final reward = 1.0003194468454581\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  276\n",
      "Total training rewards: 96.28168502098201 after n steps = 276 with final reward = 1.000319242399477\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  277\n",
      "Total training rewards: 96.28161413502903 after n steps = 277 with final reward = 1.000319242399477\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  278\n",
      "Total training rewards: 95.53187210238397 after n steps = 278 with final reward = 1.0003193445897456\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  279\n",
      "Total training rewards: 97.78144790045671 after n steps = 279 with final reward = 0.7501151012891344\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  280\n",
      "Total training rewards: 95.03099333044617 after n steps = 280 with final reward = 1.000319242399477\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  281\n",
      "Total training rewards: 90.03120278529026 after n steps = 281 with final reward = 1.0003193445897456\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  282\n",
      "Total training rewards: 92.53147988160747 after n steps = 282 with final reward = 1.0003193445897456\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  283\n",
      "Total training rewards: 95.28123742775361 after n steps = 283 with final reward = 1.000319549166677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  284\n",
      "Total training rewards: 96.03183085816448 after n steps = 284 with final reward = 0.0004232196140830846\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  285\n",
      "Total training rewards: 96.53216497395664 after n steps = 285 with final reward = 1.000319140274589\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  286\n",
      "Total training rewards: 94.03075234386536 after n steps = 286 with final reward = 1.0003190382150193\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  287\n",
      "Total training rewards: 95.03129286478561 after n steps = 287 with final reward = 1.0003190382150193\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  288\n",
      "Total training rewards: 95.28134190201607 after n steps = 288 with final reward = 1.0003190382150193\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  289\n",
      "Total training rewards: 98.53192208400993 after n steps = 289 with final reward = 1.0003188342915836\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  290\n",
      "Total training rewards: 92.53070869783082 after n steps = 290 with final reward = 1.00031863062867\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  291\n",
      "Total training rewards: 97.53197267691317 after n steps = 291 with final reward = 1.000318427225779\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  292\n",
      "Total training rewards: 95.78153354439996 after n steps = 292 with final reward = 1.0003185288947527\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  293\n",
      "Total training rewards: 98.03169536756609 after n steps = 293 with final reward = 1.000318427225779\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  294\n",
      "Total training rewards: 93.78213547556814 after n steps = 294 with final reward = 1.000318427225779\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  295\n",
      "Total training rewards: 97.53165695486086 after n steps = 295 with final reward = 1.0003183256216865\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  296\n",
      "Total training rewards: 97.28150696119906 after n steps = 296 with final reward = 1.0003182240824133\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  297\n",
      "Total training rewards: 98.78148741906229 after n steps = 297 with final reward = 1.0003181226078972\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  298\n",
      "Total training rewards: 93.78112021905743 after n steps = 298 with final reward = 1.0003181226078972\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  299\n",
      "Total training rewards: 92.78078959407384 after n steps = 299 with final reward = 1.0003182240824133\n",
      "Copying main network weights to the target network weights\n",
      "All Nodes Were Saved !\n",
      "[25.49442104953507, 28.503396276099668, 25.658674563054642, 33.61233597965538, 36.34342311003635, 32.07253883114332, 34.81140759180278, 28.304570746145604, 29.049571584141386, 26.54684280357214, 19.538406622607003, 29.789151212575522, 27.790367927874456, 36.534572350116726, 37.53719961380015, 28.78302505550288, 33.28158025362068, 28.780037951923415, 34.28200028414995, 25.779749912129958, 23.027832667947536, 31.52857843961642, 37.03109140550478, 43.53257117819618, 28.530682515679533, 44.78117063544018, 33.78175449611723, 14.275497019527288, 24.77691301334604, 31.77991561472446, 27.028452393133065, 26.777671238417728, 30.278500932929383, 28.279181708306687, 31.778044939099185, 29.030284532830137, 18.02683813008886, 22.275871928762545, 33.02975944157259, 37.529470470612246, 31.279516851395947, 33.027295276089, 28.526841264095363, 25.02673479728009, 29.77700748782052, 32.029634865517735, 24.277335466215877, 15.529066002853945, 29.529101145743024, 28.777645055378585, 31.779772557792874, 18.27444975977476, 39.279787436591455, 25.77880699542565, 43.28231931384887, 21.277525243699696, 46.78177771730998, 30.52804648210244, 22.02427037414702, 22.777723273153107, 36.029627762433655, 40.52763571495817, 26.275540541847565, 11.527771966139847, 25.529012265270694, 25.02825310987581, 34.530620986141585, 21.779282828831796, 10.523708513611426, 34.52768377933498, 28.775333104071343, 21.02492137969099, 31.779408202367073, 34.777704535233916, 25.528148802873535, 39.029528045486835, 13.525897134114373, 27.775856725637087, 28.776614280769586, 48.02958479142703, 34.527822352812294, 43.2776641981125, 40.77658788233254, 46.7782025813694, 26.026209370086246, 36.77710471432649, 35.2799945538689, 5.027947966054864, 18.029875477038637, 41.777168830078665, 38.02964460886571, 50.27889222239229, 39.77792668461448, 36.02938534010744, 40.77724159290891, 13.277012664842472, 28.77868548600992, 57.03096885555745, 55.28048564993151, 56.781710604106266, 47.52995759829215, 31.278205900135525, 14.028080616320468, 9.525336455066137, -13.97948696400775, 11.526549098430635, 9.7729533559935, 26.277315020338808, 21.279613365259358, 31.78076517461221, 38.53312804002742, 11.026891528429445, 15.782909834705142, 8.525546120464389, -2.9755335776166674, 12.278327884175736, 19.02603170942033, 13.026227218660551, 8.278676969856432, 9.528153028762665, 30.0276555358707, 37.77768931933046, 20.528155091112726, 48.02220973799132, 40.527615046190384, 48.03368967416732, 59.533344835817374, 60.78314896477734, 61.28296968323123, 58.783347767296576, 68.03344412219055, 62.28216170454771, 63.78318340303851, 64.78316827879449, 69.03364900151527, 60.03136993926464, 62.28347501180779, 67.03432017551908, 59.78113868927285, 70.53298567453415, 67.03253489382261, 56.779829993335255, 65.53178100836901, 68.53134144632213, 62.28081223892996, 64.53175788965318, 71.03208913292875, 64.28101609455487, 61.78100535736309, 68.78091856949663, 59.7789621528304, 59.27945814335464, 69.53127790867993, 67.03014096101803, 63.03054314061419, 65.5293835317177, 72.03013089793684, 65.0290016545406, 65.78003070901701, 67.27995206950587, 70.27899023615433, 67.52831362528028, 63.52875539114195, 65.27845123728075, 67.27996338703825, 68.5278219960015, 67.77828767560709, 71.0281805021573, 64.02752092477726, 68.77797443026994, 63.52729879315025, 67.77738218927564, 65.27710955808494, 62.276775331296136, 70.77704615539146, 64.77638551735298, 65.5261790081471, 68.02645233143954, 69.27644201924937, 68.77614102754247, 64.52572364650933, 65.27608541323842, 65.27481589972108, 68.02544287324415, 67.52505672814374, 67.77480174968795, 66.52461715837642, 64.5257329187389, 67.77491143683346, 69.77432213814298, 67.02475746801014, 66.77494963362501, 67.77343036255309, 64.02416846204078, 69.52382392953176, 61.27381552345624, 70.27422931559691, 70.27474777578216, 70.02278256110615, 69.52329962372518, 69.77252712990504, 73.27340006365944, 66.02270716304689, 70.02735641842983, 68.0218986683636, 67.2721739149792, 69.52276996619989, 69.52191475481187, 66.27223394263513, 68.5226534826173, 66.0214300456746, 72.02259131244448, 72.52186135051791, 66.27172549611048, 70.52166976533226, 69.2730949898388, 64.02523125364448, 69.27212464374107, 70.52148562874864, 68.52157091849574, 71.77082534159983, 66.77251775473297, 67.27178058658033, 73.02057663505178, 70.77079833850496, 66.77115671128603, 71.77170425325177, 74.02036632598615, 72.27089522772647, 69.52056974324536, 72.5199651441554, 71.27054624344997, 68.7702118266749, 68.76922463436362, 70.51999821686395, 72.26979107714068, 73.01969234060284, 68.52023129697116, 69.51955228385275, 70.52136988614917, 72.51985347442557, 71.02015809652927, 72.0193469817419, 69.26888771381826, 71.51865833882759, 70.26912802420335, 71.51908982807468, 79.77415039646311, 76.26902285021727, 69.5190829253616, 68.76843092960989, 72.51854821992413, 74.52871291508825, 91.03109824391684, 94.78119499455784, 89.53124324368466, 97.28228571251437, 94.28196481209814, 97.03217332624172, 97.78202448185168, 95.2818077870864, 94.28163554910793, 90.28066573190893, 92.03136703820375, 96.03186093411698, 92.78190094971686, 93.03124671534606, 95.53224376737222, 91.03035995821891, 92.53083209033203, 90.78132165761137, 92.53185695002092, 94.53165809931821, 93.78098938681687, 94.78133971375881, 95.78112681452505, 96.28168502098201, 96.28161413502903, 95.53187210238397, 97.78144790045671, 95.03099333044617, 90.03120278529026, 92.53147988160747, 95.28123742775361, 96.03183085816448, 96.53216497395664, 94.03075234386536, 95.03129286478561, 95.28134190201607, 98.53192208400993, 92.53070869783082, 97.53197267691317, 95.78153354439996, 98.03169536756609, 93.78213547556814, 97.53165695486086, 97.28150696119906, 98.78148741906229, 93.78112021905743, 92.78078959407384]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 191974\n",
      "Collisions : 86508\n",
      "Received : 105465\n",
      "Processed : 191642\n",
      "Loss :  0\n",
      "DER1 : 54.94 %\n",
      "DER2 : 54.94 %\n",
      "Energy : 2.653176176639998 J\n",
      "[0.8271411200000001, 0.33549312, 1.6285559466666664, 0.0, 0.0, 0.0]\n",
      "DER ==> 18.002171353162403\n",
      "[0.1912292631744937, 0.5112041590986157, 0.0384994280619183, 1.0, 1.0, 1.0]\n",
      "[0.8271411200000001, 0.33549312, 1.6285559466666664, 0.0, 0.0, 0.0]\n",
      "Error : 36.93\n",
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 192095\n",
      "Collisions : 87480\n",
      "Received : 104611\n",
      "Processed : 187945\n",
      "Loss :  0\n",
      "DER1 : 54.46 %\n",
      "DER2 : 54.46 %\n",
      "Energy : 3.235583364096018 J\n",
      "[0.6136610133333333, 0.50049536, 1.3913156266666666, 0.8859443199999998, 0.50413568, 0.5011865600000001]\n",
      "DER ==> 23.3423140124678\n",
      "[0.2930763784397639, 0.36751515613351765, 0.06187548292140641, 0.17001159472380567, 0.36484912756096294, 0.3670074541861174]\n",
      "[0.6136610133333333, 0.50049536, 1.3913156266666666, 0.8859443199999998, 0.50413568, 0.5011865600000001]\n",
      "Error : 31.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_names = ['nodes_2000.alloc','nodes_2000.rl']\n",
    "for i in range (2):\n",
    "    file_name = file_names [i]\n",
    "    nodes = load_nodes(file_name)\n",
    "    sf_distribution, tp_dist, u_ch = compute_sf_ch_utilization(nodes)\n",
    "    def transmit(env, node):\n",
    "\n",
    "        \"\"\"transmit\n",
    "\n",
    "        Parameters\n",
    "                #print (\"frequency coll 125\")\n",
    "        ----------\n",
    "            env : simpy.core.environment\n",
    "            node\n",
    "        \"\"\"\n",
    "        while True:\n",
    "\n",
    "            r = random.expovariate(1.0 / float(AVGSENDTIME))\n",
    "            # r = random.expovariate(1.0 / float(avg_gen(node, AVGSENDTIME)))\n",
    "            # node.freq = random.choice(ch)\n",
    "            # node.packet.freq =node.freq\n",
    "            # get_next_ch(node)\n",
    "            # print('duty cycle : ',r-(node.rectime*99))\n",
    "            # if (r-(node.rectime*99) < 0):\n",
    "            # r = node.rectime*99\n",
    "            yield env.timeout(r)\n",
    "            \"\"\"print(\n",
    "                \"Node-ID :{} SF :{} TX :{} Channel:{}  \".format(\n",
    "                    node.id, node.sf, node.tx, node.freq\n",
    "                )\n",
    "            )\"\"\"\n",
    "            node.sent = node.sent + 1\n",
    "            if node in packetsAtBS:\n",
    "                print(\"ERROR: packet already in\")\n",
    "            else:\n",
    "                sensitivity = config_dict[node.sf - 7][\"sens\"]\n",
    "                # print(\"node id: {} sf :{} s: {} \".format(node.id,node.sf,sensitivity))\n",
    "                if node.packet.rssi < sensitivity:\n",
    "                    node.packet.lost = True\n",
    "                else:\n",
    "                    node.packet.lost = False\n",
    "                    if (\n",
    "                        checkcollision(\n",
    "                            env,\n",
    "                            node.packet,\n",
    "                            packetsAtBS,\n",
    "                            maxBSReceives,\n",
    "                            full_collision,\n",
    "                        )\n",
    "                        == 1\n",
    "                    ):\n",
    "                        node.packet.collided = 1\n",
    "                    else:\n",
    "                        node.packet.collided = 0\n",
    "                        node.received = node.received + 1\n",
    "                    packetsAtBS.append(node)\n",
    "                    node.packet.addTime = env.now\n",
    "            # print('rectime : ',node.packet.rectime)\n",
    "            yield env.timeout(node.packet.rectime)\n",
    "\n",
    "            if node.packet.lost:\n",
    "                global nrLost\n",
    "                nrLost += 1\n",
    "            if node.packet.collided == 1:\n",
    "                global nrCollisions\n",
    "                nrCollisions = nrCollisions + 1\n",
    "            if not node.packet.collided and not node.packet.lost:\n",
    "                global nrReceived\n",
    "                nrReceived = nrReceived + 1\n",
    "            if node.packet.processed == 1:\n",
    "                global nrProcessed\n",
    "                nrProcessed = nrProcessed + 1\n",
    "            if node in packetsAtBS:\n",
    "                packetsAtBS.remove(node)\n",
    "                # reset the packet\n",
    "            node.packet.collided = 0\n",
    "            node.packet.processed = 0\n",
    "            node.packet.lost = False\n",
    "\n",
    "    MODEL = 6\n",
    "    packetsAtBS = []\n",
    "    sim_env = simpy.Environment()\n",
    "    bsId = 1   \n",
    "    maxBSReceives = 8\n",
    "    nrCollisions = 0\n",
    "    nrReceived = 0\n",
    "    nrProcessed = 0\n",
    "    nrLost = 0\n",
    "    full_collision = 1\n",
    "    # AVGSENDTIME = 800\n",
    "    simtime = 7200\n",
    "    # print(NODES)\n",
    "    for n in nodes:\n",
    "        # print(n.packet)\n",
    "        sim_env.process(transmit(sim_env, n))\n",
    "    print(\"Simulation Started\")\n",
    "    sim_env.run(until=simtime)\n",
    "    print(\"Simulation Finished\")\n",
    "\n",
    "    sent = sum(n.sent for n in nodes)\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent\n",
    "            for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "\n",
    "    print(\"Sent :\", sent)\n",
    "    print(\"Collisions :\", nrCollisions)\n",
    "    print(\"Received :\", nrReceived)\n",
    "    print(\"Processed :\", nrProcessed)\n",
    "\n",
    "    print(\"Loss : \", nrLost)\n",
    "    # print(energy)\n",
    "    der1 = (float(nrReceived) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER1 : {:.2f} %\".format(der1))\n",
    "    der2 = (float(sent - nrCollisions) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER2 : {:.2f} %\".format(der2))\n",
    "    print(\"Energy : {} J\".format(energy))\n",
    "\n",
    "    DER, DERs, charges = mathematical_model(\n",
    "        len(nodes), sf_distribution, AVGSENDTIME\n",
    "    )\n",
    "    print(charges)\n",
    "    DER = DER * 100\n",
    "    print(\"DER ==>\", DER)\n",
    "    print(DERs)\n",
    "    print(charges)\n",
    "    error = abs(DER - der1)\n",
    "    print(\"Error : {:.2f}\".format(error))\n",
    "\n",
    "    data = {\n",
    "        \"Experience\": 1,\n",
    "        \"Rho_max\":rho_max,\n",
    "        \"Packet_generation\":p,\n",
    "        \"Nodes\": len(nodes),\n",
    "        \"PathLoss\": MODEL,\n",
    "        \"Packet_Sent\": sent,\n",
    "        \"Packet_Loss\": nrLost,\n",
    "        \"Collisions\": nrCollisions,\n",
    "        \"Energy\": energy,\n",
    "        \"DER\": der1,\n",
    "        \"Mathematical-Model\": DER,\n",
    "        \"Error\": error,\n",
    "        \"SF7\": charges[0],\n",
    "        \"SF8\": charges[1],\n",
    "        \"SF9\": charges[2],\n",
    "        \"SF10\": charges[3],\n",
    "        \"SF11\": charges[4],\n",
    "        \"SF12\": charges[5],\n",
    "        \"DSF7\": sf_distribution[0],\n",
    "        \"DSF8\": sf_distribution[1],\n",
    "        \"DSF9\": sf_distribution[2],\n",
    "        \"DSF10\": sf_distribution[3],\n",
    "        \"DSF11\": sf_distribution[4],\n",
    "        \"DSF12\": sf_distribution[5],\n",
    "       }\n",
    "\n",
    "    finalReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devices = load_nodes('nodes_1000.alloc')\n",
    "#print_nodes(devices)\n",
    "#model = agent.create_model(env.action_space.shape, env.observation_space.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
