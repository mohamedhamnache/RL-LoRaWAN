{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import simpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling2D, Activation, Flatten\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "# log-distance loss model parameters\n",
    "\n",
    "PTX = 14.0\n",
    "GAMMA = 2.08\n",
    "D0 = 40.0\n",
    "VAR = 0  # variance ignored for now\n",
    "LPLD0 = 127.41\n",
    "\n",
    "# Gate way localisation\n",
    "BSX = 0.0\n",
    "BSY = 0.0\n",
    "HM = 1.0  # m\n",
    "HB = 15.0  # m\n",
    "\n",
    "RAY = 700.0\n",
    "\n",
    "# Created Nodes\n",
    "NODES = []\n",
    "\n",
    "packetlen = 20\n",
    "\n",
    "\n",
    "BW = 125\n",
    "PL = 20\n",
    "CR = 1\n",
    "\n",
    "p = 300\n",
    "AVGSENDTIME = p\n",
    "# Inter-SF Thresshold Mateix\n",
    "\n",
    "IS7 = np.array([6, -8, -9, -9, -9, -9])\n",
    "IS8 = np.array([-11, 6, -11, -12, -13, -13])\n",
    "IS9 = np.array([-15, -13, 6, -13, -14, -15])\n",
    "IS10 = np.array([-19, -18, -17, 6, -17, -18])\n",
    "IS11 = np.array([-22, -22, -21, -20, 6, -20])\n",
    "IS12 = np.array([-25, -25, -25, -24, -23, 6])\n",
    "SIR = np.array([IS7, IS8, IS9, IS10, IS11, IS12])\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    0: {\"sf\": 7, \"sens\": -126.5, \"snr\": -7.5},\n",
    "    1: {\"sf\": 8, \"sens\": -127.25, \"snr\": -10},\n",
    "    2: {\"sf\": 9, \"sens\": -131.25, \"snr\": -12.5},\n",
    "    3: {\"sf\": 10, \"sens\": -132.75, \"snr\": -15},\n",
    "    4: {\"sf\": 11, \"sens\": -133.25, \"snr\": -17.5},\n",
    "    5: {\"sf\": 12, \"sens\": -134.5, \"snr\": -20},\n",
    "}\n",
    "\n",
    "ch = [868100000, 868300000, 868500000]\n",
    "tpx = [2, 5, 8, 11, 14]\n",
    "MODEL = 6\n",
    "SEED = 913\n",
    "\n",
    "rho_max = 0.5\n",
    "tp_max = 14\n",
    "tp_min = 2\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "TX = [\n",
    "    22,\n",
    "    22,\n",
    "    22,\n",
    "    23,  # RFO/PA0: -2..1\n",
    "    24,\n",
    "    24,\n",
    "    24,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    26,\n",
    "    31,\n",
    "    32,\n",
    "    34,\n",
    "    35,\n",
    "    44,  # PA_BOOST/PA1: 2..14\n",
    "    82,\n",
    "    85,\n",
    "    90,  # PA_BOOST/PA1: 15..17\n",
    "    105,\n",
    "    115,\n",
    "    125,\n",
    "]  # PA_BOOST/PA1+PA2: 18..20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SF_alloc_ch_utilization(NODES):\n",
    "    sorted_nodes = []\n",
    "    sorted_nodes = sorted(NODES, key=lambda x: x.dist, reverse=False)\n",
    "    c_class = np.zeros(6, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_u = np.zeros(ch_len, dtype=int)\n",
    "    # print(c_class)\n",
    "    for n in sorted_nodes:\n",
    "        sf = n.sf\n",
    "        # print(sf)\n",
    "        config = config_dict[sf - 7]\n",
    "        # print(config)\n",
    "        for i in range(sf, 12):\n",
    "            if n.snr < config[\"snr\"] or n.rssi < config[\"sens\"]:\n",
    "                if n.sf < 12:\n",
    "                    n.sf = n.sf + 1\n",
    "                    n.packet.sf = n.sf\n",
    "                    n.update_rectime(n.sf)\n",
    "            config = config_dict[n.sf - 7]\n",
    "        c_class[n.sf - 7] = c_class[n.sf - 7] + 1\n",
    "        ch_index = ch.index(n.freq)\n",
    "        ch_u[ch_index] = ch_u[ch_index] + 1\n",
    "    return sorted_nodes, c_class, ch_u\n",
    "\n",
    "\n",
    "def compute_sf_ch_utilization(nodes):\n",
    "    sf_dist = np.zeros(6, dtype=int)\n",
    "    tp_dist = np.zeros(5, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_dist = np.zeros(ch_len, dtype=int)\n",
    "    # print(ch_dist)\n",
    "    # print(ch_len)\n",
    "    # print_nodes(nodes)\n",
    "    for n in nodes:\n",
    "        sf = n.sf\n",
    "        freq = n.freq\n",
    "        tx = n.tx\n",
    "        sf_dist[sf - 7] = sf_dist[sf - 7] + 1\n",
    "        tp_index = tpx.index(tx)\n",
    "        # print('index',tp_index)\n",
    "        tp_dist[tp_index] = tp_dist[tp_index] + 1\n",
    "\n",
    "        # print('Channel : ',freq)\n",
    "        ch_index = ch.index(freq)\n",
    "        # print('index',ch_index)\n",
    "        ch_dist[ch_index] = ch_dist[ch_index] + 1\n",
    "    return sf_dist, tp_dist, ch_dist\n",
    "\n",
    "\n",
    "def energy_consumption(nodes):\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "    return energy\n",
    "\n",
    "\n",
    "def print_nodes(nodes):\n",
    "    for n in nodes:\n",
    "        print(\"node_id: {} SF: {} TP: {} CH: {} RSSI: {} SNR: {}\".format(n.id, n.sf, n.tx, n.freq,n.rssi,n.snr))\n",
    "\n",
    "\n",
    "def reward_plot(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Packet:\n",
    "    def __init__(self, nodeid, sf, bw, freq, rssi, rectime):\n",
    "        self.nodeid = nodeid\n",
    "        self.pl = packetlen\n",
    "        self.arriveTime = 0\n",
    "        # Reception Parameters\n",
    "        self.sf = sf\n",
    "        self.bw = bw\n",
    "        self.freq = freq\n",
    "        self.rssi = rssi\n",
    "        self.rectime = rectime\n",
    "        self.collided = 0\n",
    "        self.processed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class Device:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.dist = 0\n",
    "        found = False\n",
    "        while not found:\n",
    "            a = random.random()\n",
    "            b = random.random()\n",
    "            if b < a:\n",
    "                a, b = b, a\n",
    "            posx = b * RAY * math.cos(2 * math.pi * a / b) + BSX\n",
    "            posy = b * RAY * math.sin(2 * math.pi * a / b) + BSY\n",
    "            if len(NODES) > 0:\n",
    "                for index, n in enumerate(NODES):\n",
    "                    dist = np.sqrt(((abs(n.x - posx)) ** 2) + ((abs(n.y - posy)) ** 2))\n",
    "                    if dist >= 20:\n",
    "                        found = 1\n",
    "                        self.x = posx\n",
    "                        self.y = posy\n",
    "                        break\n",
    "            else:\n",
    "                self.x = posx\n",
    "                self.y = posy\n",
    "                found = True\n",
    "\n",
    "        dist_2d = np.sqrt(\n",
    "            (self.x - BSX) * (self.x - BSX) + (self.y - BSY) * (self.y - BSY)\n",
    "        )\n",
    "        # self.dist = np.sqrt((dist_2d)**2+(HB-HM)**2)\n",
    "        self.dist = dist_2d\n",
    "        # Radio Parameters\n",
    "        self.bw = BW\n",
    "        self.sf = 7\n",
    "        self.tx = PTX\n",
    "        self.cr = 1\n",
    "        self.freq = random.choice(ch)\n",
    "        # self.freq = 868100000\n",
    "        self.rssi = self.tx - self.estimatePathLoss(MODEL)\n",
    "        self.snr = random.randrange(-10, 20)\n",
    "        self.rectime = self.airtime(self.sf, self.cr, packetlen, BW)\n",
    "        self.packet = Packet(\n",
    "            self.id, self.sf, self.bw, self.freq, self.rssi, self.rectime\n",
    "        )\n",
    "        self.sent = 0\n",
    "        self.received = 0\n",
    "\n",
    "    def airtime(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def log_distance_loss(self, dist):\n",
    "        Lpl = LPLD0 + 10 * GAMMA * math.log10(dist / D0)\n",
    "        rssi = PTX - Lpl\n",
    "        return rssi\n",
    "\n",
    "    def update_rectime(self, sf):\n",
    "        self.rectime = self.airtime(sf, self.cr, packetlen, BW)\n",
    "        self.packet.rectime = self.rectime\n",
    "\n",
    "    def estimatePathLoss(self, model):\n",
    "        # Log-Distance model\n",
    "        if model == 0:\n",
    "            Lpl = LPLD0 + 10 * GAMMA * math.log10(self.dist / D0)\n",
    "\n",
    "        # Okumura-Hata model\n",
    "        elif model >= 1 and model <= 4:\n",
    "            # small and medium-size cities\n",
    "            if model == 1:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = 0\n",
    "            # metropolitan areas\n",
    "            elif model == 2:\n",
    "                if self.freq <= 200000000:\n",
    "                    ahm = 8.29 * ((math.log10(1.54 * HM)) ** 2) - 1.1\n",
    "                elif self.freq >= 400000000:\n",
    "                    ahm = 3.2 * ((math.log10(11.75 * HM)) ** 2) - 4.97\n",
    "                C = 0\n",
    "            # suburban enviroments\n",
    "            elif model == 3:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = -2 * ((math.log10(self.freq) - math.log10(28000000)) ** 2) - 5.4\n",
    "            # rural area\n",
    "            elif model == 4:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = (\n",
    "                    -4.78 * ((math.log10(self.freq) - math.log10(1000000)) ** 2)\n",
    "                    + 18.33 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                    - 40.98\n",
    "                )\n",
    "\n",
    "            A = (\n",
    "                69.55\n",
    "                + 26.16 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log(HB)\n",
    "                - ahm\n",
    "            )\n",
    "\n",
    "            B = 44.9 - 6.55 * math.log10(HB)\n",
    "\n",
    "            Lpl = A + B * (math.log10(self.dist) - math.log10(1000)) + C\n",
    "\n",
    "        # 3GPP model\n",
    "        elif model >= 5 and model < 7:\n",
    "            # Suburban Macro\n",
    "            if model == 5:\n",
    "                C = 0  # dB\n",
    "            # Urban Macro\n",
    "            elif model == 6:\n",
    "                C = 3  # dB\n",
    "\n",
    "            Lpl = (\n",
    "                (44.9 - 6.55 * math.log10(HB))\n",
    "                * (math.log10(self.dist) - math.log10(1000))\n",
    "                + 45.5\n",
    "                + (35.46 - 1.1 * HM) * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log10(HM)\n",
    "                + 0.7 * HM\n",
    "                + C\n",
    "            )\n",
    "\n",
    "        # Polynomial 3rd degree\n",
    "        elif model == 7:\n",
    "            p1 = -5.491e-06\n",
    "            p2 = 0.002936\n",
    "            p3 = -0.5004\n",
    "            p4 = -70.57\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 3)\n",
    "                + p2 * math.pow(self.dist, 2)\n",
    "                + p3 * self.dist\n",
    "                + p4\n",
    "            )\n",
    "\n",
    "        # Polynomial 6th degree\n",
    "        elif model == 8:\n",
    "            p1 = 3.69e-12\n",
    "            p2 = 5.997e-11\n",
    "            p3 = -1.381e-06\n",
    "            p4 = 0.0005134\n",
    "            p5 = -0.07318\n",
    "            p6 = 4.254\n",
    "            p7 = -171\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 6)\n",
    "                + p2 * math.pow(self.dist, 5)\n",
    "                + p3 * math.pow(self.dist, 4)\n",
    "                + p4 * math.pow(self.dist, 3)\n",
    "                + p5 * math.pow(self.dist, 2)\n",
    "                + p6 * self.dist\n",
    "                + p7\n",
    "            )\n",
    "\n",
    "        return Lpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Collision Detection ################\n",
    "\"\"\"\n",
    "\n",
    "This Part Allows Collision Checking Between Two Packets :\n",
    " 1- Timing Collision\n",
    " 2- Frequency Collision\n",
    " 3- SF Collision\n",
    " 4- Capture Effect\n",
    " 5- Imperfect SF Orthogonality\n",
    " \n",
    "\"\"\"\n",
    "########################### 1-Timining Collision #################\n",
    "\n",
    "\n",
    "def timingCollision(env, p1, p2):\n",
    "    Npream = 8\n",
    "    Tpreamb = 2 ** p1.sf / (1.0 * p1.bw) * (Npream - 5)\n",
    "    p2_end = p2.addTime + p2.rectime\n",
    "\n",
    "    p1_cs = env.now + (Tpreamb / 1000.0)  # to sec\n",
    "\n",
    "    \"\"\" print (\"collision timing node {} ({},{},{}) node {} ({},{})\".format(\n",
    "        p1.nodeid, env.now - env.now, p1_cs - env.now, p1.rectime,\n",
    "        p2.nodeid, p2.addTime - env.now, p2_end - env.now\n",
    "    ))\"\"\"\n",
    "    if p1_cs < p2_end:\n",
    "        # p1 collided with p2 and lost\n",
    "        # print (\"not late enough\")\n",
    "        return True\n",
    "    # print (\"saved by the preamble\")\n",
    "    return False\n",
    "\n",
    "\n",
    "######################### 2-Frequency Collision #######################\n",
    "def frequencyCollision(p1, p2):\n",
    "    if abs(p1.freq - p2.freq) <= 120 and (p1.bw == 500 or p2.freq == 500):\n",
    "        # print (\"frequency coll 500\")\n",
    "        return True\n",
    "    elif abs(p1.freq - p2.freq) <= 60 and (p1.bw == 250 or p2.freq == 250):\n",
    "        # print( \"frequency coll 250\")\n",
    "        return True\n",
    "    else:\n",
    "        if abs(p1.freq - p2.freq) <= 30:\n",
    "            # print (\"frequency coll 125\")\n",
    "            return True\n",
    "        # else:\n",
    "    # print (\"no frequency coll\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 3- SF Collision ###############################\n",
    "def sfCollision(p1, p2):\n",
    "    if p1.sf == p2.sf:\n",
    "        # print (\"collision sf node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "        return True\n",
    "    # print (\"no sf collision\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 4-5 ############################################\n",
    "def powerCollision_2(p1, p2):\n",
    "    # powerThreshold = 6\n",
    "    # print (\"SF: node {0.nodeid} {0.sf} node {1.nodeid} {1.sf}\".format(p1, p2))\n",
    "    # print (\"pwr: node {0.nodeid} {0.rssi:3.2f} dBm node {1.nodeid} {1.rssi:3.2f} dBm; diff {2:3.2f} dBm\".format(p1, p2, round(p1.rssi - p2.rssi,2)))\n",
    "\n",
    "    if p1.sf == p2.sf:\n",
    "\n",
    "        if abs(p1.rssi - p2.rssi) < 6:\n",
    "            # print (\"collision pwr both node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "            # packets are too close to each other, both collide\n",
    "            # return both packets as casualties\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{} ==> |{}| < {}\".format(\n",
    "                    p1.freq, p2.freq, abs(p1.rssi - p2.rssi), SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1, p2)\n",
    "        elif p1.rssi - p2.rssi < 6:\n",
    "            # p2 overpowered p1, return p1 as casualty\n",
    "            # print (\"collision pwr node {} overpowered node {}\".format(p2.nodeid, p1.nodeid))\n",
    "            # print (\"capture - p2 wins, p1 lost\")\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{}  => {} < {}\".format(\n",
    "                    p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1,)\n",
    "        # print (\"capture - p1 wins, p2 lost\")\n",
    "        # p2 was the weaker packet, return it as a casualty\n",
    "        \"\"\"print(\n",
    "            \"power coll  cap : freq 1 : {} and freq 2 :{} => {} > {}\".format(\n",
    "                p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "            )\n",
    "        )\"\"\"\n",
    "        return (p2,)\n",
    "    else:\n",
    "\n",
    "        if p1.rssi - p2.rssi > SIR[p1.sf - 7][p2.sf - 7]:\n",
    "\n",
    "            # print (\"P1 is OK\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "                # print (\"p2 is OK\")\n",
    "                return ()\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"power coll  Imp : 2\")\n",
    "                return (p2,)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # print (\"p1 is lost\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "\n",
    "                # print (\"p2 is OK\")\n",
    "                # print(\"power coll  Imp : 1\")\n",
    "                return (p1,)\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"opwer coll  cap : 1\")\n",
    "                return (p1, p2)\n",
    "\n",
    "\n",
    "######################## Check Collision ############################\n",
    "def checkcollision(env, packet, packetsAtBS, maxBSReceives, full_collision):\n",
    "    col = 0  # flag needed since there might be several collisions for packet\n",
    "    processing = 0\n",
    "    for i in range(0, len(packetsAtBS)):\n",
    "        if packetsAtBS[i].packet.processed == 1:\n",
    "            processing = processing + 1\n",
    "    if processing > maxBSReceives:\n",
    "        # print (\"too long:\", len(packetsAtBS))\n",
    "        packet.processed = 0\n",
    "    else:\n",
    "        packet.processed = 1\n",
    "\n",
    "    if packetsAtBS:\n",
    "        # print (\"CHECK node {} (sf:{} bw:{} freq:{:.6e}) others: {}\".format(\n",
    "        # packet.nodeid, packet.sf, packet.bw, packet.freq,\n",
    "        # len(packetsAtBS)))\n",
    "        # print(len(packetsAtBS))\n",
    "        for other in packetsAtBS:\n",
    "\n",
    "            if other.id != packet.nodeid:\n",
    "                # print (\">> node {} (sf:{} bw:{} freq:{:.6e})\".format(\n",
    "                # other.id, other.packet.sf, other.packet.bw, other.packet.freq))\n",
    "                if full_collision == 1 or full_collision == 2:\n",
    "\n",
    "                    if frequencyCollision(packet, other.packet) and timingCollision(\n",
    "                        env, packet, other.packet\n",
    "                    ):\n",
    "                        # check who collides in the power domain\n",
    "                        if full_collision == 1:\n",
    "                            # Capture effect\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        else:\n",
    "                            # Capture + Non-orthognalitiy SFs effects\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        # mark all the collided packets\n",
    "                        # either this one, the other one, or both\n",
    "                        for p in c:\n",
    "                            p.collided = 1\n",
    "                            if p == packet:\n",
    "                                col = 1\n",
    "\n",
    "                    else:\n",
    "                        # no freq or timing collision, all fimone\n",
    "                        pass\n",
    "                else:\n",
    "                    # simple collision\n",
    "                    if frequencyCollision(packet, other.packet) and sfCollision(\n",
    "                        packet, other.packet\n",
    "                    ):\n",
    "                        packet.collided = 1\n",
    "                        other.packet.collided = (\n",
    "                            1  # other also got lost, if it wasn't lost already\n",
    "                        )\n",
    "                        col = 1\n",
    "        return col\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Node generation and Plot Function\n",
    "\n",
    "\n",
    "def node_Gen(nrNodes, display):\n",
    "    NODES = []\n",
    "    x = []\n",
    "    y = []\n",
    "    rssi = []\n",
    "    dist = []\n",
    "\n",
    "    for i in range(nrNodes):\n",
    "        device = Device(i)\n",
    "        NODES.append(device)\n",
    "\n",
    "        x.append(NODES[i].x)\n",
    "        y.append(NODES[i].y)\n",
    "        rssi.append(NODES[i].rssi)\n",
    "        dist.append(NODES[i].dist)\n",
    "    pickle.dump(NODES, open(\"data/nodes.p\", \"wb\"))\n",
    "    if display:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x, y, \"ro\")\n",
    "        ax.plot(0, 0, \"bo\")\n",
    "        for i in range(6):\n",
    "            v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            X, Y = np.meshgrid(v1, v2)\n",
    "            F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "            ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "        ax.set_aspect(1)\n",
    "        plt.show()\n",
    "\n",
    "    return NODES\n",
    "\n",
    "\n",
    "# SF Allocation Display\n",
    "\n",
    "# RA-Lora Plot\n",
    "def SF_alloc_plot(sorted_nodes, exp, k, display=False, save=False):\n",
    "    groups = []\n",
    "    for s in range(7, 13):\n",
    "        group = []\n",
    "        posx = []\n",
    "        posy = []\n",
    "        for n in sorted_nodes:\n",
    "            if n.sf == s:\n",
    "                posx.append(n.x)\n",
    "                posy.append(n.y)\n",
    "\n",
    "        group.append(posx)\n",
    "        group.append(posy)\n",
    "        groups.append(group)\n",
    "        group = []\n",
    "    # print(groups)\n",
    "    colors = [\"ro\", \"go\", \"bo\", \"yo\", \"co\", \"mo\"]\n",
    "    # print(len(groups[3][0]))\n",
    "    # print(len(groups[3][1]))\n",
    "    fig, ax = plt.subplots()\n",
    "    for g, c in zip(groups, colors):\n",
    "        ax.plot(g[0], g[1], c)\n",
    "\n",
    "    ax.plot(0, 0, \"ko\")\n",
    "\n",
    "    for i in range(6):\n",
    "        v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        X, Y = np.meshgrid(v1, v2)\n",
    "        F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "        ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "    ax.set_aspect(1)\n",
    "    if save:\n",
    "        if not os.path.exists(\"reports/graphics/{}\".format(exp)):\n",
    "            os.mkdir(\"reports/graphics/{}\".format(exp))\n",
    "        plt.savefig(\"reports/graphics/{}/sf_alloc_{}\".format(exp, k))\n",
    "    if display:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def finalReport(data):\n",
    "    fname = \"sim_results-v2-1.csv\"\n",
    "    if not os.path.isfile(\"reports/results/{}\".format(fname)):\n",
    "        df_new = pd.DataFrame(data, index=[0])\n",
    "        df_new.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "    else:\n",
    "        df = pd.read_csv(\"reports/results/{}\".format(fname))\n",
    "        df_new = pd.DataFrame(data, index=[df.ndim - 1])\n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "        df.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "\n",
    "\n",
    "def airtime(sf, cr, pl, bw):\n",
    "    H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "    DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "    Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "    if bw == 125 and sf in [11, 12]:\n",
    "        # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "        DE = 1\n",
    "    if sf == 6:\n",
    "        # can only have implicit header with SF6\n",
    "        H = 1\n",
    "    Tsym = (2.0 ** sf) / bw  # msec\n",
    "    Tpream = (Npream + 4.25) * Tsym\n",
    "    # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "    payloadSymbNB = 8 + max(\n",
    "        math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "        * (cr + 4),\n",
    "        0,\n",
    "    )\n",
    "    Tpayload = payloadSymbNB * Tsym\n",
    "    return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "        \n",
    "        \n",
    "def mathematical_model(N, sf_dist, avgtime):\n",
    "    charges = []\n",
    "    DERs = []\n",
    "    s = 0\n",
    "    for i in range(6):\n",
    "        Nsf = sf_dist[i]\n",
    "        toa = airtime(i + 7, 1, packetlen, BW)\n",
    "        c = (Nsf * toa) / avgtime\n",
    "        charges.append(c)\n",
    "        DERsf = math.exp(-2*c)\n",
    "        DERs.append(DERsf)\n",
    "        s = s + (DERsf * Nsf)\n",
    "    DER = s / N\n",
    "    return DER, DERs, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Environment(gym.Env):\n",
    "    def __init__(self, nodes, SF_dist_init, Ch_dist_init):\n",
    "\n",
    "        \"\"\" Environment Parameter\"\"\"\n",
    "\n",
    "        super(Environment, self).__init__()\n",
    "\n",
    "        self.NODES = nodes\n",
    "        self.N = len(self.NODES)\n",
    "        self.TP_SET = [2, 5, 8, 11, 14]\n",
    "        self.SF_SET = [7, 8, 9, 10, 11, 12]\n",
    "        self.CH_SET = [\"868.1\", \"868.3\", \"868.5\"]\n",
    "        self.ACTION_SPACE_GEN = self.action_space_generator()\n",
    "        self.ACTION_SPACE_SIZE = len(self.ACTION_SPACE_GEN)\n",
    "        self.STATE_SPACE = []\n",
    "        self.SF_DIST = SF_dist_init\n",
    "        self.ENERGY = 1\n",
    "        self.CH_DIST = Ch_dist_init\n",
    "\n",
    "        \"\"\" Simulation Parameters\"\"\"\n",
    "        self.action_space = spaces.Box(\n",
    "            np.array([0,7, 0, 0]), np.array([self.N - 1,12, 4, 2]), dtype=np.int32\n",
    "        )\n",
    "        # self.observation_space = spaces.Box(np.array([0,7,0,0,-10,-135]), np.array([N,12,4,2,10,-70]),dtype =np.int)\n",
    "        self.observation_space = spaces.Box(\n",
    "            np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "            np.array(\n",
    "                [\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                ]\n",
    "            ),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        self.condition = 0\n",
    "\n",
    "    def action_space_generator(self):\n",
    "        action_space = []\n",
    "\n",
    "        for n in range(self.N):\n",
    "\n",
    "            for sf in self.SF_SET:\n",
    "\n",
    "                for tp in range(len(self.TP_SET)):\n",
    "\n",
    "                    for ch in range(len(self.CH_SET)):\n",
    "                        action_space.append([n, sf, tp, ch])\n",
    "        return action_space\n",
    "\n",
    "    def toa(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def rho_compute(self, sf, n):\n",
    "        return (self.toa(sf, CR, PL, BW) * n) / p\n",
    "\n",
    "    def reward_function(self, rho, u_ch, tp):\n",
    "        return (rho_max) / (rho * u_ch) + ((tp_max - tp) / (tp_max - tp_min))\n",
    "\n",
    "    def reward_function_zero(self, tp):\n",
    "        return (tp_max - tp) / (tp_max - tp_min)\n",
    "\n",
    "    def compute_der(self, sf_dist):\n",
    "        der = 0\n",
    "        for index in range(len(sf_dist)):\n",
    "            rho = self.rho_compute(self.SF_SET[index], sf_dist[index])\n",
    "            der_sf = math.exp(-2 * rho)\n",
    "            der = der + (der_sf * sf_dist[index])\n",
    "        return (der / self.N) * 100\n",
    "\n",
    "    def reset(self):\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        self.condition = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print('Step : ',self.condition)\n",
    "        # print('Action : ',action)\n",
    "        # assert self.action_space.contains(action)\n",
    "        reward = 0\n",
    "        #node_index = random.randint(0,self.N-1)\n",
    "        node_index = action[0]\n",
    "        node = self.NODES[node_index]\n",
    "        old_sf = node.sf\n",
    "        old_tp = node.tx\n",
    "        old_ch = node.freq\n",
    "        new_sf = action[1]\n",
    "        new_tp = self.TP_SET[action[2]]\n",
    "        new_ch = action[3]\n",
    "\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        der = self.compute_der(sf_dist)\n",
    "        #print(\"DER : \", der)\n",
    "        config = config_dict[new_sf - 7]\n",
    "        # print(\"config : \", config)\n",
    "        \n",
    "        # print('rho',rho)\n",
    "        # print(\"node snr : {} rssi : {}\".format(node.snr, node.rssi))\n",
    "        # print(\"node old sf : {} new sf: {}\".format(old_sf, new_sf))\n",
    "        if node.snr < config[\"snr\"] or node.rssi < config[\"sens\"]:\n",
    "             \n",
    "            #rho = self.rho_compute(new_sf, sf_dist[new_sf - 7])\n",
    "            #if rho > 0 and u_ch[new_ch] > 0:\n",
    "                #reward = - self.reward_function(rho, u_ch[new_ch], new_tp)\n",
    "            #else:\n",
    "                #reward = - self.reward_function_zero(new_tp)\n",
    "            reward = -1\n",
    "\n",
    "        else: \n",
    "            rho = self.rho_compute(new_sf, sf_dist[new_sf - 7])\n",
    "            old_rho = self.rho_compute(old_sf, sf_dist[old_sf - 7])\n",
    "            if new_sf > old_sf:\n",
    "                if old_rho > rho_max and rho < rho_max :\n",
    "                    self.NODES[node_index].sf = new_sf\n",
    "                    self.NODES[node_index].freq = ch[new_ch]\n",
    "                    self.NODES[node_index].tx = new_tp\n",
    "                    \n",
    "                    self.NODES[node_index].packet.sf = new_sf\n",
    "                    self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "            \n",
    "                    \n",
    "                    self.NODES[node_index].update_rectime(new_sf)\n",
    "\n",
    "                    sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                    sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                    old_tp_index = self.TP_SET.index(old_tp)\n",
    "                    new_tp_index = self.TP_SET.index(new_tp)\n",
    "                    tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                    tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                    old_ch_index = ch.index(old_ch)\n",
    "                    new_ch_index = new_ch\n",
    "                    u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                    u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "            else:\n",
    "                \n",
    "                self.NODES[node_index].sf = new_sf\n",
    "                self.NODES[node_index].freq = ch[new_ch]\n",
    "                self.NODES[node_index].tx = new_tp\n",
    "                self.NODES[node_index].update_rectime(new_sf)\n",
    "                \n",
    "                self.NODES[node_index].packet.sf = new_sf\n",
    "                self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "\n",
    "                sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                old_tp_index = self.TP_SET.index(old_tp)\n",
    "                new_tp_index = self.TP_SET.index(new_tp)\n",
    "                tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                old_ch_index = ch.index(old_ch)\n",
    "                new_ch_index = new_ch\n",
    "                u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "        \n",
    "\n",
    "            if rho > 0 and u_ch[new_ch] > 0:\n",
    "                reward = self.reward_function(rho, u_ch[new_ch], new_tp)\n",
    "            else:\n",
    "                reward = self.reward_function_zero(new_tp)\n",
    "\n",
    "           \n",
    "        # print(sf_dist)\n",
    "        # print(tp_dist)\n",
    "        # print(u_ch)\n",
    "\n",
    "        # print('Reward = ',reward)\n",
    "        new_state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        # print('')\n",
    "        done = False\n",
    "        self.condition = self.condition + 1\n",
    "        if self.condition == 100:\n",
    "            done = True\n",
    "        #if der > 95:\n",
    "            #done = True\n",
    "\n",
    "        info = \"\"\n",
    "\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nodes (nodes,file_name):\n",
    "    pickle.dump(nodes, open(\"data/\"+file_name, \"wb\"))\n",
    "    print('All Nodes Were Saved !')\n",
    "    \n",
    "def load_nodes (file_name):\n",
    "    print('Loading Nodes From File')\n",
    "    nodes = pickle.load(open(\"data/\"+file_name,\"rb\"))\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Nodes Were Saved !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nodes = node_Gen(5000, display=False)\n",
    "nodes, sf_distribution, ch_u = SF_alloc_ch_utilization(nodes)\n",
    "save_nodes(nodes,'nodes_{}.alloc'.format(len(nodes)))\n",
    "#train_episodes = 300\n",
    "#test_episodes = 100\n",
    "\n",
    "train_episodes = 300\n",
    "test_episodes = 100\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def create_model(self, action_shape, state_shape):\n",
    "        model = Sequential()\n",
    "        learning_rate = 0.001\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "\n",
    "        model.add(\n",
    "            Dense(\n",
    "                256, input_shape=state_shape, activation=\"relu\", kernel_initializer=init\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(256, activation=\"relu\", kernel_initializer=init))\n",
    "\n",
    "        model.add(Dense(450000, activation=\"relu\", kernel_initializer=init))\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        model.save('lora_rl_model.h5')\n",
    "        return model\n",
    "\n",
    "    def get_qs(self, model, state, step):\n",
    "        return model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "\n",
    "    def train(self, env, replay_memory, model, target_model, done):\n",
    "        learning_rate = 0.7  # Learning rate\n",
    "        discount_factor = 0.618\n",
    "\n",
    "        MIN_REPLAY_SIZE = 1000\n",
    "        if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "            return\n",
    "\n",
    "        batch_size = 64 * 2\n",
    "        mini_batch = random.sample(replay_memory, batch_size)\n",
    "        # print(len(mini_batch))\n",
    "        # print(mini_batch)\n",
    "\n",
    "        # current_states = np.array([transition[0] for transition in mini_batch],dtype=object)\n",
    "        # print()\n",
    "        current_states = np.array(\n",
    "            np.array(\n",
    "                [\n",
    "                    transition[0]\n",
    "                    for transition in mini_batch\n",
    "                    if transition[0] is not None\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        # print('states len : ',len(current_states))\n",
    "        # current_states= np.array(current_states).astype(\"float32\")\n",
    "        # print(current_states)\n",
    "        # current_states = np.asarray(current_states).astype(np.float32)\n",
    "        current_qs_list = model.predict(current_states)\n",
    "        # print(current_qs_list)\n",
    "        # print('Current: ',len(current_qs_list))\n",
    "        # print('Mini batch : ',len(mini_batch))\n",
    "        new_current_states = np.array(\n",
    "            [transition[3] for transition in mini_batch if transition[3] is not None]\n",
    "        )\n",
    "        future_qs_list = target_model.predict(new_current_states)\n",
    "        # print()\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        #print(mini_batch)\n",
    "        #print(len(mini_batch))\n",
    "        for index, (observation, action, reward, new_observation, done) in enumerate(\n",
    "            mini_batch\n",
    "        ):\n",
    "            if not done:\n",
    "                max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "            else:\n",
    "                max_future_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            #print('The current_qs : ',current_qs)\n",
    "            #print('Action : ',action)\n",
    "            #print(type(action))\n",
    "            #print(env.ACTION_SPACE_GEN)\n",
    "            action_index = env.ACTION_SPACE_GEN.index(action)\n",
    "            current_qs[action_index] = (1 - learning_rate) * current_qs[\n",
    "                action_index\n",
    "            ] + learning_rate * max_future_q\n",
    "\n",
    "            X.append(observation)\n",
    "            Y.append(current_qs)\n",
    "        model.fit(\n",
    "            np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True\n",
    "        )\n",
    "\n",
    "\n",
    "env = Environment(nodes, sf_distribution, ch_u)\n",
    "\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "# agent.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    epsilon = 1  # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "    max_epsilon = 1  # You can't explore more than 100% of the time\n",
    "    min_epsilon = 0.01  # At a minimum, we'll always explore 1% of the time\n",
    "    decay = 0.01\n",
    "    final_rewards = []\n",
    "    # 1. Initialize the Target and Main models\n",
    "    # Main Model (updated every 4 steps)\n",
    "    print(env.observation_space.shape)\n",
    "    print(env.action_space.shape)\n",
    "    #print_nodes(nodes)\n",
    "    #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    model = agent.create_model(env.action_space.shape, env.observation_space.shape)\n",
    "    print(\"Model Created !\")\n",
    "    # Target Model (updated every 100 steps)\n",
    "    target_model = agent.create_model(\n",
    "        env.action_space.shape, env.observation_space.shape\n",
    "    )\n",
    "    print(\"Target Model Created !\")\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "    target_update_counter = 0\n",
    "\n",
    "    # X = states, y = actions\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    steps_to_update_target_model = 0\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        print('Episode : ',episode)\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps_to_update_target_model += 1\n",
    "            # if True:\n",
    "            # env.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                # print('Explore')\n",
    "                action = env.action_space.sample()\n",
    "                a =[]\n",
    "                a.append(action[0])\n",
    "                a.append(action[1])\n",
    "                a.append(action[2])\n",
    "                a.append(action[3])\n",
    "                action = a\n",
    "            else:\n",
    "\n",
    "                # Exploit best known action\n",
    "                # model dims are (batch, env.observation_space.n)\n",
    "                # print('Exploit')\n",
    "                encoded = observation\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                # print('model : ',model)\n",
    "                # print('Encoded state : ',encoded_reshaped)\n",
    "                predicted = model.predict(encoded_reshaped)\n",
    "                action = np.argmax(predicted)\n",
    "                action = env.ACTION_SPACE_GEN[action]\n",
    "            #print(action)\n",
    "            #print(type(action))\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "            # 3. Update the Main Network using the Bellman Equation\n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                agent.train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\n",
    "                    \"Total training rewards: {} after n steps = {} with final reward = {}\".format(\n",
    "                        total_training_rewards, episode, reward\n",
    "                    )\n",
    "                )\n",
    "                final_rewards.append(total_training_rewards)\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    print(\"Copying main network weights to the target network weights\")\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    env.close()\n",
    "    save_nodes(env.NODES,'nodes_{}.rl'.format(len(env.NODES)))\n",
    "    #print_nodes(env.NODES)\n",
    "    print(final_rewards)\n",
    "    #print_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "(4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamnache/anaconda3/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created !\n",
      "Target Model Created !\n",
      "Episode :  0\n",
      "Total training rewards: 85.79273517553132 after n steps = 0 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  1\n",
      "Total training rewards: 73.94649412345075 after n steps = 1 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff208087b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff208087b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: 75.52544187422083 after n steps = 2 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  3\n",
      "Total training rewards: 67.81408475255135 after n steps = 3 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  4\n",
      "Total training rewards: 76.49306202503988 after n steps = 4 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  5\n",
      "Total training rewards: 87.58223161318678 after n steps = 5 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  6\n",
      "Total training rewards: 81.87122132451404 after n steps = 6 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  7\n",
      "Total training rewards: 78.32203998245839 after n steps = 7 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  8\n",
      "Total training rewards: 90.78823211653788 after n steps = 8 with final reward = 1.36464309746423\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  9\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff2006a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff2006a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff200573cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff200573cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: 79.0596524271714 after n steps = 9 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  10\n",
      "Total training rewards: 104.47751497889914 after n steps = 10 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  11\n",
      "Total training rewards: 56.060250099426874 after n steps = 11 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  12\n",
      "Total training rewards: 67.21749098997728 after n steps = 12 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  13\n",
      "Total training rewards: 76.89288841331793 after n steps = 13 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  14\n",
      "Total training rewards: 108.76699284348376 after n steps = 14 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  15\n",
      "Total training rewards: 78.27499963207036 after n steps = 15 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  16\n",
      "Total training rewards: 59.30391879259012 after n steps = 16 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  17\n",
      "Total training rewards: 75.44415214932981 after n steps = 17 with final reward = 0.9751845333570962\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  18\n",
      "Total training rewards: 78.48304951809223 after n steps = 18 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  19\n",
      "Total training rewards: 71.70212675825458 after n steps = 19 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  20\n",
      "Total training rewards: 68.83655817471939 after n steps = 20 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  21\n",
      "Total training rewards: 73.85669747737619 after n steps = 21 with final reward = 1.1244644592797217\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  22\n",
      "Total training rewards: 106.67105388653987 after n steps = 22 with final reward = 3.292252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  23\n",
      "Total training rewards: 73.35974649425117 after n steps = 23 with final reward = 1.0915979087913152\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  24\n",
      "Total training rewards: 75.84050443099585 after n steps = 24 with final reward = 3.6144358336522515\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  25\n",
      "Total training rewards: 101.2028025027802 after n steps = 25 with final reward = 0.7852552508111901\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  26\n",
      "Total training rewards: 95.70481112313054 after n steps = 26 with final reward = 1.5528828762167852\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  27\n",
      "Total training rewards: 66.75826144540333 after n steps = 27 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  28\n",
      "Total training rewards: 102.03187337071338 after n steps = 28 with final reward = 1.1492288213627995\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  29\n",
      "Total training rewards: 56.35334883844719 after n steps = 29 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  30\n",
      "Total training rewards: 71.82483406296217 after n steps = 30 with final reward = 0.8120180133517495\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  31\n",
      "Total training rewards: 76.29498513787559 after n steps = 31 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  32\n",
      "Total training rewards: 81.45925361006357 after n steps = 32 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  33\n",
      "Total training rewards: 65.68390461473044 after n steps = 33 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  34\n",
      "Total training rewards: 47.4900464596263 after n steps = 34 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  35\n",
      "Total training rewards: 89.7586565555564 after n steps = 35 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  36\n",
      "Total training rewards: 128.89747537729448 after n steps = 36 with final reward = 2.5762905557681677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  37\n",
      "Total training rewards: 94.18655926433829 after n steps = 37 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  38\n",
      "Total training rewards: 107.24658436792824 after n steps = 38 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  39\n",
      "Total training rewards: 73.6926550363372 after n steps = 39 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  40\n",
      "Total training rewards: 94.6296182893354 after n steps = 40 with final reward = 1.8011555060987605\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  41\n",
      "Total training rewards: 112.2243123871617 after n steps = 41 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  42\n",
      "Total training rewards: 87.15890999084762 after n steps = 42 with final reward = 2.5762905557681677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  43\n",
      "Total training rewards: 84.63888495970947 after n steps = 43 with final reward = 1.8011555060987605\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  44\n",
      "Total training rewards: 114.21627135665383 after n steps = 44 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  45\n",
      "Total training rewards: 81.99876615686242 after n steps = 45 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  46\n",
      "Total training rewards: 117.16215567622217 after n steps = 46 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  47\n",
      "Total training rewards: 46.16707750559215 after n steps = 47 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  48\n",
      "Total training rewards: 66.37253167610011 after n steps = 48 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  49\n",
      "Total training rewards: 101.50579737597653 after n steps = 49 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  50\n",
      "Total training rewards: 97.33768183772979 after n steps = 50 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  51\n",
      "Total training rewards: 57.1069235288631 after n steps = 51 with final reward = 2.6952051822105982\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  52\n",
      "Total training rewards: 78.05556188305673 after n steps = 52 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  53\n",
      "Total training rewards: 136.7353247705295 after n steps = 53 with final reward = 1.7751972597955166\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  54\n",
      "Total training rewards: 105.17204206670047 after n steps = 54 with final reward = 2.949177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  55\n",
      "Total training rewards: 99.18513563313552 after n steps = 55 with final reward = 0.6075870414613509\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  56\n",
      "Total training rewards: 100.12234080791032 after n steps = 56 with final reward = 0.8028828762167851\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  57\n",
      "Total training rewards: 71.47629953958781 after n steps = 57 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  58\n",
      "Total training rewards: 62.29161972817679 after n steps = 58 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  59\n",
      "Total training rewards: 75.33202123220028 after n steps = 59 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  60\n",
      "Total training rewards: 65.2700806271997 after n steps = 60 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  61\n",
      "Total training rewards: 127.62050978559091 after n steps = 61 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  62\n",
      "Total training rewards: 65.47956744656103 after n steps = 62 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  63\n",
      "Total training rewards: 103.19696770355344 after n steps = 63 with final reward = 1.548310256928536\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  64\n",
      "Total training rewards: 90.76045599730497 after n steps = 64 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  65\n",
      "Total training rewards: 133.7055941193153 after n steps = 65 with final reward = 0.5483102569285362\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  66\n",
      "Total training rewards: 143.1562425858874 after n steps = 66 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  67\n",
      "Total training rewards: 101.21824084139011 after n steps = 67 with final reward = 1.7667625314515876\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  68\n",
      "Total training rewards: 192.54683415904552 after n steps = 68 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  69\n",
      "Total training rewards: 108.41804022575056 after n steps = 69 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  70\n",
      "Total training rewards: 134.4191396131539 after n steps = 70 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  71\n",
      "Total training rewards: 129.4470652838893 after n steps = 71 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  72\n",
      "Total training rewards: 107.26756457553695 after n steps = 72 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  73\n",
      "Total training rewards: 133.44558569511833 after n steps = 73 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  74\n",
      "Total training rewards: 112.67589809584366 after n steps = 74 with final reward = 1.1244644592797217\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  75\n",
      "Total training rewards: 122.71908907706242 after n steps = 75 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  76\n",
      "Total training rewards: 169.1761505984714 after n steps = 76 with final reward = 2.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  77\n",
      "Total training rewards: 143.84147422697973 after n steps = 77 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  78\n",
      "Total training rewards: 98.28167123551117 after n steps = 78 with final reward = 3.0304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  79\n",
      "Total training rewards: 147.45135656696502 after n steps = 79 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  80\n",
      "Total training rewards: 153.47065668105094 after n steps = 80 with final reward = 3.6419761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  81\n",
      "Total training rewards: 224.7186684794444 after n steps = 81 with final reward = 2.8816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  82\n",
      "Total training rewards: 116.35910298736358 after n steps = 82 with final reward = 1.2728074542806973\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  83\n",
      "Total training rewards: 118.04537503228266 after n steps = 83 with final reward = 2.5584634355414404\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  84\n",
      "Total training rewards: 127.38787741341801 after n steps = 84 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  85\n",
      "Total training rewards: 95.07050919221086 after n steps = 85 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  86\n",
      "Total training rewards: 136.26757679154514 after n steps = 86 with final reward = 2.0167625314515876\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  87\n",
      "Total training rewards: 155.02594755674755 after n steps = 87 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  88\n",
      "Total training rewards: 144.00687691320996 after n steps = 88 with final reward = 1.7493573511356661\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  89\n",
      "Total training rewards: 142.53639108337728 after n steps = 89 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  90\n",
      "Total training rewards: 116.89874454388361 after n steps = 90 with final reward = 2.8135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  91\n",
      "Total training rewards: 182.9370346017841 after n steps = 91 with final reward = 3.199177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  92\n",
      "Total training rewards: 120.48049404302797 after n steps = 92 with final reward = 1.357587041461351\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  93\n",
      "Total training rewards: 125.63475785159973 after n steps = 93 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  94\n",
      "Total training rewards: 145.43926620088305 after n steps = 94 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  95\n",
      "Total training rewards: 156.93054149356783 after n steps = 95 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  96\n",
      "Total training rewards: 120.30750207165144 after n steps = 96 with final reward = 3.188522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  97\n",
      "Total training rewards: 153.29963916322865 after n steps = 97 with final reward = 2.249314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  98\n",
      "Total training rewards: 221.00744424718891 after n steps = 98 with final reward = 1.6611976627667642\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  99\n",
      "Total training rewards: 221.39107796031004 after n steps = 99 with final reward = 3.542252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  100\n",
      "Total training rewards: 154.28659131445062 after n steps = 100 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  101\n",
      "Total training rewards: 122.60503789711146 after n steps = 101 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  102\n",
      "Total training rewards: 130.7366409752383 after n steps = 102 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  103\n",
      "Total training rewards: 172.36695390945135 after n steps = 103 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  104\n",
      "Total training rewards: 158.83451867282267 after n steps = 104 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  105\n",
      "Total training rewards: 192.52146904841004 after n steps = 105 with final reward = 2.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  106\n",
      "Total training rewards: 164.45767244418698 after n steps = 106 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  107\n",
      "Total training rewards: 110.60203051596409 after n steps = 107 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  108\n",
      "Total training rewards: 201.5401068584631 after n steps = 108 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  109\n",
      "Total training rewards: 70.83010337830343 after n steps = 109 with final reward = 3.188522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  110\n",
      "Total training rewards: 174.3047512639508 after n steps = 110 with final reward = 3.0635809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  111\n",
      "Total training rewards: 146.96327127674834 after n steps = 111 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  112\n",
      "Total training rewards: 231.4555298242836 after n steps = 112 with final reward = 2.5577561133827116\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  113\n",
      "Total training rewards: 218.772968474889 after n steps = 113 with final reward = 3.3919761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  114\n",
      "Total training rewards: 233.30499369759139 after n steps = 114 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  115\n",
      "Total training rewards: 190.23014908303765 after n steps = 115 with final reward = 3.3919761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  116\n",
      "Total training rewards: 158.5451696004555 after n steps = 116 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  117\n",
      "Total training rewards: 174.3038867074789 after n steps = 117 with final reward = 1.9409244048790084\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  118\n",
      "Total training rewards: 86.99691724433953 after n steps = 118 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  119\n",
      "Total training rewards: 176.3770665684808 after n steps = 119 with final reward = 1.4778711844912906\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  120\n",
      "Total training rewards: 196.1812162738155 after n steps = 120 with final reward = 3.9990863583317977\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  121\n",
      "Total training rewards: 235.9402665449393 after n steps = 121 with final reward = 3.6419761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  122\n",
      "Total training rewards: 236.44733920174642 after n steps = 122 with final reward = 2.3077561133827116\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  123\n",
      "Total training rewards: 177.04752867460928 after n steps = 123 with final reward = 2.3510271165322316\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  124\n",
      "Total training rewards: 245.3247016439396 after n steps = 124 with final reward = 2.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  125\n",
      "Total training rewards: 155.9128896941434 after n steps = 125 with final reward = 1.4751845333570963\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  126\n",
      "Total training rewards: 198.72594439603662 after n steps = 126 with final reward = 2.7037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  127\n",
      "Total training rewards: 250.30743619692282 after n steps = 127 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  128\n",
      "Total training rewards: 192.45455842580927 after n steps = 128 with final reward = 2.8011555060987607\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  129\n",
      "Total training rewards: 152.67971948679744 after n steps = 129 with final reward = 2.8135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  130\n",
      "Total training rewards: 222.3888915331134 after n steps = 130 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  131\n",
      "Total training rewards: 212.57071233258353 after n steps = 131 with final reward = 1.6467707484331524\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  132\n",
      "Total training rewards: 212.4143689054561 after n steps = 132 with final reward = 2.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  133\n",
      "Total training rewards: 260.93929576518315 after n steps = 133 with final reward = 2.6952051822105982\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  134\n",
      "Total training rewards: 234.89524925276842 after n steps = 134 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  135\n",
      "Total training rewards: 245.48795928216006 after n steps = 135 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  136\n",
      "Total training rewards: 274.1788567746042 after n steps = 136 with final reward = 3.1316274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  137\n",
      "Total training rewards: 252.41238149663383 after n steps = 137 with final reward = 2.8816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  138\n",
      "Total training rewards: 232.56465150256994 after n steps = 138 with final reward = 2.9537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  139\n",
      "Total training rewards: 235.92920327346167 after n steps = 139 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  140\n",
      "Total training rewards: 158.8049676194753 after n steps = 140 with final reward = 3.199177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  141\n",
      "Total training rewards: 254.02267704158066 after n steps = 141 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  142\n",
      "Total training rewards: 205.2392171320265 after n steps = 142 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  143\n",
      "Total training rewards: 192.2901162830903 after n steps = 143 with final reward = 2.8816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  144\n",
      "Total training rewards: 261.4942637412565 after n steps = 144 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  145\n",
      "Total training rewards: 221.50318765225202 after n steps = 145 with final reward = 1.6812339555778784\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  146\n",
      "Total training rewards: 233.37671586292942 after n steps = 146 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  147\n",
      "Total training rewards: 256.4879580995865 after n steps = 147 with final reward = 3.249314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  148\n",
      "Total training rewards: 252.50881695421157 after n steps = 148 with final reward = 3.188522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  149\n",
      "Total training rewards: 279.8979902654147 after n steps = 149 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  150\n",
      "Total training rewards: 228.99381536818794 after n steps = 150 with final reward = 1.3744644592797217\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  151\n",
      "Total training rewards: 204.8656131707131 after n steps = 151 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  152\n",
      "Total training rewards: 205.06449418335393 after n steps = 152 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  153\n",
      "Total training rewards: 243.88938468617806 after n steps = 153 with final reward = 2.6952051822105982\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  154\n",
      "Total training rewards: 229.82212853493147 after n steps = 154 with final reward = 0.9312339555778782\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  155\n",
      "Total training rewards: 295.19482998083726 after n steps = 155 with final reward = 3.2037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  156\n",
      "Total training rewards: 271.84648903777526 after n steps = 156 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  157\n",
      "Total training rewards: 277.52342443706164 after n steps = 157 with final reward = 3.188522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  158\n",
      "Total training rewards: 231.90031348226455 after n steps = 158 with final reward = 3.2037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  159\n",
      "Total training rewards: 255.49377210516093 after n steps = 159 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  160\n",
      "Total training rewards: 227.0279516674898 after n steps = 160 with final reward = 3.0584634355414404\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  161\n",
      "Total training rewards: 234.31697180740127 after n steps = 161 with final reward = 2.4778711844912906\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  162\n",
      "Total training rewards: 280.07814843717836 after n steps = 162 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  163\n",
      "Total training rewards: 317.84614126709806 after n steps = 163 with final reward = 3.055479047806413\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  164\n",
      "Total training rewards: 340.9996910874262 after n steps = 164 with final reward = 3.0243832918739635\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  165\n",
      "Total training rewards: 344.3590218953565 after n steps = 165 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  166\n",
      "Total training rewards: 303.06274191813344 after n steps = 166 with final reward = 2.4409244048790084\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  167\n",
      "Total training rewards: 277.7677171675417 after n steps = 167 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  168\n",
      "Total training rewards: 319.3130764647466 after n steps = 168 with final reward = 4.520666594563416\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  169\n",
      "Total training rewards: 333.6573104296658 after n steps = 169 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  170\n",
      "Total training rewards: 301.92289298849846 after n steps = 170 with final reward = 3.0762905557681677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  171\n",
      "Total training rewards: 326.9581224524906 after n steps = 171 with final reward = 4.520666594563416\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  172\n",
      "Total training rewards: 380.14672371822405 after n steps = 172 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  173\n",
      "Total training rewards: 361.0009688773673 after n steps = 173 with final reward = 0.25\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  174\n",
      "Total training rewards: 378.4366736917261 after n steps = 174 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  175\n",
      "Total training rewards: 369.16336688433825 after n steps = 175 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  176\n",
      "Total training rewards: 325.3097186525139 after n steps = 176 with final reward = 2.938522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  177\n",
      "Total training rewards: 272.42358604409804 after n steps = 177 with final reward = 4.1144358336522515\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  178\n",
      "Total training rewards: 308.4048932293679 after n steps = 178 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  179\n",
      "Total training rewards: 356.0502713096996 after n steps = 179 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  180\n",
      "Total training rewards: 357.23191078876505 after n steps = 180 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  181\n",
      "Total training rewards: 345.73569089538597 after n steps = 181 with final reward = 4.1144358336522515\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  182\n",
      "Total training rewards: 347.76460785732763 after n steps = 182 with final reward = 4.239013266998342\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  183\n",
      "Total training rewards: 356.6793423646534 after n steps = 183 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  184\n",
      "Total training rewards: 336.3255633970414 after n steps = 184 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  185\n",
      "Total training rewards: 391.19925611569835 after n steps = 185 with final reward = 4.680696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  186\n",
      "Total training rewards: 396.58791645688024 after n steps = 186 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  187\n",
      "Total training rewards: 384.09170461880717 after n steps = 187 with final reward = 4.680696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  188\n",
      "Total training rewards: 334.3873333421753 after n steps = 188 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  189\n",
      "Total training rewards: 355.320738197263 after n steps = 189 with final reward = 4.3739721531232725\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  190\n",
      "Total training rewards: 335.22253380260304 after n steps = 190 with final reward = 4.520666594563416\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  191\n",
      "Total training rewards: 359.3527266480363 after n steps = 191 with final reward = 4.680696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  192\n",
      "Total training rewards: 346.0449649825968 after n steps = 192 with final reward = 4.180696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  193\n",
      "Total training rewards: 296.84760797415515 after n steps = 193 with final reward = 2.8809297809199617\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  194\n",
      "Total training rewards: 319.14632185907647 after n steps = 194 with final reward = 4.680696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  195\n",
      "Total training rewards: 412.6624135434731 after n steps = 195 with final reward = 5.048766583747927\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  196\n",
      "Total training rewards: 392.11466236188653 after n steps = 196 with final reward = 4.355968174998026\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  197\n",
      "Total training rewards: 390.5228704525824 after n steps = 197 with final reward = 4.355968174998026\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  198\n",
      "Total training rewards: 353.7720404676896 after n steps = 198 with final reward = 3.61986255432547\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  199\n",
      "Total training rewards: 336.03000731556335 after n steps = 199 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  200\n",
      "Total training rewards: 358.51298277892073 after n steps = 200 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  201\n",
      "Total training rewards: 382.9673340136843 after n steps = 201 with final reward = 4.855968174998026\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  202\n",
      "Total training rewards: 387.8049125903756 after n steps = 202 with final reward = 4.855968174998026\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  203\n",
      "Total training rewards: 384.3566312174595 after n steps = 203 with final reward = 4.105968174998026\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  204\n",
      "Total training rewards: 395.3626199667264 after n steps = 204 with final reward = 4.680696894316298\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  205\n",
      "Total training rewards: 391.11693657747185 after n steps = 205 with final reward = 4.520666594563416\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  206\n",
      "Total training rewards: 349.48849401468624 after n steps = 206 with final reward = 1.0228074542806973\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  207\n",
      "Total training rewards: 263.5074007739758 after n steps = 207 with final reward = 1.4751845333570963\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  208\n",
      "Total training rewards: 345.8199819983532 after n steps = 208 with final reward = 3.8644358336522515\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  209\n",
      "Total training rewards: 227.77857628370117 after n steps = 209 with final reward = 3.9990863583317977\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  210\n",
      "Total training rewards: 311.4806279502816 after n steps = 210 with final reward = 3.4990863583317977\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  211\n",
      "Total training rewards: 294.24090004199405 after n steps = 211 with final reward = 3.199177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  212\n",
      "Total training rewards: 264.4406594993691 after n steps = 212 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  213\n",
      "Total training rewards: 297.2189413689706 after n steps = 213 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  214\n",
      "Total training rewards: 301.2718958237202 after n steps = 214 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  215\n",
      "Total training rewards: 282.72777580812266 after n steps = 215 with final reward = 3.2037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  216\n",
      "Total training rewards: 202.75594197504347 after n steps = 216 with final reward = 3.3621074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  217\n",
      "Total training rewards: 316.529876656055 after n steps = 217 with final reward = 3.792252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  218\n",
      "Total training rewards: 291.0901832524038 after n steps = 218 with final reward = 2.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  219\n",
      "Total training rewards: 232.8477848498195 after n steps = 219 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  220\n",
      "Total training rewards: 304.621807617976 after n steps = 220 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  221\n",
      "Total training rewards: 294.7825307853997 after n steps = 221 with final reward = 2.1092572966180754\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  222\n",
      "Total training rewards: 298.9109849131141 after n steps = 222 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  223\n",
      "Total training rewards: 309.89585545730745 after n steps = 223 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  224\n",
      "Total training rewards: 295.57538570678616 after n steps = 224 with final reward = 2.6316274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  225\n",
      "Total training rewards: 227.91319094703422 after n steps = 225 with final reward = 3.1316274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  226\n",
      "Total training rewards: 265.82232619517845 after n steps = 226 with final reward = 3.542252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  227\n",
      "Total training rewards: 270.45565606710323 after n steps = 227 with final reward = 2.8621074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  228\n",
      "Total training rewards: 281.58325336661727 after n steps = 228 with final reward = 2.749314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  229\n",
      "Total training rewards: 238.60185871728376 after n steps = 229 with final reward = 2.8135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  230\n",
      "Total training rewards: 288.7984633389422 after n steps = 230 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  231\n",
      "Total training rewards: 314.760524810446 after n steps = 231 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  232\n",
      "Total training rewards: 311.78916984062084 after n steps = 232 with final reward = 2.9537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  233\n",
      "Total training rewards: 268.5449289621332 after n steps = 233 with final reward = 3.2037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  234\n",
      "Total training rewards: 324.38759910555484 after n steps = 234 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  235\n",
      "Total training rewards: 290.75303645402033 after n steps = 235 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  236\n",
      "Total training rewards: 315.43591391279983 after n steps = 236 with final reward = 3.0635809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  237\n",
      "Total training rewards: 290.98697909762234 after n steps = 237 with final reward = 3.249314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  238\n",
      "Total training rewards: 290.00458669619184 after n steps = 238 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  239\n",
      "Total training rewards: 277.03831465097835 after n steps = 239 with final reward = 3.3919761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  240\n",
      "Total training rewards: 365.1637907757038 after n steps = 240 with final reward = 2.7465750362169796\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  241\n",
      "Total training rewards: 299.8774095288395 after n steps = 241 with final reward = 3.9990863583317977\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  242\n",
      "Total training rewards: 320.7026534424935 after n steps = 242 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  243\n",
      "Total training rewards: 292.3227529314556 after n steps = 243 with final reward = 3.0762905557681677\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  244\n",
      "Total training rewards: 279.13140480264747 after n steps = 244 with final reward = 2.938522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  245\n",
      "Total training rewards: 293.9094244959332 after n steps = 245 with final reward = 2.938522477701582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  246\n",
      "Total training rewards: 303.4910968701723 after n steps = 246 with final reward = 3.449177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  247\n",
      "Total training rewards: 309.2869739983031 after n steps = 247 with final reward = 3.2804791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  248\n",
      "Total training rewards: 290.5128074984461 after n steps = 248 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  249\n",
      "Total training rewards: 301.6177875428222 after n steps = 249 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  250\n",
      "Total training rewards: 273.7827172716658 after n steps = 250 with final reward = 2.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  251\n",
      "Total training rewards: 273.45091720891827 after n steps = 251 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  252\n",
      "Total training rewards: 250.2798160235982 after n steps = 252 with final reward = 0.0\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  253\n",
      "Total training rewards: 283.1451377610864 after n steps = 253 with final reward = 2.999314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  254\n",
      "Total training rewards: 265.79047088138293 after n steps = 254 with final reward = 1.4993573511356661\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  255\n",
      "Total training rewards: 292.8626995208808 after n steps = 255 with final reward = 2.999314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  256\n",
      "Total training rewards: 275.86716976252137 after n steps = 256 with final reward = 2.499314768748848\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  257\n",
      "Total training rewards: 267.71755946322287 after n steps = 257 with final reward = 0.75\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  258\n",
      "Total training rewards: 310.1286426064795 after n steps = 258 with final reward = 3.1316274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  259\n",
      "Total training rewards: 270.6365484349683 after n steps = 259 with final reward = 3.1121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  260\n",
      "Total training rewards: 299.8344372484755 after n steps = 260 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  261\n",
      "Total training rewards: 311.376203315072 after n steps = 261 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  262\n",
      "Total training rewards: 314.64694439817254 after n steps = 262 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  263\n",
      "Total training rewards: 323.2115396861898 after n steps = 263 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  264\n",
      "Total training rewards: 334.5279832818635 after n steps = 264 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  265\n",
      "Total training rewards: 301.1412955178118 after n steps = 265 with final reward = 3.2804791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  266\n",
      "Total training rewards: 324.6814414469522 after n steps = 266 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  267\n",
      "Total training rewards: 313.7028263884026 after n steps = 267 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  268\n",
      "Total training rewards: 305.9843416954717 after n steps = 268 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  269\n",
      "Total training rewards: 304.17987069347805 after n steps = 269 with final reward = 2.9874819377641493\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  270\n",
      "Total training rewards: 328.49630492446886 after n steps = 270 with final reward = 3.292252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  271\n",
      "Total training rewards: 297.8267881215088 after n steps = 271 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  272\n",
      "Total training rewards: 293.48103580478715 after n steps = 272 with final reward = 3.2037979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  273\n",
      "Total training rewards: 286.9597225718209 after n steps = 273 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  274\n",
      "Total training rewards: 278.78321861274674 after n steps = 274 with final reward = 3.2804791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  275\n",
      "Total training rewards: 301.601923090988 after n steps = 275 with final reward = 2.949177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  276\n",
      "Total training rewards: 304.0604717706909 after n steps = 276 with final reward = 3.3135809049988154\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  277\n",
      "Total training rewards: 300.98416373843276 after n steps = 277 with final reward = 3.3816274022046633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  278\n",
      "Total training rewards: 310.1908572716201 after n steps = 278 with final reward = 3.5304791148424544\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  279\n",
      "Total training rewards: 308.9376458581342 after n steps = 279 with final reward = 0.5\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  280\n",
      "Total training rewards: 304.9864546067162 after n steps = 280 with final reward = 3.449177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  281\n",
      "Total training rewards: 310.15720999090684 after n steps = 281 with final reward = 3.1419761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  282\n",
      "Total training rewards: 275.2776739332837 after n steps = 282 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  283\n",
      "Total training rewards: 182.85566335595126 after n steps = 283 with final reward = 3.3621074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  284\n",
      "Total training rewards: 284.8511000478259 after n steps = 284 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  285\n",
      "Total training rewards: 271.1747416494085 after n steps = 285 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  286\n",
      "Total training rewards: 328.6361356937764 after n steps = 286 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  287\n",
      "Total training rewards: 358.80247634753715 after n steps = 287 with final reward = 3.8919761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  288\n",
      "Total training rewards: 292.86072454261307 after n steps = 288 with final reward = 3.6121074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  289\n",
      "Total training rewards: 331.9724140622603 after n steps = 289 with final reward = 1.7751972597955166\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  290\n",
      "Total training rewards: 308.51099849497587 after n steps = 290 with final reward = 3.3621074733857594\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  291\n",
      "Total training rewards: 281.57151892463077 after n steps = 291 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  292\n",
      "Total training rewards: 297.0910650790219 after n steps = 292 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  293\n",
      "Total training rewards: 275.57643603630936 after n steps = 293 with final reward = 3.6419761312485197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  294\n",
      "Total training rewards: 300.50539106617384 after n steps = 294 with final reward = 3.4537979295441983\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  295\n",
      "Total training rewards: 356.52645038507035 after n steps = 295 with final reward = 3.792252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  296\n",
      "Total training rewards: 341.0501092952055 after n steps = 296 with final reward = 3.792252816377881\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  297\n",
      "Total training rewards: 322.8586312177273 after n steps = 297 with final reward = 3.449177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  298\n",
      "Total training rewards: 332.0362366127489 after n steps = 298 with final reward = 3.699177722498618\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  299\n",
      "Total training rewards: 343.2806960976521 after n steps = 299 with final reward = 3.792252816377881\n",
      "Copying main network weights to the target network weights\n",
      "All Nodes Were Saved !\n",
      "[85.79273517553132, 73.94649412345075, 75.52544187422083, 67.81408475255135, 76.49306202503988, 87.58223161318678, 81.87122132451404, 78.32203998245839, 90.78823211653788, 79.0596524271714, 104.47751497889914, 56.060250099426874, 67.21749098997728, 76.89288841331793, 108.76699284348376, 78.27499963207036, 59.30391879259012, 75.44415214932981, 78.48304951809223, 71.70212675825458, 68.83655817471939, 73.85669747737619, 106.67105388653987, 73.35974649425117, 75.84050443099585, 101.2028025027802, 95.70481112313054, 66.75826144540333, 102.03187337071338, 56.35334883844719, 71.82483406296217, 76.29498513787559, 81.45925361006357, 65.68390461473044, 47.4900464596263, 89.7586565555564, 128.89747537729448, 94.18655926433829, 107.24658436792824, 73.6926550363372, 94.6296182893354, 112.2243123871617, 87.15890999084762, 84.63888495970947, 114.21627135665383, 81.99876615686242, 117.16215567622217, 46.16707750559215, 66.37253167610011, 101.50579737597653, 97.33768183772979, 57.1069235288631, 78.05556188305673, 136.7353247705295, 105.17204206670047, 99.18513563313552, 100.12234080791032, 71.47629953958781, 62.29161972817679, 75.33202123220028, 65.2700806271997, 127.62050978559091, 65.47956744656103, 103.19696770355344, 90.76045599730497, 133.7055941193153, 143.1562425858874, 101.21824084139011, 192.54683415904552, 108.41804022575056, 134.4191396131539, 129.4470652838893, 107.26756457553695, 133.44558569511833, 112.67589809584366, 122.71908907706242, 169.1761505984714, 143.84147422697973, 98.28167123551117, 147.45135656696502, 153.47065668105094, 224.7186684794444, 116.35910298736358, 118.04537503228266, 127.38787741341801, 95.07050919221086, 136.26757679154514, 155.02594755674755, 144.00687691320996, 142.53639108337728, 116.89874454388361, 182.9370346017841, 120.48049404302797, 125.63475785159973, 145.43926620088305, 156.93054149356783, 120.30750207165144, 153.29963916322865, 221.00744424718891, 221.39107796031004, 154.28659131445062, 122.60503789711146, 130.7366409752383, 172.36695390945135, 158.83451867282267, 192.52146904841004, 164.45767244418698, 110.60203051596409, 201.5401068584631, 70.83010337830343, 174.3047512639508, 146.96327127674834, 231.4555298242836, 218.772968474889, 233.30499369759139, 190.23014908303765, 158.5451696004555, 174.3038867074789, 86.99691724433953, 176.3770665684808, 196.1812162738155, 235.9402665449393, 236.44733920174642, 177.04752867460928, 245.3247016439396, 155.9128896941434, 198.72594439603662, 250.30743619692282, 192.45455842580927, 152.67971948679744, 222.3888915331134, 212.57071233258353, 212.4143689054561, 260.93929576518315, 234.89524925276842, 245.48795928216006, 274.1788567746042, 252.41238149663383, 232.56465150256994, 235.92920327346167, 158.8049676194753, 254.02267704158066, 205.2392171320265, 192.2901162830903, 261.4942637412565, 221.50318765225202, 233.37671586292942, 256.4879580995865, 252.50881695421157, 279.8979902654147, 228.99381536818794, 204.8656131707131, 205.06449418335393, 243.88938468617806, 229.82212853493147, 295.19482998083726, 271.84648903777526, 277.52342443706164, 231.90031348226455, 255.49377210516093, 227.0279516674898, 234.31697180740127, 280.07814843717836, 317.84614126709806, 340.9996910874262, 344.3590218953565, 303.06274191813344, 277.7677171675417, 319.3130764647466, 333.6573104296658, 301.92289298849846, 326.9581224524906, 380.14672371822405, 361.0009688773673, 378.4366736917261, 369.16336688433825, 325.3097186525139, 272.42358604409804, 308.4048932293679, 356.0502713096996, 357.23191078876505, 345.73569089538597, 347.76460785732763, 356.6793423646534, 336.3255633970414, 391.19925611569835, 396.58791645688024, 384.09170461880717, 334.3873333421753, 355.320738197263, 335.22253380260304, 359.3527266480363, 346.0449649825968, 296.84760797415515, 319.14632185907647, 412.6624135434731, 392.11466236188653, 390.5228704525824, 353.7720404676896, 336.03000731556335, 358.51298277892073, 382.9673340136843, 387.8049125903756, 384.3566312174595, 395.3626199667264, 391.11693657747185, 349.48849401468624, 263.5074007739758, 345.8199819983532, 227.77857628370117, 311.4806279502816, 294.24090004199405, 264.4406594993691, 297.2189413689706, 301.2718958237202, 282.72777580812266, 202.75594197504347, 316.529876656055, 291.0901832524038, 232.8477848498195, 304.621807617976, 294.7825307853997, 298.9109849131141, 309.89585545730745, 295.57538570678616, 227.91319094703422, 265.82232619517845, 270.45565606710323, 281.58325336661727, 238.60185871728376, 288.7984633389422, 314.760524810446, 311.78916984062084, 268.5449289621332, 324.38759910555484, 290.75303645402033, 315.43591391279983, 290.98697909762234, 290.00458669619184, 277.03831465097835, 365.1637907757038, 299.8774095288395, 320.7026534424935, 292.3227529314556, 279.13140480264747, 293.9094244959332, 303.4910968701723, 309.2869739983031, 290.5128074984461, 301.6177875428222, 273.7827172716658, 273.45091720891827, 250.2798160235982, 283.1451377610864, 265.79047088138293, 292.8626995208808, 275.86716976252137, 267.71755946322287, 310.1286426064795, 270.6365484349683, 299.8344372484755, 311.376203315072, 314.64694439817254, 323.2115396861898, 334.5279832818635, 301.1412955178118, 324.6814414469522, 313.7028263884026, 305.9843416954717, 304.17987069347805, 328.49630492446886, 297.8267881215088, 293.48103580478715, 286.9597225718209, 278.78321861274674, 301.601923090988, 304.0604717706909, 300.98416373843276, 310.1908572716201, 308.9376458581342, 304.9864546067162, 310.15720999090684, 275.2776739332837, 182.85566335595126, 284.8511000478259, 271.1747416494085, 328.6361356937764, 358.80247634753715, 292.86072454261307, 331.9724140622603, 308.51099849497587, 281.57151892463077, 297.0910650790219, 275.57643603630936, 300.50539106617384, 356.52645038507035, 341.0501092952055, 322.8586312177273, 332.0362366127489, 343.2806960976521]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 2425\n",
      "Collisions : 23\n",
      "Received : 2402\n",
      "Processed : 2425\n",
      "Loss :  0\n",
      "DER1 : 99.05 %\n",
      "DER2 : 99.05 %\n",
      "Energy : 0.03567245721600001 J\n",
      "[0.008674986666666667, 0.00617472, 0.02224128, 0.0, 0.0, 0.0]\n",
      "DER ==> 97.42158400260867\n",
      "[0.9827996707638043, 0.9877265014014472, 0.956492281173545, 1.0, 1.0, 1.0]\n",
      "[0.008674986666666667, 0.00617472, 0.02224128, 0.0, 0.0, 0.0]\n",
      "Error : 1.63\n",
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 2428\n",
      "Collisions : 29\n",
      "Received : 2399\n",
      "Processed : 2428\n",
      "Loss :  0\n",
      "DER1 : 98.81 %\n",
      "DER2 : 98.81 %\n",
      "Energy : 0.022289221632000013 J\n",
      "[0.008674986666666667, 0.00617472, 0.02224128, 0.0, 0.0, 0.0]\n",
      "DER ==> 97.42158400260867\n",
      "[0.9827996707638043, 0.9877265014014472, 0.956492281173545, 1.0, 1.0, 1.0]\n",
      "[0.008674986666666667, 0.00617472, 0.02224128, 0.0, 0.0, 0.0]\n",
      "Error : 1.38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_names = ['nodes_5000.alloc','nodes_5000.rl']\n",
    "for i in range (2):\n",
    "    file_name = file_names [i]\n",
    "    nodes = load_nodes(file_name)\n",
    "    sf_distribution, tp_dist, u_ch = compute_sf_ch_utilization(nodes)\n",
    "    def transmit(env, node):\n",
    "\n",
    "        \"\"\"transmit\n",
    "\n",
    "        Parameters\n",
    "                #print (\"frequency coll 125\")\n",
    "        ----------\n",
    "            env : simpy.core.environment\n",
    "            node\n",
    "        \"\"\"\n",
    "        while True:\n",
    "\n",
    "            r = random.expovariate(1.0 / float(AVGSENDTIME))\n",
    "            # r = random.expovariate(1.0 / float(avg_gen(node, AVGSENDTIME)))\n",
    "            # node.freq = random.choice(ch)\n",
    "            # node.packet.freq =node.freq\n",
    "            # get_next_ch(node)\n",
    "            # print('duty cycle : ',r-(node.rectime*99))\n",
    "            # if (r-(node.rectime*99) < 0):\n",
    "            # r = node.rectime*99\n",
    "            yield env.timeout(r)\n",
    "            \"\"\"print(\n",
    "                \"Node-ID :{} SF :{} TX :{} Channel:{}  \".format(\n",
    "                    node.id, node.sf, node.tx, node.freq\n",
    "                )\n",
    "            )\"\"\"\n",
    "            node.sent = node.sent + 1\n",
    "            if node in packetsAtBS:\n",
    "                print(\"ERROR: packet already in\")\n",
    "            else:\n",
    "                sensitivity = config_dict[node.sf - 7][\"sens\"]\n",
    "                # print(\"node id: {} sf :{} s: {} \".format(node.id,node.sf,sensitivity))\n",
    "                if node.packet.rssi < sensitivity:\n",
    "                    node.packet.lost = True\n",
    "                else:\n",
    "                    node.packet.lost = False\n",
    "                    if (\n",
    "                        checkcollision(\n",
    "                            env,\n",
    "                            node.packet,\n",
    "                            packetsAtBS,\n",
    "                            maxBSReceives,\n",
    "                            full_collision,\n",
    "                        )\n",
    "                        == 1\n",
    "                    ):\n",
    "                        node.packet.collided = 1\n",
    "                    else:\n",
    "                        node.packet.collided = 0\n",
    "                        node.received = node.received + 1\n",
    "                    packetsAtBS.append(node)\n",
    "                    node.packet.addTime = env.now\n",
    "            # print('rectime : ',node.packet.rectime)\n",
    "            yield env.timeout(node.packet.rectime)\n",
    "\n",
    "            if node.packet.lost:\n",
    "                global nrLost\n",
    "                nrLost += 1\n",
    "            if node.packet.collided == 1:\n",
    "                global nrCollisions\n",
    "                nrCollisions = nrCollisions + 1\n",
    "            if not node.packet.collided and not node.packet.lost:\n",
    "                global nrReceived\n",
    "                nrReceived = nrReceived + 1\n",
    "            if node.packet.processed == 1:\n",
    "                global nrProcessed\n",
    "                nrProcessed = nrProcessed + 1\n",
    "            if node in packetsAtBS:\n",
    "                packetsAtBS.remove(node)\n",
    "                # reset the packet\n",
    "            node.packet.collided = 0\n",
    "            node.packet.processed = 0\n",
    "            node.packet.lost = False\n",
    "\n",
    "    MODEL = 6\n",
    "    packetsAtBS = []\n",
    "    sim_env = simpy.Environment()\n",
    "    bsId = 1   \n",
    "    maxBSReceives = 8\n",
    "    nrCollisions = 0\n",
    "    nrReceived = 0\n",
    "    nrProcessed = 0\n",
    "    nrLost = 0\n",
    "    full_collision = 1\n",
    "    # AVGSENDTIME = 800\n",
    "    simtime = 7200\n",
    "    # print(NODES)\n",
    "    for n in nodes:\n",
    "        # print(n.packet)\n",
    "        sim_env.process(transmit(sim_env, n))\n",
    "    print(\"Simulation Started\")\n",
    "    sim_env.run(until=simtime)\n",
    "    print(\"Simulation Finished\")\n",
    "\n",
    "    sent = sum(n.sent for n in nodes)\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent\n",
    "            for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "\n",
    "    print(\"Sent :\", sent)\n",
    "    print(\"Collisions :\", nrCollisions)\n",
    "    print(\"Received :\", nrReceived)\n",
    "    print(\"Processed :\", nrProcessed)\n",
    "\n",
    "    print(\"Loss : \", nrLost)\n",
    "    # print(energy)\n",
    "    der1 = (float(nrReceived) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER1 : {:.2f} %\".format(der1))\n",
    "    der2 = (float(sent - nrCollisions) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER2 : {:.2f} %\".format(der2))\n",
    "    print(\"Energy : {} J\".format(energy))\n",
    "\n",
    "    DER, DERs, charges = mathematical_model(\n",
    "        len(nodes), sf_distribution, AVGSENDTIME\n",
    "    )\n",
    "    print(charges)\n",
    "    DER = DER * 100\n",
    "    print(\"DER ==>\", DER)\n",
    "    print(DERs)\n",
    "    print(charges)\n",
    "    error = abs(DER - der1)\n",
    "    print(\"Error : {:.2f}\".format(error))\n",
    "\n",
    "    data = {\n",
    "        \"Experience\": 1,\n",
    "        \"Rho_max\":rho_max,\n",
    "        \"Packet_generation\":p,\n",
    "        \"Nodes\": len(nodes),\n",
    "        \"PathLoss\": MODEL,\n",
    "        \"Packet_Sent\": sent,\n",
    "        \"Packet_Loss\": nrLost,\n",
    "        \"Collisions\": nrCollisions,\n",
    "        \"Energy\": energy,\n",
    "        \"DER\": der1,\n",
    "        \"Mathematical-Model\": DER,\n",
    "        \"Error\": error,\n",
    "        \"SF7\": charges[0],\n",
    "        \"SF8\": charges[1],\n",
    "        \"SF9\": charges[2],\n",
    "        \"SF10\": charges[3],\n",
    "        \"SF11\": charges[4],\n",
    "        \"SF12\": charges[5],\n",
    "        \"DSF7\": sf_distribution[0],\n",
    "        \"DSF8\": sf_distribution[1],\n",
    "        \"DSF9\": sf_distribution[2],\n",
    "        \"DSF10\": sf_distribution[3],\n",
    "        \"DSF11\": sf_distribution[4],\n",
    "        \"DSF12\": sf_distribution[5],\n",
    "       }\n",
    "\n",
    "    finalReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devices = load_nodes('nodes_1000.alloc')\n",
    "#print_nodes(devices)\n",
    "#model = agent.create_model(env.action_space.shape, env.observation_space.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
