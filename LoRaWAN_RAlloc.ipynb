{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import simpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling2D, Activation, Flatten\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "# log-distance loss model parameters\n",
    "\n",
    "PTX = 14.0\n",
    "GAMMA = 2.08\n",
    "D0 = 40.0\n",
    "VAR = 0  # variance ignored for now\n",
    "LPLD0 = 127.41\n",
    "\n",
    "# Gate way localisation\n",
    "BSX = 0.0\n",
    "BSY = 0.0\n",
    "HM = 1.0  # m\n",
    "HB = 15.0  # m\n",
    "\n",
    "RAY = 1000.0\n",
    "\n",
    "# Created Nodes\n",
    "NODES = []\n",
    "\n",
    "packetlen = 20\n",
    "\n",
    "\n",
    "BW = 125\n",
    "PL = 20\n",
    "CR = 1\n",
    "\n",
    "p = 300\n",
    "AVGSENDTIME = p\n",
    "# Inter-SF Thresshold Mateix\n",
    "\n",
    "IS7 = np.array([6, -8, -9, -9, -9, -9])\n",
    "IS8 = np.array([-11, 6, -11, -12, -13, -13])\n",
    "IS9 = np.array([-15, -13, 6, -13, -14, -15])\n",
    "IS10 = np.array([-19, -18, -17, 6, -17, -18])\n",
    "IS11 = np.array([-22, -22, -21, -20, 6, -20])\n",
    "IS12 = np.array([-25, -25, -25, -24, -23, 6])\n",
    "SIR = np.array([IS7, IS8, IS9, IS10, IS11, IS12])\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    0: {\"sf\": 7, \"sens\": -126.5, \"snr\": -7.5},\n",
    "    1: {\"sf\": 8, \"sens\": -127.25, \"snr\": -10},\n",
    "    2: {\"sf\": 9, \"sens\": -131.25, \"snr\": -12.5},\n",
    "    3: {\"sf\": 10, \"sens\": -132.75, \"snr\": -15},\n",
    "    4: {\"sf\": 11, \"sens\": -133.25, \"snr\": -17.5},\n",
    "    5: {\"sf\": 12, \"sens\": -134.5, \"snr\": -20},\n",
    "}\n",
    "\n",
    "ch = [868100000, 868300000, 868500000]\n",
    "tpx = [2, 5, 8, 11, 14]\n",
    "MODEL = 6\n",
    "SEED = 913\n",
    "\n",
    "rho_max = 0.2\n",
    "tp_max = 14\n",
    "tp_min = 2\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "TX = [\n",
    "    22,\n",
    "    22,\n",
    "    22,\n",
    "    23,  # RFO/PA0: -2..1\n",
    "    24,\n",
    "    24,\n",
    "    24,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    25,\n",
    "    26,\n",
    "    31,\n",
    "    32,\n",
    "    34,\n",
    "    35,\n",
    "    44,  # PA_BOOST/PA1: 2..14\n",
    "    82,\n",
    "    85,\n",
    "    90,  # PA_BOOST/PA1: 15..17\n",
    "    105,\n",
    "    115,\n",
    "    125,\n",
    "]  # PA_BOOST/PA1+PA2: 18..20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SF_alloc_ch_utilization(NODES):\n",
    "    sorted_nodes = []\n",
    "    sorted_nodes = sorted(NODES, key=lambda x: x.dist, reverse=False)\n",
    "    c_class = np.zeros(6, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_u = np.zeros(ch_len, dtype=int)\n",
    "    # print(c_class)\n",
    "    for n in sorted_nodes:\n",
    "        sf = n.sf\n",
    "        # print(sf)\n",
    "        config = config_dict[sf - 7]\n",
    "        # print(config)\n",
    "        for i in range(sf, 12):\n",
    "            if n.snr < config[\"snr\"] or n.rssi < config[\"sens\"]:\n",
    "                if n.sf < 12:\n",
    "                    n.sf = n.sf + 1\n",
    "                    n.packet.sf = n.sf\n",
    "                    n.update_rectime(n.sf)\n",
    "            config = config_dict[n.sf - 7]\n",
    "        c_class[n.sf - 7] = c_class[n.sf - 7] + 1\n",
    "        ch_index = ch.index(n.freq)\n",
    "        ch_u[ch_index] = ch_u[ch_index] + 1\n",
    "    return sorted_nodes, c_class, ch_u\n",
    "\n",
    "\n",
    "def compute_sf_ch_utilization(nodes):\n",
    "    sf_dist = np.zeros(6, dtype=int)\n",
    "    tp_dist = np.zeros(5, dtype=int)\n",
    "    ch_len = len(ch)\n",
    "    ch_dist = np.zeros(ch_len, dtype=int)\n",
    "    # print(ch_dist)\n",
    "    # print(ch_len)\n",
    "    # print_nodes(nodes)\n",
    "    for n in nodes:\n",
    "        sf = n.sf\n",
    "        freq = n.freq\n",
    "        tx = n.tx\n",
    "        sf_dist[sf - 7] = sf_dist[sf - 7] + 1\n",
    "        tp_index = tpx.index(tx)\n",
    "        # print('index',tp_index)\n",
    "        tp_dist[tp_index] = tp_dist[tp_index] + 1\n",
    "\n",
    "        # print('Channel : ',freq)\n",
    "        ch_index = ch.index(freq)\n",
    "        # print('index',ch_index)\n",
    "        ch_dist[ch_index] = ch_dist[ch_index] + 1\n",
    "    return sf_dist, tp_dist, ch_dist\n",
    "\n",
    "\n",
    "def energy_consumption(nodes):\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "    return energy\n",
    "\n",
    "\n",
    "def print_nodes(nodes):\n",
    "    for n in nodes:\n",
    "        print(\"node_id: {} SF: {} TP: {} CH: {} RSSI: {} SNR: {}\".format(n.id, n.sf, n.tx, n.freq,n.rssi,n.snr))\n",
    "\n",
    "\n",
    "def reward_plot(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Packet:\n",
    "    def __init__(self, nodeid, sf, bw, freq, rssi, rectime):\n",
    "        self.nodeid = nodeid\n",
    "        self.pl = packetlen\n",
    "        self.arriveTime = 0\n",
    "        # Reception Parameters\n",
    "        self.sf = sf\n",
    "        self.bw = bw\n",
    "        self.freq = freq\n",
    "        self.rssi = rssi\n",
    "        self.rectime = rectime\n",
    "        self.collided = 0\n",
    "        self.processed = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class Device:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.dist = 0\n",
    "        found = False\n",
    "        while not found:\n",
    "            a = random.random()\n",
    "            b = random.random()\n",
    "            if b < a:\n",
    "                a, b = b, a\n",
    "            posx = b * RAY * math.cos(2 * math.pi * a / b) + BSX\n",
    "            posy = b * RAY * math.sin(2 * math.pi * a / b) + BSY\n",
    "            if len(NODES) > 0:\n",
    "                for index, n in enumerate(NODES):\n",
    "                    dist = np.sqrt(((abs(n.x - posx)) ** 2) + ((abs(n.y - posy)) ** 2))\n",
    "                    if dist >= 20:\n",
    "                        found = 1\n",
    "                        self.x = posx\n",
    "                        self.y = posy\n",
    "                        break\n",
    "            else:\n",
    "                self.x = posx\n",
    "                self.y = posy\n",
    "                found = True\n",
    "\n",
    "        dist_2d = np.sqrt(\n",
    "            (self.x - BSX) * (self.x - BSX) + (self.y - BSY) * (self.y - BSY)\n",
    "        )\n",
    "        # self.dist = np.sqrt((dist_2d)**2+(HB-HM)**2)\n",
    "        self.dist = dist_2d\n",
    "        # Radio Parameters\n",
    "        self.bw = BW\n",
    "        self.sf = 7\n",
    "        self.tx = PTX\n",
    "        self.cr = 1\n",
    "        self.freq = random.choice(ch)\n",
    "        # self.freq = 868100000\n",
    "        self.rssi = self.tx - self.estimatePathLoss(MODEL)\n",
    "        self.snr = random.randrange(-10, 20)\n",
    "        self.rectime = self.airtime(self.sf, self.cr, packetlen, BW)\n",
    "        self.packet = Packet(\n",
    "            self.id, self.sf, self.bw, self.freq, self.rssi, self.rectime\n",
    "        )\n",
    "        self.sent = 0\n",
    "        self.received = 0\n",
    "\n",
    "    def airtime(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def log_distance_loss(self, dist):\n",
    "        Lpl = LPLD0 + 10 * GAMMA * math.log10(dist / D0)\n",
    "        rssi = PTX - Lpl\n",
    "        return rssi\n",
    "\n",
    "    def update_rectime(self, sf):\n",
    "        self.rectime = self.airtime(sf, self.cr, packetlen, BW)\n",
    "        self.packet.rectime = self.rectime\n",
    "\n",
    "    def estimatePathLoss(self, model):\n",
    "        # Log-Distance model\n",
    "        if model == 0:\n",
    "            Lpl = LPLD0 + 10 * GAMMA * math.log10(self.dist / D0)\n",
    "\n",
    "        # Okumura-Hata model\n",
    "        elif model >= 1 and model <= 4:\n",
    "            # small and medium-size cities\n",
    "            if model == 1:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = 0\n",
    "            # metropolitan areas\n",
    "            elif model == 2:\n",
    "                if self.freq <= 200000000:\n",
    "                    ahm = 8.29 * ((math.log10(1.54 * HM)) ** 2) - 1.1\n",
    "                elif self.freq >= 400000000:\n",
    "                    ahm = 3.2 * ((math.log10(11.75 * HM)) ** 2) - 4.97\n",
    "                C = 0\n",
    "            # suburban enviroments\n",
    "            elif model == 3:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = -2 * ((math.log10(self.freq) - math.log10(28000000)) ** 2) - 5.4\n",
    "            # rural area\n",
    "            elif model == 4:\n",
    "                ahm = (\n",
    "                    1.1 * (math.log10(self.freq) - math.log10(1000000)) - 0.7\n",
    "                ) * HM - (1.56 * (math.log10(self.freq) - math.log10(1000000)) - 0.8)\n",
    "\n",
    "                C = (\n",
    "                    -4.78 * ((math.log10(self.freq) - math.log10(1000000)) ** 2)\n",
    "                    + 18.33 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                    - 40.98\n",
    "                )\n",
    "\n",
    "            A = (\n",
    "                69.55\n",
    "                + 26.16 * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log(HB)\n",
    "                - ahm\n",
    "            )\n",
    "\n",
    "            B = 44.9 - 6.55 * math.log10(HB)\n",
    "\n",
    "            Lpl = A + B * (math.log10(self.dist) - math.log10(1000)) + C\n",
    "\n",
    "        # 3GPP model\n",
    "        elif model >= 5 and model < 7:\n",
    "            # Suburban Macro\n",
    "            if model == 5:\n",
    "                C = 0  # dB\n",
    "            # Urban Macro\n",
    "            elif model == 6:\n",
    "                C = 3  # dB\n",
    "\n",
    "            Lpl = (\n",
    "                (44.9 - 6.55 * math.log10(HB))\n",
    "                * (math.log10(self.dist) - math.log10(1000))\n",
    "                + 45.5\n",
    "                + (35.46 - 1.1 * HM) * (math.log10(self.freq) - math.log10(1000000))\n",
    "                - 13.82 * math.log10(HM)\n",
    "                + 0.7 * HM\n",
    "                + C\n",
    "            )\n",
    "\n",
    "        # Polynomial 3rd degree\n",
    "        elif model == 7:\n",
    "            p1 = -5.491e-06\n",
    "            p2 = 0.002936\n",
    "            p3 = -0.5004\n",
    "            p4 = -70.57\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 3)\n",
    "                + p2 * math.pow(self.dist, 2)\n",
    "                + p3 * self.dist\n",
    "                + p4\n",
    "            )\n",
    "\n",
    "        # Polynomial 6th degree\n",
    "        elif model == 8:\n",
    "            p1 = 3.69e-12\n",
    "            p2 = 5.997e-11\n",
    "            p3 = -1.381e-06\n",
    "            p4 = 0.0005134\n",
    "            p5 = -0.07318\n",
    "            p6 = 4.254\n",
    "            p7 = -171\n",
    "\n",
    "            Lpl = (\n",
    "                p1 * math.pow(self.dist, 6)\n",
    "                + p2 * math.pow(self.dist, 5)\n",
    "                + p3 * math.pow(self.dist, 4)\n",
    "                + p4 * math.pow(self.dist, 3)\n",
    "                + p5 * math.pow(self.dist, 2)\n",
    "                + p6 * self.dist\n",
    "                + p7\n",
    "            )\n",
    "\n",
    "        return Lpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Collision Detection ################\n",
    "\"\"\"\n",
    "\n",
    "This Part Allows Collision Checking Between Two Packets :\n",
    " 1- Timing Collision\n",
    " 2- Frequency Collision\n",
    " 3- SF Collision\n",
    " 4- Capture Effect\n",
    " 5- Imperfect SF Orthogonality\n",
    " \n",
    "\"\"\"\n",
    "########################### 1-Timining Collision #################\n",
    "\n",
    "\n",
    "def timingCollision(env, p1, p2):\n",
    "    Npream = 8\n",
    "    Tpreamb = 2 ** p1.sf / (1.0 * p1.bw) * (Npream - 5)\n",
    "    p2_end = p2.addTime + p2.rectime\n",
    "\n",
    "    p1_cs = env.now + (Tpreamb / 1000.0)  # to sec\n",
    "\n",
    "    \"\"\" print (\"collision timing node {} ({},{},{}) node {} ({},{})\".format(\n",
    "        p1.nodeid, env.now - env.now, p1_cs - env.now, p1.rectime,\n",
    "        p2.nodeid, p2.addTime - env.now, p2_end - env.now\n",
    "    ))\"\"\"\n",
    "    if p1_cs < p2_end:\n",
    "        # p1 collided with p2 and lost\n",
    "        # print (\"not late enough\")\n",
    "        return True\n",
    "    # print (\"saved by the preamble\")\n",
    "    return False\n",
    "\n",
    "\n",
    "######################### 2-Frequency Collision #######################\n",
    "def frequencyCollision(p1, p2):\n",
    "    if abs(p1.freq - p2.freq) <= 120 and (p1.bw == 500 or p2.freq == 500):\n",
    "        # print (\"frequency coll 500\")\n",
    "        return True\n",
    "    elif abs(p1.freq - p2.freq) <= 60 and (p1.bw == 250 or p2.freq == 250):\n",
    "        # print( \"frequency coll 250\")\n",
    "        return True\n",
    "    else:\n",
    "        if abs(p1.freq - p2.freq) <= 30:\n",
    "            # print (\"frequency coll 125\")\n",
    "            return True\n",
    "        # else:\n",
    "    # print (\"no frequency coll\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 3- SF Collision ###############################\n",
    "def sfCollision(p1, p2):\n",
    "    if p1.sf == p2.sf:\n",
    "        # print (\"collision sf node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "        return True\n",
    "    # print (\"no sf collision\")\n",
    "    return False\n",
    "\n",
    "\n",
    "####################### 4-5 ############################################\n",
    "def powerCollision_2(p1, p2):\n",
    "    # powerThreshold = 6\n",
    "    # print (\"SF: node {0.nodeid} {0.sf} node {1.nodeid} {1.sf}\".format(p1, p2))\n",
    "    # print (\"pwr: node {0.nodeid} {0.rssi:3.2f} dBm node {1.nodeid} {1.rssi:3.2f} dBm; diff {2:3.2f} dBm\".format(p1, p2, round(p1.rssi - p2.rssi,2)))\n",
    "\n",
    "    if p1.sf == p2.sf:\n",
    "\n",
    "        if abs(p1.rssi - p2.rssi) < 6:\n",
    "            # print (\"collision pwr both node {} and node {}\".format(p1.nodeid, p2.nodeid))\n",
    "            # packets are too close to each other, both collide\n",
    "            # return both packets as casualties\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{} ==> |{}| < {}\".format(\n",
    "                    p1.freq, p2.freq, abs(p1.rssi - p2.rssi), SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1, p2)\n",
    "        elif p1.rssi - p2.rssi < 6:\n",
    "            # p2 overpowered p1, return p1 as casualty\n",
    "            # print (\"collision pwr node {} overpowered node {}\".format(p2.nodeid, p1.nodeid))\n",
    "            # print (\"capture - p2 wins, p1 lost\")\n",
    "            \"\"\"print(\n",
    "                \"power coll  cap : freq 1 : {} and freq 2 :{}  => {} < {}\".format(\n",
    "                    p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "                )\n",
    "            )\"\"\"\n",
    "            return (p1,)\n",
    "        # print (\"capture - p1 wins, p2 lost\")\n",
    "        # p2 was the weaker packet, return it as a casualty\n",
    "        \"\"\"print(\n",
    "            \"power coll  cap : freq 1 : {} and freq 2 :{} => {} > {}\".format(\n",
    "                p1.freq, p2.freq, p1.rssi - p2.rssi, SIR[p1.sf - 7][p2.sf - 7]\n",
    "            )\n",
    "        )\"\"\"\n",
    "        return (p2,)\n",
    "    else:\n",
    "\n",
    "        if p1.rssi - p2.rssi > SIR[p1.sf - 7][p2.sf - 7]:\n",
    "\n",
    "            # print (\"P1 is OK\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "                # print (\"p2 is OK\")\n",
    "                return ()\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"power coll  Imp : 2\")\n",
    "                return (p2,)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # print (\"p1 is lost\")\n",
    "            if p2.rssi - p1.rssi > SIR[p2.sf - 7][p1.sf - 7]:\n",
    "\n",
    "                # print (\"p2 is OK\")\n",
    "                # print(\"power coll  Imp : 1\")\n",
    "                return (p1,)\n",
    "            else:\n",
    "                # print (\"p2 is lost\")\n",
    "                # print(\"opwer coll  cap : 1\")\n",
    "                return (p1, p2)\n",
    "\n",
    "\n",
    "######################## Check Collision ############################\n",
    "def checkcollision(env, packet, packetsAtBS, maxBSReceives, full_collision):\n",
    "    col = 0  # flag needed since there might be several collisions for packet\n",
    "    processing = 0\n",
    "    for i in range(0, len(packetsAtBS)):\n",
    "        if packetsAtBS[i].packet.processed == 1:\n",
    "            processing = processing + 1\n",
    "    if processing > maxBSReceives:\n",
    "        # print (\"too long:\", len(packetsAtBS))\n",
    "        packet.processed = 0\n",
    "    else:\n",
    "        packet.processed = 1\n",
    "\n",
    "    if packetsAtBS:\n",
    "        # print (\"CHECK node {} (sf:{} bw:{} freq:{:.6e}) others: {}\".format(\n",
    "        # packet.nodeid, packet.sf, packet.bw, packet.freq,\n",
    "        # len(packetsAtBS)))\n",
    "        # print(len(packetsAtBS))\n",
    "        for other in packetsAtBS:\n",
    "\n",
    "            if other.id != packet.nodeid:\n",
    "                # print (\">> node {} (sf:{} bw:{} freq:{:.6e})\".format(\n",
    "                # other.id, other.packet.sf, other.packet.bw, other.packet.freq))\n",
    "                if full_collision == 1 or full_collision == 2:\n",
    "\n",
    "                    if frequencyCollision(packet, other.packet) and timingCollision(\n",
    "                        env, packet, other.packet\n",
    "                    ):\n",
    "                        # check who collides in the power domain\n",
    "                        if full_collision == 1:\n",
    "                            # Capture effect\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        else:\n",
    "                            # Capture + Non-orthognalitiy SFs effects\n",
    "                            c = powerCollision_2(packet, other.packet)\n",
    "                        # mark all the collided packets\n",
    "                        # either this one, the other one, or both\n",
    "                        for p in c:\n",
    "                            p.collided = 1\n",
    "                            if p == packet:\n",
    "                                col = 1\n",
    "\n",
    "                    else:\n",
    "                        # no freq or timing collision, all fimone\n",
    "                        pass\n",
    "                else:\n",
    "                    # simple collision\n",
    "                    if frequencyCollision(packet, other.packet) and sfCollision(\n",
    "                        packet, other.packet\n",
    "                    ):\n",
    "                        packet.collided = 1\n",
    "                        other.packet.collided = (\n",
    "                            1  # other also got lost, if it wasn't lost already\n",
    "                        )\n",
    "                        col = 1\n",
    "        return col\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Node generation and Plot Function\n",
    "\n",
    "\n",
    "def node_Gen(nrNodes, display):\n",
    "    NODES = []\n",
    "    x = []\n",
    "    y = []\n",
    "    rssi = []\n",
    "    dist = []\n",
    "\n",
    "    for i in range(nrNodes):\n",
    "        device = Device(i)\n",
    "        NODES.append(device)\n",
    "\n",
    "        x.append(NODES[i].x)\n",
    "        y.append(NODES[i].y)\n",
    "        rssi.append(NODES[i].rssi)\n",
    "        dist.append(NODES[i].dist)\n",
    "    pickle.dump(NODES, open(\"data/nodes.p\", \"wb\"))\n",
    "    if display:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x, y, \"ro\")\n",
    "        ax.plot(0, 0, \"bo\")\n",
    "        for i in range(6):\n",
    "            v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "            X, Y = np.meshgrid(v1, v2)\n",
    "            F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "            ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "        ax.set_aspect(1)\n",
    "        plt.show()\n",
    "\n",
    "    return NODES\n",
    "\n",
    "\n",
    "# SF Allocation Display\n",
    "\n",
    "# RA-Lora Plot\n",
    "def SF_alloc_plot(sorted_nodes, exp, k, display=False, save=False):\n",
    "    groups = []\n",
    "    for s in range(7, 13):\n",
    "        group = []\n",
    "        posx = []\n",
    "        posy = []\n",
    "        for n in sorted_nodes:\n",
    "            if n.sf == s:\n",
    "                posx.append(n.x)\n",
    "                posy.append(n.y)\n",
    "\n",
    "        group.append(posx)\n",
    "        group.append(posy)\n",
    "        groups.append(group)\n",
    "        group = []\n",
    "    # print(groups)\n",
    "    colors = [\"ro\", \"go\", \"bo\", \"yo\", \"co\", \"mo\"]\n",
    "    # print(len(groups[3][0]))\n",
    "    # print(len(groups[3][1]))\n",
    "    fig, ax = plt.subplots()\n",
    "    for g, c in zip(groups, colors):\n",
    "        ax.plot(g[0], g[1], c)\n",
    "\n",
    "    ax.plot(0, 0, \"ko\")\n",
    "\n",
    "    for i in range(6):\n",
    "        v1 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        v2 = np.linspace((-RAY / 6) * (i + 1), (RAY / 6) * (i + 1), 100)\n",
    "        X, Y = np.meshgrid(v1, v2)\n",
    "        F = X ** 2 + Y ** 2 - ((RAY / 6) * (i + 1)) ** 2\n",
    "\n",
    "        ax.contour(X, Y, F, [0], colors=\"k\", linestyles=\"dashed\", linewidths=1)\n",
    "    ax.set_aspect(1)\n",
    "    if save:\n",
    "        if not os.path.exists(\"reports/graphics/{}\".format(exp)):\n",
    "            os.mkdir(\"reports/graphics/{}\".format(exp))\n",
    "        plt.savefig(\"reports/graphics/{}/sf_alloc_{}\".format(exp, k))\n",
    "    if display:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def finalReport(data):\n",
    "    fname = \"results.csv\"\n",
    "    if not os.path.isfile(\"reports/results/{}\".format(fname)):\n",
    "        df_new = pd.DataFrame(data, index=[0])\n",
    "        df_new.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "    else:\n",
    "        df = pd.read_csv(\"reports/results/{}\".format(fname))\n",
    "        df_new = pd.DataFrame(data, index=[df.ndim - 1])\n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "        df.to_csv(\"reports/results/{}\".format(fname), index=False)\n",
    "\n",
    "\n",
    "def airtime(sf, cr, pl, bw):\n",
    "    H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "    DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "    Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "    if bw == 125 and sf in [11, 12]:\n",
    "        # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "        DE = 1\n",
    "    if sf == 6:\n",
    "        # can only have implicit header with SF6\n",
    "        H = 1\n",
    "    Tsym = (2.0 ** sf) / bw  # msec\n",
    "    Tpream = (Npream + 4.25) * Tsym\n",
    "    # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "    payloadSymbNB = 8 + max(\n",
    "        math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "        * (cr + 4),\n",
    "        0,\n",
    "    )\n",
    "    Tpayload = payloadSymbNB * Tsym\n",
    "    return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "        \n",
    "        \n",
    "def mathematical_model(N, sf_dist, avgtime):\n",
    "    charges = []\n",
    "    DERs = []\n",
    "    s = 0\n",
    "    for i in range(6):\n",
    "        Nsf = sf_dist[i]\n",
    "        toa = airtime(i + 7, 1, packetlen, BW)\n",
    "        c = (Nsf * toa) / avgtime\n",
    "        charges.append(c)\n",
    "        DERsf = math.exp(-2*c)\n",
    "        DERs.append(DERsf)\n",
    "        s = s + (DERsf * Nsf)\n",
    "    DER = s / N\n",
    "    return DER, DERs, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Environment(gym.Env):\n",
    "    def __init__(self, nodes, SF_dist_init, Ch_dist_init):\n",
    "\n",
    "        \"\"\" Environment Parameter\"\"\"\n",
    "\n",
    "        super(Environment, self).__init__()\n",
    "\n",
    "        self.NODES = nodes\n",
    "        self.N = len(self.NODES)\n",
    "        self.TP_SET = [2, 5, 8, 11, 14]\n",
    "        self.SF_SET = [7, 8, 9, 10, 11, 12]\n",
    "        self.CH_SET = [\"868.1\", \"868.3\", \"868.5\"]\n",
    "        self.ACTION_SPACE_GEN = self.action_space_generator()\n",
    "        self.ACTION_SPACE_SIZE = len(self.ACTION_SPACE_GEN)\n",
    "        self.STATE_SPACE = []\n",
    "        self.SF_DIST = SF_dist_init\n",
    "        self.ENERGY = 1\n",
    "        self.CH_DIST = Ch_dist_init\n",
    "\n",
    "        \"\"\" Simulation Parameters\"\"\"\n",
    "        AVGSENDTIME = 100\n",
    "        self.action_space = spaces.Box(\n",
    "            np.array([7, 0, 0]), np.array([12, 4, 2]), dtype=np.int32\n",
    "        )\n",
    "        # self.observation_space = spaces.Box(np.array([0,7,0,0,-10,-135]), np.array([N,12,4,2,10,-70]),dtype =np.int)\n",
    "        self.observation_space = spaces.Box(\n",
    "            np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "            np.array(\n",
    "                [\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                    self.N,\n",
    "                ]\n",
    "            ),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        self.condition = 0\n",
    "\n",
    "    def action_space_generator(self):\n",
    "        action_space = []\n",
    "        for sf in self.SF_SET:\n",
    "            for tp in range(len(self.TP_SET)):\n",
    "                for ch in range(len(self.CH_SET)):\n",
    "                    action_space.append([sf, tp, ch])\n",
    "                    \n",
    "        return action_space\n",
    "\n",
    "    def toa(self, sf, cr, pl, bw):\n",
    "        H = 0  # implicit header disabled (H=0) or not (H=1)\n",
    "        DE = 0  # low data rate optimization enabled (=1) or not (=0)\n",
    "        Npream = 8  # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "        if bw == 125 and sf in [11, 12]:\n",
    "            # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "            DE = 1\n",
    "        if sf == 6:\n",
    "            # can only have implicit header with SF6\n",
    "            H = 1\n",
    "        Tsym = (2.0 ** sf) / bw  # msec\n",
    "        Tpream = (Npream + 4.25) * Tsym\n",
    "        # print \"sf\", sf, \" cr\", cr, \"pl\", pl, \"bw\", bw\n",
    "        payloadSymbNB = 8 + max(\n",
    "            math.ceil((8.0 * pl - 4.0 * sf + 28 + 16 - 20 * H) / (4.0 * (sf - 2 * DE)))\n",
    "            * (cr + 4),\n",
    "            0,\n",
    "        )\n",
    "        Tpayload = payloadSymbNB * Tsym\n",
    "        return (Tpream + Tpayload) / 1000.0  # in seconds\n",
    "\n",
    "    def rho_compute(self, sf, n):\n",
    "        return (self.toa(sf, CR, PL, BW) * n) / p\n",
    "\n",
    "    def reward_function(self, rho, u_ch, tp):\n",
    "        return (rho_max) / (rho * u_ch) + ((tp_max - tp) / (tp_max - tp_min))\n",
    "\n",
    "    def reward_function_zero(self, tp):\n",
    "        return (tp_max - tp) / (tp_max - tp_min)\n",
    "\n",
    "    def compute_der(self, sf_dist):\n",
    "        der = 0\n",
    "        for index in range(len(sf_dist)):\n",
    "            rho = self.rho_compute(self.SF_SET[index], sf_dist[index])\n",
    "            der_sf = math.exp(-2 * rho)\n",
    "            der = der + (der_sf * sf_dist[index])\n",
    "        return (der / self.N) * 100\n",
    "\n",
    "    def reset(self):\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        self.condition = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print('Step : ',self.condition)\n",
    "        # print('Action : ',action)\n",
    "        # assert self.action_space.contains(action)\n",
    "        reward = 0\n",
    "        node_index = random.randint(0,self.N-1)\n",
    "        node = self.NODES[node_index]\n",
    "        old_sf = node.sf\n",
    "        old_tp = node.tx\n",
    "        old_ch = node.freq\n",
    "        new_sf = action[0]\n",
    "        new_tp = self.TP_SET[action[1]]\n",
    "        new_ch = action[2]\n",
    "\n",
    "        sf_dist, tp_dist, u_ch = compute_sf_ch_utilization(self.NODES)\n",
    "        der = self.compute_der(sf_dist)\n",
    "        #print(\"DER : \", der)\n",
    "        config = config_dict[new_sf - 7]\n",
    "        # print(\"config : \", config)\n",
    "        \n",
    "        # print('rho',rho)\n",
    "        # print(\"node snr : {} rssi : {}\".format(node.snr, node.rssi))\n",
    "        # print(\"node old sf : {} new sf: {}\".format(old_sf, new_sf))\n",
    "        if node.snr < config[\"snr\"] or node.rssi < config[\"sens\"]:\n",
    "            reward = -1\n",
    "\n",
    "        else: \n",
    "            rho = self.rho_compute(new_sf, sf_dist[new_sf - 7])\n",
    "            old_rho = self.rho_compute(old_sf, sf_dist[old_sf - 7])\n",
    "            if new_sf > old_sf:\n",
    "                if old_rho > rho_max and rho < rho_max :\n",
    "                    self.NODES[node_index].sf = new_sf\n",
    "                    self.NODES[node_index].freq = ch[new_ch]\n",
    "                    self.NODES[node_index].tx = new_tp\n",
    "                    \n",
    "                    self.NODES[node_index].packet.sf = new_sf\n",
    "                    self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "            \n",
    "                    \n",
    "                    self.NODES[node_index].update_rectime(new_sf)\n",
    "\n",
    "                    sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                    sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                    old_tp_index = self.TP_SET.index(old_tp)\n",
    "                    new_tp_index = self.TP_SET.index(new_tp)\n",
    "                    tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                    tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                    old_ch_index = ch.index(old_ch)\n",
    "                    new_ch_index = new_ch\n",
    "                    u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                    u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "            else:\n",
    "                \n",
    "                self.NODES[node_index].sf = new_sf\n",
    "                self.NODES[node_index].freq = ch[new_ch]\n",
    "                self.NODES[node_index].tx = new_tp\n",
    "                self.NODES[node_index].update_rectime(new_sf)\n",
    "                \n",
    "                self.NODES[node_index].packet.sf = new_sf\n",
    "                self.NODES[node_index].packet.freq = ch[new_ch]\n",
    "\n",
    "                sf_dist[old_sf - 7] = sf_dist[old_sf - 7] - 1\n",
    "                sf_dist[new_sf - 7] = sf_dist[new_sf - 7] + 1\n",
    "\n",
    "                old_tp_index = self.TP_SET.index(old_tp)\n",
    "                new_tp_index = self.TP_SET.index(new_tp)\n",
    "                tp_dist[old_tp_index] = tp_dist[old_tp_index] - 1\n",
    "                tp_dist[new_tp_index] = tp_dist[new_tp_index] + 1\n",
    "\n",
    "                old_ch_index = ch.index(old_ch)\n",
    "                new_ch_index = new_ch\n",
    "                u_ch[old_ch_index] = u_ch[old_ch_index] - 1\n",
    "                u_ch[new_ch_index] = u_ch[new_ch_index] + 1\n",
    "        \n",
    "\n",
    "            if rho > 0 and u_ch[new_ch] > 0:\n",
    "                reward = self.reward_function(rho, u_ch[new_ch], new_tp)\n",
    "            else:\n",
    "                reward = self.reward_function_zero(new_tp)\n",
    "\n",
    "           \n",
    "        # print(sf_dist)\n",
    "        # print(tp_dist)\n",
    "        # print(u_ch)\n",
    "\n",
    "        # print('Reward = ',reward)\n",
    "        new_state = np.concatenate((sf_dist, tp_dist, u_ch))\n",
    "        # print('')\n",
    "        done = False\n",
    "        self.condition = self.condition + 1\n",
    "        if self.condition == 100:\n",
    "            done = True\n",
    "        #if der > 95:\n",
    "            #done = True\n",
    "\n",
    "        info = \"\"\n",
    "\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nodes (nodes,file_name):\n",
    "    pickle.dump(nodes, open(\"data/\"+file_name, \"wb\"))\n",
    "    print('All Nodes Were Saved !')\n",
    "    \n",
    "def load_nodes (file_name):\n",
    "    print('Loading Nodes From File')\n",
    "    nodes = pickle.load(open(\"data/\"+file_name,\"rb\"))\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Nodes Were Saved !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nodes = node_Gen(6000, display=False)\n",
    "nodes, sf_distribution, ch_u = SF_alloc_ch_utilization(nodes)\n",
    "save_nodes(nodes,'nodes_{}.alloc'.format(len(nodes)))\n",
    "#train_episodes = 300\n",
    "#test_episodes = 100\n",
    "\n",
    "train_episodes = 300\n",
    "test_episodes = 100\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def create_model(self, action_shape, state_shape):\n",
    "        model = Sequential()\n",
    "        learning_rate = 0.001\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "\n",
    "        model.add(\n",
    "            Dense(\n",
    "                14, input_shape=state_shape, activation=\"relu\", kernel_initializer=init\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(28, activation=\"relu\", kernel_initializer=init))\n",
    "\n",
    "        model.add(Dense(90, activation=\"linear\", kernel_initializer=init))\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        model.save('lora_rl_model.h5')\n",
    "        return model\n",
    "\n",
    "    def get_qs(self, model, state, step):\n",
    "        return model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "\n",
    "    def train(self, env, replay_memory, model, target_model, done):\n",
    "        learning_rate = 0.7  # Learning rate\n",
    "        discount_factor = 0.618\n",
    "\n",
    "        MIN_REPLAY_SIZE = 1000\n",
    "        if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "            return\n",
    "\n",
    "        batch_size = 64 * 2\n",
    "        mini_batch = random.sample(replay_memory, batch_size)\n",
    "        # print(len(mini_batch))\n",
    "        # print(mini_batch)\n",
    "\n",
    "        # current_states = np.array([transition[0] for transition in mini_batch],dtype=object)\n",
    "        # print()\n",
    "        current_states = np.array(\n",
    "            np.array(\n",
    "                [\n",
    "                    transition[0]\n",
    "                    for transition in mini_batch\n",
    "                    if transition[0] is not None\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        # print('states len : ',len(current_states))\n",
    "        # current_states= np.array(current_states).astype(\"float32\")\n",
    "        # print(current_states)\n",
    "        # current_states = np.asarray(current_states).astype(np.float32)\n",
    "        current_qs_list = model.predict(current_states)\n",
    "        # print(current_qs_list)\n",
    "        # print('Current: ',len(current_qs_list))\n",
    "        # print('Mini batch : ',len(mini_batch))\n",
    "        new_current_states = np.array(\n",
    "            [transition[3] for transition in mini_batch if transition[3] is not None]\n",
    "        )\n",
    "        future_qs_list = target_model.predict(new_current_states)\n",
    "        # print()\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        #print(mini_batch)\n",
    "        #print(len(mini_batch))\n",
    "        for index, (observation, action, reward, new_observation, done) in enumerate(\n",
    "            mini_batch\n",
    "        ):\n",
    "            if not done:\n",
    "                max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "            else:\n",
    "                max_future_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            #print('The current_qs : ',current_qs)\n",
    "            #print('Action : ',action)\n",
    "            #print(type(action))\n",
    "            #print(env.ACTION_SPACE_GEN)\n",
    "            action_index = env.ACTION_SPACE_GEN.index(action)\n",
    "            current_qs[action_index] = (1 - learning_rate) * current_qs[\n",
    "                action_index\n",
    "            ] + learning_rate * max_future_q\n",
    "\n",
    "            X.append(observation)\n",
    "            Y.append(current_qs)\n",
    "        model.fit(\n",
    "            np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True\n",
    "        )\n",
    "\n",
    "\n",
    "env = Environment(nodes, sf_distribution, ch_u)\n",
    "\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "# agent.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    epsilon = 1  # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "    max_epsilon = 1  # You can't explore more than 100% of the time\n",
    "    min_epsilon = 0.01  # At a minimum, we'll always explore 1% of the time\n",
    "    decay = 0.01\n",
    "    final_rewards = []\n",
    "    # 1. Initialize the Target and Main models\n",
    "    # Main Model (updated every 4 steps)\n",
    "    print(env.observation_space.shape)\n",
    "    print(env.action_space.shape)\n",
    "    #print_nodes(nodes)\n",
    "    #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    model = agent.create_model(env.action_space.shape, env.observation_space.shape)\n",
    "    print(\"Model Created !\")\n",
    "    # Target Model (updated every 100 steps)\n",
    "    target_model = agent.create_model(\n",
    "        env.action_space.shape, env.observation_space.shape\n",
    "    )\n",
    "    print(\"Target Model Created !\")\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "    target_update_counter = 0\n",
    "\n",
    "    # X = states, y = actions\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    steps_to_update_target_model = 0\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        print('Episode : ',episode)\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps_to_update_target_model += 1\n",
    "            # if True:\n",
    "            # env.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                # print('Explore')\n",
    "                action = env.action_space.sample()\n",
    "                a =[]\n",
    "                a.append(action[0])\n",
    "                a.append(action[1])\n",
    "                a.append(action[2])\n",
    "                action = a\n",
    "            else:\n",
    "\n",
    "                # Exploit best known action\n",
    "                # model dims are (batch, env.observation_space.n)\n",
    "                # print('Exploit')\n",
    "                encoded = observation\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                # print('model : ',model)\n",
    "                # print('Encoded state : ',encoded_reshaped)\n",
    "                predicted = model.predict(encoded_reshaped)\n",
    "                action = np.argmax(predicted)\n",
    "                action = env.ACTION_SPACE_GEN[action]\n",
    "            #print(action)\n",
    "            #print(type(action))\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "            # 3. Update the Main Network using the Bellman Equation\n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                agent.train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\n",
    "                    \"Total training rewards: {} after n steps = {} with final reward = {}\".format(\n",
    "                        total_training_rewards, episode, reward\n",
    "                    )\n",
    "                )\n",
    "                final_rewards.append(total_training_rewards)\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    print(\"Copying main network weights to the target network weights\")\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        #SF_alloc_plot(nodes,10,5,display=True)\n",
    "    env.close()\n",
    "    save_nodes(env.NODES,'nodes_{}.rl'.format(len(env.NODES)))\n",
    "    #print_nodes(env.NODES)\n",
    "    #print(final_rewards)\n",
    "    #print_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "(3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamnache/anaconda3/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created !\n",
      "Target Model Created !\n",
      "Episode :  0\n",
      "Total training rewards: -22.240527616497612 after n steps = 0 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  1\n",
      "Total training rewards: -33.24054026175822 after n steps = 1 with final reward = 0.25013150657677985\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f81fc58d4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f81fc58d4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: -21.23928449224001 after n steps = 2 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  3\n",
      "Total training rewards: -1.9876121568473886 after n steps = 3 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  4\n",
      "Total training rewards: -12.242431390767955 after n steps = 4 with final reward = 1.0001267412010377\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  5\n",
      "Total training rewards: -15.4905140275369 after n steps = 5 with final reward = 1.0000123662793103\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  6\n",
      "Total training rewards: -19.489223179878717 after n steps = 6 with final reward = 0.7500122121631674\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  7\n",
      "Total training rewards: -20.24050921237618 after n steps = 7 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  8\n",
      "Total training rewards: -12.740178455483017 after n steps = 8 with final reward = 0.00012472543640891387\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  9\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f81fc4c4d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f81fc4c4d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f81fc2eedd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f81fc2eedd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Total training rewards: -15.488332338197496 after n steps = 9 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  10\n",
      "Total training rewards: -14.240063093752543 after n steps = 10 with final reward = 1.000131639411706\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  11\n",
      "Total training rewards: -18.737900593277843 after n steps = 11 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  12\n",
      "Total training rewards: -8.487751418752445 after n steps = 12 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  13\n",
      "Total training rewards: -5.488237429912356 after n steps = 13 with final reward = 0.7501246634764306\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  14\n",
      "Total training rewards: -15.739642670777219 after n steps = 14 with final reward = 0.5001547477435689\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  15\n",
      "Total training rewards: -12.990514104759058 after n steps = 15 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  16\n",
      "Total training rewards: -22.991101759953892 after n steps = 16 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  17\n",
      "Total training rewards: -31.74166223053133 after n steps = 17 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  18\n",
      "Total training rewards: -24.492727882533515 after n steps = 18 with final reward = 1.0001297391912283\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  19\n",
      "Total training rewards: -13.740111206692102 after n steps = 19 with final reward = 1.0003394775385712\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  20\n",
      "Total training rewards: -18.98983911441782 after n steps = 20 with final reward = 0.750687348079578\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  21\n",
      "Total training rewards: -26.99027005260045 after n steps = 21 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  22\n",
      "Total training rewards: -24.741911597435017 after n steps = 22 with final reward = 0.7500123912996023\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  23\n",
      "Total training rewards: -15.740043233575534 after n steps = 23 with final reward = 0.5001293528710559\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  24\n",
      "Total training rewards: -24.989825585574287 after n steps = 24 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  25\n",
      "Total training rewards: -24.989484890977813 after n steps = 25 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  26\n",
      "Total training rewards: -6.491247299180398 after n steps = 26 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  27\n",
      "Total training rewards: -26.24352670495859 after n steps = 27 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  28\n",
      "Total training rewards: -9.490576938333348 after n steps = 28 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  29\n",
      "Total training rewards: -23.740325102550653 after n steps = 29 with final reward = 0.25012954574313\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  30\n",
      "Total training rewards: -6.489142626062627 after n steps = 30 with final reward = 1.000154513750005\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  31\n",
      "Total training rewards: -20.241293231211152 after n steps = 31 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  32\n",
      "Total training rewards: -19.49132164399621 after n steps = 32 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  33\n",
      "Total training rewards: -21.741420700782697 after n steps = 33 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  34\n",
      "Total training rewards: -18.992183136378934 after n steps = 34 with final reward = 0.7503484387457731\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  35\n",
      "Total training rewards: -5.241100852635037 after n steps = 35 with final reward = 0.0003528180864256139\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  36\n",
      "Total training rewards: -11.739483501027 after n steps = 36 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  37\n",
      "Total training rewards: -17.492645212167822 after n steps = 37 with final reward = 0.7500121455623766\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  38\n",
      "Total training rewards: -7.239434326435523 after n steps = 38 with final reward = 1.0000122487996568\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  39\n",
      "Total training rewards: -15.738814165652732 after n steps = 39 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  40\n",
      "Total training rewards: -30.490369132860334 after n steps = 40 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  41\n",
      "Total training rewards: -24.49395062756478 after n steps = 41 with final reward = 0.25001225492712037\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  42\n",
      "Total training rewards: -18.491658136961036 after n steps = 42 with final reward = 0.7503572507754571\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  43\n",
      "Total training rewards: -5.739572420641487 after n steps = 43 with final reward = 0.7500123165406303\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  44\n",
      "Total training rewards: -12.990858312743114 after n steps = 44 with final reward = 0.7501303230175889\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  45\n",
      "Total training rewards: -21.990901250748593 after n steps = 45 with final reward = 0.5001532010394852\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  46\n",
      "Total training rewards: -16.741519126552852 after n steps = 46 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  47\n",
      "Total training rewards: -13.24011175886241 after n steps = 47 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  48\n",
      "Total training rewards: -15.488694189735604 after n steps = 48 with final reward = 0.5001525150646816\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  49\n",
      "Total training rewards: -9.238593699435224 after n steps = 49 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  50\n",
      "Total training rewards: -13.992226985900494 after n steps = 50 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  51\n",
      "Total training rewards: -4.989315487774497 after n steps = 51 with final reward = 0.5003619758148108\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  52\n",
      "Total training rewards: -15.490285719917601 after n steps = 52 with final reward = 0.2505362348269396\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  53\n",
      "Total training rewards: -32.74009439351269 after n steps = 53 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  54\n",
      "Total training rewards: -22.74158367578685 after n steps = 54 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  55\n",
      "Total training rewards: -14.990310772054588 after n steps = 55 with final reward = 0.7501523634592495\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  56\n",
      "Total training rewards: -12.48996160605083 after n steps = 56 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  57\n",
      "Total training rewards: -22.491545042775716 after n steps = 57 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  58\n",
      "Total training rewards: -19.490515923853167 after n steps = 58 with final reward = 1.2169696628742962e-05\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  59\n",
      "Total training rewards: -19.991881735705487 after n steps = 59 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  60\n",
      "Total training rewards: -37.24188134630742 after n steps = 60 with final reward = 1.2109539947434298e-05\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  61\n",
      "Total training rewards: -28.98921409033759 after n steps = 61 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  62\n",
      "Total training rewards: -39.74240706936948 after n steps = 62 with final reward = 0.7500121215236584\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  63\n",
      "Total training rewards: -36.74317489523323 after n steps = 63 with final reward = 0.2501509380994633\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  64\n",
      "Total training rewards: -46.740365777949215 after n steps = 64 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  65\n",
      "Total training rewards: -24.491124607620364 after n steps = 65 with final reward = 0.5001236804228953\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  66\n",
      "Total training rewards: -19.490810047439243 after n steps = 66 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  67\n",
      "Total training rewards: -10.491865276234776 after n steps = 67 with final reward = 1.00001208564347\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  68\n",
      "Total training rewards: 1.009433822249314 after n steps = 68 with final reward = 0.750151460118582\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  69\n",
      "Total training rewards: -28.490023018591256 after n steps = 69 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  70\n",
      "Total training rewards: -31.99117450706279 after n steps = 70 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  71\n",
      "Total training rewards: -18.989231113964117 after n steps = 71 with final reward = 0.00015049351006875074\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  72\n",
      "Total training rewards: -21.491598487771967 after n steps = 72 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  73\n",
      "Total training rewards: -17.488943993784638 after n steps = 73 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  74\n",
      "Total training rewards: -25.24152441609099 after n steps = 74 with final reward = 0.5001277050637813\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  75\n",
      "Total training rewards: -21.990259838358142 after n steps = 75 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  76\n",
      "Total training rewards: -21.739790153879923 after n steps = 76 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  77\n",
      "Total training rewards: -19.738909768782257 after n steps = 77 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  78\n",
      "Total training rewards: -27.489666197766628 after n steps = 78 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  79\n",
      "Total training rewards: -21.239373361711117 after n steps = 79 with final reward = 0.7501226527751979\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  80\n",
      "Total training rewards: -17.737147587387245 after n steps = 80 with final reward = 1.0001597474101147\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  81\n",
      "Total training rewards: -29.23895686499617 after n steps = 81 with final reward = 0.00015958109318586427\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  82\n",
      "Total training rewards: -28.237378486571355 after n steps = 82 with final reward = 0.5004905778903939\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  83\n",
      "Total training rewards: -24.74248316064818 after n steps = 83 with final reward = 0.2504884178116201\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  84\n",
      "Total training rewards: -13.488742849270096 after n steps = 84 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  85\n",
      "Total training rewards: -32.24199276938375 after n steps = 85 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  86\n",
      "Total training rewards: -6.737467924683663 after n steps = 86 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  87\n",
      "Total training rewards: -25.239097902051654 after n steps = 87 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  88\n",
      "Total training rewards: -36.73999310193601 after n steps = 88 with final reward = 0.50049300049726\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  89\n",
      "Total training rewards: -28.24064557145972 after n steps = 89 with final reward = 0.5004937319520038\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  90\n",
      "Total training rewards: -32.73951816017558 after n steps = 90 with final reward = 0.7500126733571204\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  91\n",
      "Total training rewards: -18.48983774713383 after n steps = 91 with final reward = 0.5004890163332394\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  92\n",
      "Total training rewards: -31.24213484648895 after n steps = 92 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  93\n",
      "Total training rewards: -23.49183161354572 after n steps = 93 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  94\n",
      "Total training rewards: -27.239805816681503 after n steps = 94 with final reward = 1.203813234086466e-05\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  95\n",
      "Total training rewards: -41.74053116442113 after n steps = 95 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  96\n",
      "Total training rewards: -24.99161243701853 after n steps = 96 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  97\n",
      "Total training rewards: -24.990968593537545 after n steps = 97 with final reward = 1.0000127591663093\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  98\n",
      "Total training rewards: -22.98845950481269 after n steps = 98 with final reward = 1.0004858034097088\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  99\n",
      "Total training rewards: -23.74031521063451 after n steps = 99 with final reward = 0.7501487410383357\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  100\n",
      "Total training rewards: -20.9882629473969 after n steps = 100 with final reward = 0.5000128058543197\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  101\n",
      "Total training rewards: -28.739974464557406 after n steps = 101 with final reward = 1.000126037734612\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  102\n",
      "Total training rewards: -21.490809268499653 after n steps = 102 with final reward = 0.7501606683857495\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  103\n",
      "Total training rewards: -38.48931180838364 after n steps = 103 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  104\n",
      "Total training rewards: -40.98971381505174 after n steps = 104 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  105\n",
      "Total training rewards: -42.2409172016292 after n steps = 105 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  106\n",
      "Total training rewards: -29.74083565755837 after n steps = 106 with final reward = 0.7500121636540783\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  107\n",
      "Total training rewards: -22.489883714922605 after n steps = 107 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  108\n",
      "Total training rewards: -31.741637678166455 after n steps = 108 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  109\n",
      "Total training rewards: -22.740696734100364 after n steps = 109 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  110\n",
      "Total training rewards: -34.48951385671678 after n steps = 110 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  111\n",
      "Total training rewards: -23.239619858372894 after n steps = 111 with final reward = 0.00015875467633870807\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  112\n",
      "Total training rewards: -46.49239254696156 after n steps = 112 with final reward = 0.7501235586302584\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  113\n",
      "Total training rewards: -26.48952013175241 after n steps = 113 with final reward = 0.7501282075923156\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  114\n",
      "Total training rewards: -14.98987786766217 after n steps = 114 with final reward = 0.750128018681325\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  115\n",
      "Total training rewards: -18.242503791477098 after n steps = 115 with final reward = 0.00012472543640891387\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  116\n",
      "Total training rewards: -35.99269079294837 after n steps = 116 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  117\n",
      "Total training rewards: -14.740938487982596 after n steps = 117 with final reward = 0.7501273307450794\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  118\n",
      "Total training rewards: -32.492282487435624 after n steps = 118 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  119\n",
      "Total training rewards: -36.741082189196305 after n steps = 119 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  120\n",
      "Total training rewards: -40.49305532787192 after n steps = 120 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  121\n",
      "Total training rewards: -30.740701715083873 after n steps = 121 with final reward = 1.0003865823900397\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  122\n",
      "Total training rewards: -26.493567870597275 after n steps = 122 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  123\n",
      "Total training rewards: -23.23523579266474 after n steps = 123 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  124\n",
      "Total training rewards: -17.23552217902553 after n steps = 124 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  125\n",
      "Total training rewards: -49.23950525716611 after n steps = 125 with final reward = 0.00012796918819721302\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  126\n",
      "Total training rewards: -34.73700308615781 after n steps = 126 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  127\n",
      "Total training rewards: -36.98852320831909 after n steps = 127 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  128\n",
      "Total training rewards: -33.485393632856045 after n steps = 128 with final reward = 0.0001327118305385046\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  129\n",
      "Total training rewards: -41.99325835215157 after n steps = 129 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  130\n",
      "Total training rewards: -24.747052753906335 after n steps = 130 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  131\n",
      "Total training rewards: -17.746117473661954 after n steps = 131 with final reward = 0.7504974220263835\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  132\n",
      "Total training rewards: -24.743970124043706 after n steps = 132 with final reward = 0.7503696836509859\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  133\n",
      "Total training rewards: -36.740996919571394 after n steps = 133 with final reward = 0.2501533543171636\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  134\n",
      "Total training rewards: -35.24137011926803 after n steps = 134 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  135\n",
      "Total training rewards: -42.24178954884247 after n steps = 135 with final reward = 1.0000120145165834\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  136\n",
      "Total training rewards: -38.992944777059755 after n steps = 136 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  137\n",
      "Total training rewards: -41.49303441536143 after n steps = 137 with final reward = 1.2378776813370178e-05\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  138\n",
      "Total training rewards: -27.49608449195919 after n steps = 138 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  139\n",
      "Total training rewards: 5.00640120695344 after n steps = 139 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  140\n",
      "Total training rewards: 20.258351645106426 after n steps = 140 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  141\n",
      "Total training rewards: 17.25502547340605 after n steps = 141 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  142\n",
      "Total training rewards: 0.25877693060266993 after n steps = 142 with final reward = 0.5000119035953905\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  143\n",
      "Total training rewards: -26.741800712758156 after n steps = 143 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  144\n",
      "Total training rewards: 27.507017146031632 after n steps = 144 with final reward = 0.2501255520400663\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  145\n",
      "Total training rewards: 6.007451729355025 after n steps = 145 with final reward = 0.00015632599694545907\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  146\n",
      "Total training rewards: 1.5087942625607855 after n steps = 146 with final reward = 0.7501473115233109\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  147\n",
      "Total training rewards: -24.492934452548553 after n steps = 147 with final reward = 0.000133186527939511\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  148\n",
      "Total training rewards: 5.252467061268355 after n steps = 148 with final reward = 0.5000124669716609\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  149\n",
      "Total training rewards: 21.502964294348054 after n steps = 149 with final reward = 0.5000124038477538\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  150\n",
      "Total training rewards: 3.002393655539212 after n steps = 150 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  151\n",
      "Total training rewards: 9.00302635210204 after n steps = 151 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  152\n",
      "Total training rewards: 12.001989245359635 after n steps = 152 with final reward = 0.5000123103514139\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  153\n",
      "Total training rewards: 19.50475110713966 after n steps = 153 with final reward = 0.7501291546979181\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  154\n",
      "Total training rewards: 20.753131437341267 after n steps = 154 with final reward = 1.0000118862684686\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  155\n",
      "Total training rewards: 45.25224741278696 after n steps = 155 with final reward = 1.0000118174622834\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  156\n",
      "Total training rewards: 43.002804674304365 after n steps = 156 with final reward = 1.0000117776919777\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  157\n",
      "Total training rewards: 25.754125533010114 after n steps = 157 with final reward = 1.000011749448112\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  158\n",
      "Total training rewards: 39.00390162619628 after n steps = 158 with final reward = 0.0005183416443154433\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  159\n",
      "Total training rewards: 3.2547544123209002 after n steps = 159 with final reward = 0.00013049796050688234\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  160\n",
      "Total training rewards: -2.744689329149429 after n steps = 160 with final reward = 0.7501238223445025\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  161\n",
      "Total training rewards: 19.504961217030424 after n steps = 161 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  162\n",
      "Total training rewards: -1.9934411063978685 after n steps = 162 with final reward = 0.7501275788398855\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  163\n",
      "Total training rewards: 40.754104768543925 after n steps = 163 with final reward = 1.0000115066225053\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  164\n",
      "Total training rewards: 13.003651073908065 after n steps = 164 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  165\n",
      "Total training rewards: 38.50406662044134 after n steps = 165 with final reward = 0.750353288219685\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  166\n",
      "Total training rewards: 25.755242016702265 after n steps = 166 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  167\n",
      "Total training rewards: -9.744314317166944 after n steps = 167 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  168\n",
      "Total training rewards: -5.991296093745922 after n steps = 168 with final reward = 0.7501324960813383\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  169\n",
      "Total training rewards: -11.242802493332336 after n steps = 169 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  170\n",
      "Total training rewards: 41.505608590703424 after n steps = 170 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  171\n",
      "Total training rewards: 50.50228846268686 after n steps = 171 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  172\n",
      "Total training rewards: 41.253178760974784 after n steps = 172 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  173\n",
      "Total training rewards: 43.25317066940501 after n steps = 173 with final reward = 1.0000114154703232\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  174\n",
      "Total training rewards: 8.256318818333046 after n steps = 174 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  175\n",
      "Total training rewards: 6.50648586367231 after n steps = 175 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  176\n",
      "Total training rewards: 34.50314693522564 after n steps = 176 with final reward = 1.0000113414811638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  177\n",
      "Total training rewards: 11.007466107627717 after n steps = 177 with final reward = 1.0001331185062194\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  178\n",
      "Total training rewards: 11.00738637756507 after n steps = 178 with final reward = 1.0001383471524297\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  179\n",
      "Total training rewards: 1.75603577358944 after n steps = 179 with final reward = 1.0000113730730333\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  180\n",
      "Total training rewards: 46.251735490773925 after n steps = 180 with final reward = 1.0000113625228728\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  181\n",
      "Total training rewards: 36.00270886146996 after n steps = 181 with final reward = 1.0000113309895067\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  182\n",
      "Total training rewards: 6.50471248583967 after n steps = 182 with final reward = 1.0000113152883665\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  183\n",
      "Total training rewards: 47.75269803774415 after n steps = 183 with final reward = 1.0000112996306798\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  184\n",
      "Total training rewards: 21.756384208424492 after n steps = 184 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  185\n",
      "Total training rewards: 6.006617070439235 after n steps = 185 with final reward = 1.0001373266781757\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  186\n",
      "Total training rewards: -23.739951403636553 after n steps = 186 with final reward = 0.00013186945772713333\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  187\n",
      "Total training rewards: 6.004234049151833 after n steps = 187 with final reward = 1.0001364638927632\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  188\n",
      "Total training rewards: 12.503333290010547 after n steps = 188 with final reward = 1.0000114421295252\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  189\n",
      "Total training rewards: 48.50242973489545 after n steps = 189 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  190\n",
      "Total training rewards: 40.00224421810864 after n steps = 190 with final reward = 1.0000113783554638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  191\n",
      "Total training rewards: 50.751893544455676 after n steps = 191 with final reward = 1.0001320081946632\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  192\n",
      "Total training rewards: 39.754242148879364 after n steps = 192 with final reward = 1.000011320517243\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  193\n",
      "Total training rewards: 33.75418258942908 after n steps = 193 with final reward = 0.0001325660739855968\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  194\n",
      "Total training rewards: -9.991933554617539 after n steps = 194 with final reward = 0.7501351898522706\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  195\n",
      "Total training rewards: 20.757960027753846 after n steps = 195 with final reward = 0.000131800198558159\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  196\n",
      "Total training rewards: 1.5065893318379344 after n steps = 196 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  197\n",
      "Total training rewards: 37.25413969725156 after n steps = 197 with final reward = 1.0000113100643184\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  198\n",
      "Total training rewards: 40.75223938455781 after n steps = 198 with final reward = 0.7501201687575738\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  199\n",
      "Total training rewards: 13.255714393436268 after n steps = 199 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  200\n",
      "Total training rewards: -8.742707336299414 after n steps = 200 with final reward = 0.5005178039455143\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  201\n",
      "Total training rewards: -30.24304164575539 after n steps = 201 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  202\n",
      "Total training rewards: 41.00304662978394 after n steps = 202 with final reward = 1.0000113730730333\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  203\n",
      "Total training rewards: 19.003498162591086 after n steps = 203 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  204\n",
      "Total training rewards: -4.993244049800188 after n steps = 204 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  205\n",
      "Total training rewards: -2.743048138268918 after n steps = 205 with final reward = 1.0001331865279395\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  206\n",
      "Total training rewards: 4.757811455055572 after n steps = 206 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  207\n",
      "Total training rewards: -3.2427343009030425 after n steps = 207 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  208\n",
      "Total training rewards: 42.752666953941784 after n steps = 208 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  209\n",
      "Total training rewards: 35.253076003162505 after n steps = 209 with final reward = 1.00013224050491\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  210\n",
      "Total training rewards: 3.255675925442078 after n steps = 210 with final reward = 1.000131639411706\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  211\n",
      "Total training rewards: -3.7433712611515233 after n steps = 211 with final reward = 1.0001309121221384\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  212\n",
      "Total training rewards: 51.2523468479802 after n steps = 212 with final reward = 1.000011458184899\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  213\n",
      "Total training rewards: 33.25341522487012 after n steps = 213 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  214\n",
      "Total training rewards: 42.75358332219453 after n steps = 214 with final reward = 1.0001309779071244\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  215\n",
      "Total training rewards: 8.507381186562919 after n steps = 215 with final reward = 0.5000122610607175\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  216\n",
      "Total training rewards: 10.2544991834388 after n steps = 216 with final reward = 1.000011458184899\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  217\n",
      "Total training rewards: 44.00462330143182 after n steps = 217 with final reward = 1.0001303230175889\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  218\n",
      "Total training rewards: 2.757644779257854 after n steps = 218 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  219\n",
      "Total training rewards: 31.25478196755759 after n steps = 219 with final reward = 1.000011458184899\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  220\n",
      "Total training rewards: 27.2511060507786 after n steps = 220 with final reward = 1.0000114474763149\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  221\n",
      "Total training rewards: 38.002745699057535 after n steps = 221 with final reward = 1.000011426119083\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  222\n",
      "Total training rewards: 49.50122445846278 after n steps = 222 with final reward = 1.0000113995343478\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  223\n",
      "Total training rewards: 53.751755405970115 after n steps = 223 with final reward = 1.0000113677955051\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  224\n",
      "Total training rewards: 37.50144003821114 after n steps = 224 with final reward = 1.0000113414811638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  225\n",
      "Total training rewards: 28.001201041537875 after n steps = 225 with final reward = 1.000011325750954\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  226\n",
      "Total training rewards: 28.001902348336536 after n steps = 226 with final reward = 1.0000113100643184\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  227\n",
      "Total training rewards: 49.752192491023344 after n steps = 227 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  228\n",
      "Total training rewards: 13.005044638871166 after n steps = 228 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  229\n",
      "Total training rewards: -18.49392092743598 after n steps = 229 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  230\n",
      "Total training rewards: 11.758336327205862 after n steps = 230 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  231\n",
      "Total training rewards: -7.743510380273483 after n steps = 231 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  232\n",
      "Total training rewards: -3.492647796029786 after n steps = 232 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  233\n",
      "Total training rewards: -23.239273218909496 after n steps = 233 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  234\n",
      "Total training rewards: -16.74016180441633 after n steps = 234 with final reward = 1.0001270204849793\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  235\n",
      "Total training rewards: 15.758109662198532 after n steps = 235 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  236\n",
      "Total training rewards: 8.507385998258306 after n steps = 236 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  237\n",
      "Total training rewards: -7.49445230748886 after n steps = 237 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  238\n",
      "Total training rewards: 43.25169862356188 after n steps = 238 with final reward = 0.2501402725422329\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  239\n",
      "Total training rewards: 43.751109320382604 after n steps = 239 with final reward = 1.0000114528281037\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  240\n",
      "Total training rewards: 47.00231035654613 after n steps = 240 with final reward = 1.0000114314509163\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  241\n",
      "Total training rewards: 56.253262594956055 after n steps = 241 with final reward = 1.000125855159429\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  242\n",
      "Total training rewards: 29.752347053307254 after n steps = 242 with final reward = 1.0000114207922208\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  243\n",
      "Total training rewards: 50.50150259375737 after n steps = 243 with final reward = 1.0000114048413937\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  244\n",
      "Total training rewards: 66.25211502893572 after n steps = 244 with final reward = 1.0000113942322388\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  245\n",
      "Total training rewards: 63.00186146010043 after n steps = 245 with final reward = 1.0000113783554638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  246\n",
      "Total training rewards: 56.75136980196096 after n steps = 246 with final reward = 1.0000113625228728\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  247\n",
      "Total training rewards: 34.507396817598604 after n steps = 247 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  248\n",
      "Total training rewards: -17.494646304694058 after n steps = 248 with final reward = 1.0001253708682913\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  249\n",
      "Total training rewards: 53.502164718070176 after n steps = 249 with final reward = 1.0000113995343478\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  250\n",
      "Total training rewards: 53.25178153900065 after n steps = 250 with final reward = 1.0000113836428037\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  251\n",
      "Total training rewards: 33.50285633610315 after n steps = 251 with final reward = 1.0000113783554638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  252\n",
      "Total training rewards: 53.251812682320825 after n steps = 252 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  253\n",
      "Total training rewards: 16.507158248153953 after n steps = 253 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  254\n",
      "Total training rewards: 14.757348775005044 after n steps = 254 with final reward = 1.0001246513798077\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  255\n",
      "Total training rewards: 55.00254318269629 after n steps = 255 with final reward = 1.0000114048413937\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  256\n",
      "Total training rewards: 46.252865176530705 after n steps = 256 with final reward = 1.0000113995343478\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  257\n",
      "Total training rewards: 57.25173463658299 after n steps = 257 with final reward = 1.0000113783554638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  258\n",
      "Total training rewards: 44.00112451570118 after n steps = 258 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  259\n",
      "Total training rewards: 33.5025896588534 after n steps = 259 with final reward = 1.0000113677955051\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  260\n",
      "Total training rewards: 57.75123213072199 after n steps = 260 with final reward = 1.0000113572551292\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  261\n",
      "Total training rewards: 38.00321165292402 after n steps = 261 with final reward = 1.0000113519922678\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  262\n",
      "Total training rewards: 38.7517194640186 after n steps = 262 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  263\n",
      "Total training rewards: 26.754723101176094 after n steps = 263 with final reward = 1.0000113677955051\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  264\n",
      "Total training rewards: 52.501171504765495 after n steps = 264 with final reward = 1.0000113519922678\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  265\n",
      "Total training rewards: 36.50259735185166 after n steps = 265 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  266\n",
      "Total training rewards: 57.253576069013455 after n steps = 266 with final reward = 1.0000113309895067\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  267\n",
      "Total training rewards: 52.00189068278142 after n steps = 267 with final reward = 1.000011320517243\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  268\n",
      "Total training rewards: 57.00137604441544 after n steps = 268 with final reward = 1.0000113152883665\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  269\n",
      "Total training rewards: 29.503734674634885 after n steps = 269 with final reward = 1.0000113414811638\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  270\n",
      "Total training rewards: 48.751526608698484 after n steps = 270 with final reward = 1.0000113152883665\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  271\n",
      "Total training rewards: 47.50161632316136 after n steps = 271 with final reward = 1.0000113100643184\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  272\n",
      "Total training rewards: 56.2514188117179 after n steps = 272 with final reward = 1.000011284016266\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  273\n",
      "Total training rewards: 58.251930670225505 after n steps = 273 with final reward = 1.0000112529165428\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  274\n",
      "Total training rewards: 44.251236981065475 after n steps = 274 with final reward = 1.0000112219877755\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  275\n",
      "Total training rewards: 45.50166797425908 after n steps = 275 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  276\n",
      "Total training rewards: 60.501375899793935 after n steps = 276 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  277\n",
      "Total training rewards: 56.50190110844535 after n steps = 277 with final reward = 1.0001250100888142\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  278\n",
      "Total training rewards: 9.005636478356548 after n steps = 278 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  279\n",
      "Total training rewards: 4.7551446124974746 after n steps = 279 with final reward = 1.0000112014628777\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  280\n",
      "Total training rewards: 51.25095842015635 after n steps = 280 with final reward = -1\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  281\n",
      "Total training rewards: 57.751271428906016 after n steps = 281 with final reward = 1.0000111454046012\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  282\n",
      "Total training rewards: 62.00102388972672 after n steps = 282 with final reward = 1.0000111352724153\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  283\n",
      "Total training rewards: 54.251586581972816 after n steps = 283 with final reward = 1.0000111302132275\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  284\n",
      "Total training rewards: 56.50125869508435 after n steps = 284 with final reward = 1.0000110999543785\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  285\n",
      "Total training rewards: 62.50168607853993 after n steps = 285 with final reward = 1.0000110899046237\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  286\n",
      "Total training rewards: 56.75218749339102 after n steps = 286 with final reward = 1.0000110698596085\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  287\n",
      "Total training rewards: 36.00259274006331 after n steps = 287 with final reward = 1.0000110748640658\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  288\n",
      "Total training rewards: 58.000874608989406 after n steps = 288 with final reward = 1.0000110698596085\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  289\n",
      "Total training rewards: 58.25132788393755 after n steps = 289 with final reward = 1.0000110548733365\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  290\n",
      "Total training rewards: 60.50206633021784 after n steps = 290 with final reward = 1.0000110498869255\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  291\n",
      "Total training rewards: 55.25114166594823 after n steps = 291 with final reward = 1.0000118060719585\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  292\n",
      "Total training rewards: 54.25130739305353 after n steps = 292 with final reward = 1.0000110349546458\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  293\n",
      "Total training rewards: 44.00114684590835 after n steps = 293 with final reward = 1.0000118231656918\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  294\n",
      "Total training rewards: 37.006474302083014 after n steps = 294 with final reward = 1.0000110349546458\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  295\n",
      "Total training rewards: 64.25103502013214 after n steps = 295 with final reward = 1.0000144358275271\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  296\n",
      "Total training rewards: 61.501157060459434 after n steps = 296 with final reward = 1.0000142926483744\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  297\n",
      "Total training rewards: 55.001223812553135 after n steps = 297 with final reward = 1.0000142262481497\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  298\n",
      "Total training rewards: 68.00115868546764 after n steps = 298 with final reward = 1.0000111708159205\n",
      "Copying main network weights to the target network weights\n",
      "Episode :  299\n",
      "Total training rewards: 62.00100280046466 after n steps = 299 with final reward = 1.000014119653783\n",
      "Copying main network weights to the target network weights\n",
      "All Nodes Were Saved !\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 143753\n",
      "Collisions : 44910\n",
      "Received : 71053\n",
      "Processed : 108878\n",
      "Loss :  27783\n",
      "DER1 : 49.43 %\n",
      "DER2 : 68.76 %\n",
      "Energy : 10.225568179200074 J\n",
      "[0.3102250666666667, 0.11903488000000001, 0.7673241599999999, 0.7969791999999999, 0.6524108799999999, 8.164065279999999]\n",
      "DER ==> 27.138542655095176\n",
      "[0.5377023453628113, 0.7881477079918726, 0.21553147595242778, 0.20311998813489063, 0.27122087314600996, 8.105559497448607e-08]\n",
      "[0.3102250666666667, 0.11903488000000001, 0.7673241599999999, 0.7969791999999999, 0.6524108799999999, 8.164065279999999]\n",
      "Error : 22.29\n",
      "Loading Nodes From File\n",
      "Simulation Started\n",
      "Simulation Finished\n",
      "Sent : 143891\n",
      "Collisions : 46126\n",
      "Received : 70120\n",
      "Processed : 108609\n",
      "Loss :  27636\n",
      "DER1 : 48.73 %\n",
      "DER2 : 67.94 %\n",
      "Energy : 8.804555830271907 J\n",
      "[0.3102250666666667, 0.11903488000000001, 0.7673241599999999, 0.7969791999999999, 0.6524108799999999, 8.164065279999999]\n",
      "DER ==> 27.138542655095176\n",
      "[0.5377023453628113, 0.7881477079918726, 0.21553147595242778, 0.20311998813489063, 0.27122087314600996, 8.105559497448607e-08]\n",
      "[0.3102250666666667, 0.11903488000000001, 0.7673241599999999, 0.7969791999999999, 0.6524108799999999, 8.164065279999999]\n",
      "Error : 21.59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_names = ['nodes_6000.alloc','nodes_6000.rl']\n",
    "for i in range (2):\n",
    "    file_name = file_names [i]\n",
    "    nodes = load_nodes(file_name)\n",
    "    def transmit(env, node):\n",
    "\n",
    "        \"\"\"transmit\n",
    "\n",
    "        Parameters\n",
    "                #print (\"frequency coll 125\")\n",
    "        ----------\n",
    "            env : simpy.core.environment\n",
    "            node\n",
    "        \"\"\"\n",
    "        while True:\n",
    "\n",
    "            r = random.expovariate(1.0 / float(AVGSENDTIME))\n",
    "            # r = random.expovariate(1.0 / float(avg_gen(node, AVGSENDTIME)))\n",
    "            # node.freq = random.choice(ch)\n",
    "            # node.packet.freq =node.freq\n",
    "            # get_next_ch(node)\n",
    "            # print('duty cycle : ',r-(node.rectime*99))\n",
    "            # if (r-(node.rectime*99) < 0):\n",
    "            # r = node.rectime*99\n",
    "            yield env.timeout(r)\n",
    "            \"\"\"print(\n",
    "                \"Node-ID :{} SF :{} TX :{} Channel:{}  \".format(\n",
    "                    node.id, node.sf, node.tx, node.freq\n",
    "                )\n",
    "            )\"\"\"\n",
    "            node.sent = node.sent + 1\n",
    "            if node in packetsAtBS:\n",
    "                print(\"ERROR: packet already in\")\n",
    "            else:\n",
    "                sensitivity = config_dict[node.sf - 7][\"sens\"]\n",
    "                # print(\"node id: {} sf :{} s: {} \".format(node.id,node.sf,sensitivity))\n",
    "                if node.packet.rssi < sensitivity:\n",
    "                    node.packet.lost = True\n",
    "                else:\n",
    "                    node.packet.lost = False\n",
    "                    if (\n",
    "                        checkcollision(\n",
    "                            env,\n",
    "                            node.packet,\n",
    "                            packetsAtBS,\n",
    "                            maxBSReceives,\n",
    "                            full_collision,\n",
    "                        )\n",
    "                        == 1\n",
    "                    ):\n",
    "                        node.packet.collided = 1\n",
    "                    else:\n",
    "                        node.packet.collided = 0\n",
    "                        node.received = node.received + 1\n",
    "                    packetsAtBS.append(node)\n",
    "                    node.packet.addTime = env.now\n",
    "            # print('rectime : ',node.packet.rectime)\n",
    "            yield env.timeout(node.packet.rectime)\n",
    "\n",
    "            if node.packet.lost:\n",
    "                global nrLost\n",
    "                nrLost += 1\n",
    "            if node.packet.collided == 1:\n",
    "                global nrCollisions\n",
    "                nrCollisions = nrCollisions + 1\n",
    "            if not node.packet.collided and not node.packet.lost:\n",
    "                global nrReceived\n",
    "                nrReceived = nrReceived + 1\n",
    "            if node.packet.processed == 1:\n",
    "                global nrProcessed\n",
    "                nrProcessed = nrProcessed + 1\n",
    "            if node in packetsAtBS:\n",
    "                packetsAtBS.remove(node)\n",
    "                # reset the packet\n",
    "            node.packet.collided = 0\n",
    "            node.packet.processed = 0\n",
    "            node.packet.lost = False\n",
    "\n",
    "    MODEL = 6\n",
    "    packetsAtBS = []\n",
    "    sim_env = simpy.Environment()\n",
    "    bsId = 1   \n",
    "    maxBSReceives = 8\n",
    "    nrCollisions = 0\n",
    "    nrReceived = 0\n",
    "    nrProcessed = 0\n",
    "    nrLost = 0\n",
    "    full_collision = 1\n",
    "    # AVGSENDTIME = 800\n",
    "    simtime = 7200\n",
    "    # print(NODES)\n",
    "    for n in nodes:\n",
    "        # print(n.packet)\n",
    "        sim_env.process(transmit(sim_env, n))\n",
    "    print(\"Simulation Started\")\n",
    "    sim_env.run(until=simtime)\n",
    "    print(\"Simulation Finished\")\n",
    "\n",
    "    sent = sum(n.sent for n in nodes)\n",
    "    energy = (\n",
    "        sum(\n",
    "            node.packet.rectime * TX[int(node.tx) + 2] * 3 * node.sent\n",
    "            for node in nodes\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "\n",
    "    print(\"Sent :\", sent)\n",
    "    print(\"Collisions :\", nrCollisions)\n",
    "    print(\"Received :\", nrReceived)\n",
    "    print(\"Processed :\", nrProcessed)\n",
    "\n",
    "    print(\"Loss : \", nrLost)\n",
    "    # print(energy)\n",
    "    der1 = (float(nrReceived) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER1 : {:.2f} %\".format(der1))\n",
    "    der2 = (float(sent - nrCollisions) / float(sent)) * 100 if sent != 0 else 0\n",
    "    print(\"DER2 : {:.2f} %\".format(der2))\n",
    "    print(\"Energy : {} J\".format(energy))\n",
    "\n",
    "    DER, DERs, charges = mathematical_model(\n",
    "        len(nodes), sf_distribution, AVGSENDTIME\n",
    "    )\n",
    "    print(charges)\n",
    "    DER = DER * 100\n",
    "    print(\"DER ==>\", DER)\n",
    "    print(DERs)\n",
    "    print(charges)\n",
    "    error = abs(DER - der1)\n",
    "    print(\"Error : {:.2f}\".format(error))\n",
    "\n",
    "    data = {\n",
    "        \"Experience\": 1,\n",
    "        \"Nodes\": len(nodes),\n",
    "        \"PathLoss\": MODEL,\n",
    "        \"Packet_Sent\": sent,\n",
    "        \"Packet_Loss\": nrLost,\n",
    "        \"Collisions\": nrCollisions,\n",
    "        \"Energy\": energy,\n",
    "        \"DER\": der1,\n",
    "        \"Mathematical-Model\": DER,\n",
    "        \"Error\": error,\n",
    "        \"SF7\": charges[0],\n",
    "        \"SF8\": charges[1],\n",
    "        \"SF9\": charges[2],\n",
    "        \"SF10\": charges[3],\n",
    "        \"SF11\": charges[4],\n",
    "        \"SF12\": charges[5],\n",
    "        \"DSF7\": sf_distribution[0],\n",
    "        \"DSF8\": sf_distribution[1],\n",
    "        \"DSF9\": sf_distribution[2],\n",
    "        \"DSF10\": sf_distribution[3],\n",
    "        \"DSF11\": sf_distribution[4],\n",
    "        \"DSF12\": sf_distribution[5],\n",
    "       }\n",
    "\n",
    "    finalReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devices = load_nodes('nodes_1000.alloc')\n",
    "#print_nodes(devices)\n",
    "#model = agent.create_model(env.action_space.shape, env.observation_space.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
